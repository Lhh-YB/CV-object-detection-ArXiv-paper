# Arxiv Papers in cs.CV on 2024-05-01
### Learning to Compose: Improving Object Centric Learning by Injecting Compositionality
- **Arxiv ID**: http://arxiv.org/abs/2405.00646v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2405.00646v1)
- **Published**: 2024-05-01 17:21:36+00:00
- **Updated**: 2024-05-01 17:21:36+00:00
- **Authors**: Whie Jung, Jaehoon Yoo, Sungjin Ahn, Seunghoon Hong
- **Comment**: None
- **Journal**: None
- **Summary**: Learning compositional representation is a key aspect of object-centric learning as it enables flexible systematic generalization and supports complex visual reasoning. However, most of the existing approaches rely on auto-encoding objective, while the compositionality is implicitly imposed by the architectural or algorithmic bias in the encoder. This misalignment between auto-encoding objective and learning compositionality often results in failure of capturing meaningful object representations. In this study, we propose a novel objective that explicitly encourages compositionality of the representations. Built upon the existing object-centric learning framework (e.g., slot attention), our method incorporates additional constraints that an arbitrary mixture of object representations from two images should be valid by maximizing the likelihood of the composite data. We demonstrate that incorporating our objective to the existing framework consistently improves the objective-centric learning and enhances the robustness to the architectural choices.



### Obtaining Favorable Layouts for Multiple Object Generation
- **Arxiv ID**: http://arxiv.org/abs/2405.00791v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, I.2, I.4
- **Links**: [PDF](http://arxiv.org/pdf/2405.00791v1)
- **Published**: 2024-05-01 18:07:48+00:00
- **Updated**: 2024-05-01 18:07:48+00:00
- **Authors**: Barak Battash, Amit Rozner, Lior Wolf, Ofir Lindenbaum
- **Comment**: None
- **Journal**: None
- **Summary**: Large-scale text-to-image models that can generate high-quality and diverse images based on textual prompts have shown remarkable success. These models aim ultimately to create complex scenes, and addressing the challenge of multi-subject generation is a critical step towards this goal. However, the existing state-of-the-art diffusion models face difficulty when generating images that involve multiple subjects. When presented with a prompt containing more than one subject, these models may omit some subjects or merge them together. To address this challenge, we propose a novel approach based on a guiding principle. We allow the diffusion model to initially propose a layout, and then we rearrange the layout grid. This is achieved by enforcing cross-attention maps (XAMs) to adhere to proposed masks and by migrating pixels from latent maps to new locations determined by us. We introduce new loss terms aimed at reducing XAM entropy for clearer spatial definition of subjects, reduce the overlap between XAMs, and ensure that XAMs align with their respective masks. We contrast our approach with several alternative methods and show that it more faithfully captures the desired concepts across a variety of text prompts.



