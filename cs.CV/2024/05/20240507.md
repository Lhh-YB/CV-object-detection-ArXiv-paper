# Arxiv Papers in cs.CV on 2024-05-07
### Deep Event-based Object Detection in Autonomous Driving: A Survey
- **Arxiv ID**: http://arxiv.org/abs/2405.03995v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2405.03995v1)
- **Published**: 2024-05-07 04:17:04+00:00
- **Updated**: 2024-05-07 04:17:04+00:00
- **Authors**: Bingquan Zhou, Jie Jiang
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection plays a critical role in autonomous driving, where accurately and efficiently detecting objects in fast-moving scenes is crucial. Traditional frame-based cameras face challenges in balancing latency and bandwidth, necessitating the need for innovative solutions. Event cameras have emerged as promising sensors for autonomous driving due to their low latency, high dynamic range, and low power consumption. However, effectively utilizing the asynchronous and sparse event data presents challenges, particularly in maintaining low latency and lightweight architectures for object detection. This paper provides an overview of object detection using event data in autonomous driving, showcasing the competitive benefits of event cameras.



### Space-time Reinforcement Network for Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2405.04042v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2405.04042v1)
- **Published**: 2024-05-07 06:26:30+00:00
- **Updated**: 2024-05-07 06:26:30+00:00
- **Authors**: Yadang Chen, Wentao Zhu, Zhi-Xin Yang, Enhua Wu
- **Comment**: Accepted by ICME 2024. 6 pages, 10 figures
- **Journal**: None
- **Summary**: Recently, video object segmentation (VOS) networks typically use memory-based methods: for each query frame, the mask is predicted by space-time matching to memory frames. Despite these methods having superior performance, they suffer from two issues: 1) Challenging data can destroy the space-time coherence between adjacent video frames. 2) Pixel-level matching will lead to undesired mismatching caused by the noises or distractors. To address the aforementioned issues, we first propose to generate an auxiliary frame between adjacent frames, serving as an implicit short-temporal reference for the query one. Next, we learn a prototype for each video object and prototype-level matching can be implemented between the query and memory. The experiment demonstrated that our network outperforms the state-of-the-art method on the DAVIS 2017, achieving a J&F score of 86.4%, and attains a competitive result 85.0% on YouTube VOS 2018. In addition, our network exhibits a high inference speed of 32+ FPS.



### DCNN: Dual Cross-current Neural Networks Realized Using An Interactive Deep Learning Discriminator for Fine-grained Objects
- **Arxiv ID**: http://arxiv.org/abs/2405.04093v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2405.04093v1)
- **Published**: 2024-05-07 07:51:28+00:00
- **Updated**: 2024-05-07 07:51:28+00:00
- **Authors**: Da Fu, Mingfei Rong, Eun-Hu Kim, Hao Huang, Witold Pedrycz
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate classification of fine-grained images remains a challenge in backbones based on convolutional operations or self-attention mechanisms. This study proposes novel dual-current neural networks (DCNN), which combine the advantages of convolutional operations and self-attention mechanisms to improve the accuracy of fine-grained image classification. The main novel design features for constructing a weakly supervised learning backbone model DCNN include (a) extracting heterogeneous data, (b) keeping the feature map resolution unchanged, (c) expanding the receptive field, and (d) fusing global representations and local features. Experimental results demonstrated that using DCNN as the backbone network for classifying certain fine-grained benchmark datasets achieved performance advantage improvements of 13.5--19.5% and 2.2--12.9%, respectively, compared to other advanced convolution or attention-based fine-grained backbones.



### Diff-IP2D: Diffusion-Based Hand-Object Interaction Prediction on Egocentric Videos
- **Arxiv ID**: http://arxiv.org/abs/2405.04370v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2405.04370v4)
- **Published**: 2024-05-07 14:51:05+00:00
- **Updated**: 2024-11-22 05:09:29+00:00
- **Authors**: Junyi Ma, Jingyi Xu, Xieyuanli Chen, Hesheng Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Understanding how humans would behave during hand-object interaction is vital for applications in service robot manipulation and extended reality. To achieve this, some recent works have been proposed to simultaneously forecast hand trajectories and object affordances on human egocentric videos. The joint prediction serves as a comprehensive representation of future hand-object interactions in 2D space, indicating potential human motion and motivation. However, the existing approaches mostly adopt the autoregressive paradigm for unidirectional prediction, which lacks mutual constraints within the holistic future sequence, and accumulates errors along the time axis. Meanwhile, these works basically overlook the effect of camera egomotion on first-person view predictions. To address these limitations, we propose a novel diffusion-based interaction prediction method, namely Diff-IP2D, to forecast future hand trajectories and object affordances concurrently in an iterative non-autoregressive manner. We transform the sequential 2D images into latent feature space and design a denoising diffusion model to predict future latent interaction features conditioned on past ones. Motion features are further integrated into the conditional denoising process to enable Diff-IP2D aware of the camera wearer's dynamics for more accurate interaction prediction. Extensive experiments demonstrate that our method significantly outperforms the state-of-the-art baselines on both the off-the-shelf metrics and our newly proposed evaluation protocol. This highlights the efficacy of leveraging a generative paradigm for 2D hand-object interaction prediction. The code of Diff-IP2D is released as open source at https://github.com/IRMVLab/Diff-IP2D.



