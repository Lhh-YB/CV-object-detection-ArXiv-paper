# Arxiv Papers in cs.CV on 2024-05-06
### Comparing fine-grained and coarse-grained object detection for ecology
- **Arxiv ID**: http://arxiv.org/abs/2407.00018v1
- **DOI**: None
- **Categories**: **q-bio.QM**, cs.CV, q-bio.PE
- **Links**: [PDF](http://arxiv.org/pdf/2407.00018v1)
- **Published**: 2024-05-06 04:50:55+00:00
- **Updated**: 2024-05-06 04:50:55+00:00
- **Authors**: Jess Tam, Justin Kay
- **Comment**: 6 pages, 4 figures, accepted to be presented as a poster presentation
  at a conference workshop (11th Fine-Grained Visual Categorisation 2024)
- **Journal**: None
- **Summary**: Computer vision applications are increasingly popular for wildlife monitoring tasks. While some studies focus on the monitoring of a single species, such as a particular endangered species, others monitor larger functional groups, such as predators. In our study, we used camera trap images collected in north-western New South Wales, Australia, to investigate how model results were affected by combining multiple species in single classes, and whether the addition of negative samples can improve model performance. We found that species that benefited the most from merging into a single class were mainly species that look alike morphologically, i.e. macropods. Whereas species that looked distinctively different gave mixed results when merged, e.g. merging pigs and goats together as non-native large mammals. We also found that adding negative samples improved model performance marginally in most instances, and recommend conducting a more comprehensive study to explore whether the marginal gains were random or consistent. We suggest that practitioners could classify morphologically similar species together as a functional group or higher taxonomic group to draw ecological inferences. Nevertheless, whether to merge classes or not will depend on the ecological question to be explored.



### Modality Prompts for Arbitrary Modality Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2405.03351v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2405.03351v1)
- **Published**: 2024-05-06 11:02:02+00:00
- **Updated**: 2024-05-06 11:02:02+00:00
- **Authors**: Nianchang Huang, Yang Yang, Qiang Zhang, Jungong Han, Jin Huang
- **Comment**: 13 pages, 7 Figures, 3 Tables
- **Journal**: None
- **Summary**: This paper delves into the task of arbitrary modality salient object detection (AM SOD), aiming to detect salient objects from arbitrary modalities, eg RGB images, RGB-D images, and RGB-D-T images. A novel modality-adaptive Transformer (MAT) will be proposed to investigate two fundamental challenges of AM SOD, ie more diverse modality discrepancies caused by varying modality types that need to be processed, and dynamic fusion design caused by an uncertain number of modalities present in the inputs of multimodal fusion strategy. Specifically, inspired by prompt learning's ability of aligning the distributions of pre-trained models to the characteristic of downstream tasks by learning some prompts, MAT will first present a modality-adaptive feature extractor (MAFE) to tackle the diverse modality discrepancies by introducing a modality prompt for each modality. In the training stage, a new modality translation contractive (MTC) loss will be further designed to assist MAFE in learning those modality-distinguishable modality prompts. Accordingly, in the testing stage, MAFE can employ those learned modality prompts to adaptively adjust its feature space according to the characteristics of the input modalities, thus being able to extract discriminative unimodal features. Then, MAFE will present a channel-wise and spatial-wise fusion hybrid (CSFH) strategy to meet the demand for dynamic fusion. For that, CSFH dedicates a channel-wise dynamic fusion module (CDFM) and a novel spatial-wise dynamic fusion module (SDFM) to fuse the unimodal features from varying numbers of modalities and meanwhile effectively capture cross-modal complementary semantic and detail information, respectively. Moreover, CSFH will carefully align CDFM and SDFM to different levels of unimodal features based on their characteristics for more effective complementary information exploitation.



### Salient Object Detection From Arbitrary Modalities
- **Arxiv ID**: http://arxiv.org/abs/2405.03352v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2405.03352v2)
- **Published**: 2024-05-06 11:02:26+00:00
- **Updated**: 2024-05-09 09:43:46+00:00
- **Authors**: Nianchang Huang, Yang Yang, Ruida Xi, Qiang Zhang, Jungong Han, Jin Huang
- **Comment**: 15 Pages, 7 Figures, 8 Tables
- **Journal**: None
- **Summary**: Toward desirable saliency prediction, the types and numbers of inputs for a salient object detection (SOD) algorithm may dynamically change in many real-life applications. However, existing SOD algorithms are mainly designed or trained for one particular type of inputs, failing to be generalized to other types of inputs. Consequentially, more types of SOD algorithms need to be prepared in advance for handling different types of inputs, raising huge hardware and research costs. Differently, in this paper, we propose a new type of SOD task, termed Arbitrary Modality SOD (AM SOD). The most prominent characteristics of AM SOD are that the modality types and modality numbers will be arbitrary or dynamically changed. The former means that the inputs to the AM SOD algorithm may be arbitrary modalities such as RGB, depths, or even any combination of them. While, the latter indicates that the inputs may have arbitrary modality numbers as the input type is changed, e.g. single-modality RGB image, dual-modality RGB-Depth (RGB-D) images or triple-modality RGB-Depth-Thermal (RGB-D-T) images. Accordingly, a preliminary solution to the above challenges, \i.e. a modality switch network (MSN), is proposed in this paper. In particular, a modality switch feature extractor (MSFE) is first designed to extract discriminative features from each modality effectively by introducing some modality indicators, which will generate some weights for modality switching. Subsequently, a dynamic fusion module (DFM) is proposed to adaptively fuse features from a variable number of modalities based on a novel Transformer structure. Finally, a new dataset, named AM-XD, is constructed to facilitate research on AM SOD. Extensive experiments demonstrate that our AM SOD method can effectively cope with changes in the type and number of input modalities for robust salient object detection.



### SSyncOA: Self-synchronizing Object-aligned Watermarking to Resist Cropping-paste Attacks
- **Arxiv ID**: http://arxiv.org/abs/2405.03458v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2405.03458v1)
- **Published**: 2024-05-06 13:29:34+00:00
- **Updated**: 2024-05-06 13:29:34+00:00
- **Authors**: Chengxin Zhao, Hefei Ling, Sijing Xie, Han Fang, Yaokun Fang, Nan Sun
- **Comment**: 7 pages, 5 figures (Have been accepted by ICME 2024)
- **Journal**: None
- **Summary**: Modern image processing tools have made it easy for attackers to crop the region or object of interest in images and paste it into other images. The challenge this cropping-paste attack poses to the watermarking technology is that it breaks the synchronization of the image watermark, introducing multiple superimposed desynchronization distortions, such as rotation, scaling, and translation. However, current watermarking methods can only resist a single type of desynchronization and cannot be applied to protect the object's copyright under the cropping-paste attack. With the finding that the key to resisting the cropping-paste attack lies in robust features of the object to protect, this paper proposes a self-synchronizing object-aligned watermarking method, called SSyncOA. Specifically, we first constrain the watermarked region to be aligned with the protected object, and then synchronize the watermark's translation, rotation, and scaling distortions by normalizing the object invariant features, i.e., its centroid, principal orientation, and minimum bounding square, respectively. To make the watermark embedded in the protected object, we introduce the object-aligned watermarking model, which incorporates the real cropping-paste attack into the encoder-noise layer-decoder pipeline and is optimized end-to-end. Besides, we illustrate the effect of different desynchronization distortions on the watermark training, which confirms the necessity of the self-synchronization process. Extensive experiments demonstrate the superiority of our method over other SOTAs.



### Low-light Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2405.03519v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2405.03519v1)
- **Published**: 2024-05-06 14:36:01+00:00
- **Updated**: 2024-05-06 14:36:01+00:00
- **Authors**: Pengpeng Li, Haowei Gu, Yang Yang
- **Comment**: None
- **Journal**: None
- **Summary**: In this competition we employed a model fusion approach to achieve object detection results close to those of real images. Our method is based on the CO-DETR model, which was trained on two sets of data: one containing images under dark conditions and another containing images enhanced with low-light conditions. We used various enhancement techniques on the test data to generate multiple sets of prediction results. Finally, we applied a clustering aggregation method guided by IoU thresholds to select the optimal results.



### Collecting Consistently High Quality Object Tracks with Minimal Human Involvement by Using Self-Supervised Learning to Detect Tracker Errors
- **Arxiv ID**: http://arxiv.org/abs/2405.03643v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2405.03643v1)
- **Published**: 2024-05-06 17:06:32+00:00
- **Updated**: 2024-05-06 17:06:32+00:00
- **Authors**: Samreen Anjum, Suyog Jain, Danna Gurari
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a hybrid framework for consistently producing high-quality object tracks by combining an automated object tracker with little human input. The key idea is to tailor a module for each dataset to intelligently decide when an object tracker is failing and so humans should be brought in to re-localize an object for continued tracking. Our approach leverages self-supervised learning on unlabeled videos to learn a tailored representation for a target object that is then used to actively monitor its tracked region and decide when the tracker fails. Since labeled data is not needed, our approach can be applied to novel object categories. Experiments on three datasets demonstrate our method outperforms existing approaches, especially for small, fast moving, or occluded objects.



### BadFusion: 2D-Oriented Backdoor Attacks against 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2405.03884v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2405.03884v1)
- **Published**: 2024-05-06 22:02:38+00:00
- **Updated**: 2024-05-06 22:02:38+00:00
- **Authors**: Saket S. Chaturvedi, Lan Zhang, Wenbin Zhang, Pan He, Xiaoyong Yuan
- **Comment**: Accepted at IJCAI 2024 Conference
- **Journal**: None
- **Summary**: 3D object detection plays an important role in autonomous driving; however, its vulnerability to backdoor attacks has become evident. By injecting ''triggers'' to poison the training dataset, backdoor attacks manipulate the detector's prediction for inputs containing these triggers. Existing backdoor attacks against 3D object detection primarily poison 3D LiDAR signals, where large-sized 3D triggers are injected to ensure their visibility within the sparse 3D space, rendering them easy to detect and impractical in real-world scenarios.   In this paper, we delve into the robustness of 3D object detection, exploring a new backdoor attack surface through 2D cameras. Given the prevalent adoption of camera and LiDAR signal fusion for high-fidelity 3D perception, we investigate the latent potential of camera signals to disrupt the process. Although the dense nature of camera signals enables the use of nearly imperceptible small-sized triggers to mislead 2D object detection, realizing 2D-oriented backdoor attacks against 3D object detection is non-trivial. The primary challenge emerges from the fusion process that transforms camera signals into a 3D space, compromising the association with the 2D trigger to the target output. To tackle this issue, we propose an innovative 2D-oriented backdoor attack against LiDAR-camera fusion methods for 3D object detection, named BadFusion, for preserving trigger effectiveness throughout the entire fusion process. The evaluation demonstrates the effectiveness of BadFusion, achieving a significantly higher attack success rate compared to existing 2D-oriented attacks.



### MVDiff: Scalable and Flexible Multi-View Diffusion for 3D Object Reconstruction from Single-View
- **Arxiv ID**: http://arxiv.org/abs/2405.03894v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2405.03894v2)
- **Published**: 2024-05-06 22:55:53+00:00
- **Updated**: 2024-06-13 00:35:06+00:00
- **Authors**: Emmanuelle Bourigault, Pauline Bourigault
- **Comment**: CVPRW: Generative Models for Computer Vision
- **Journal**: None
- **Summary**: Generating consistent multiple views for 3D reconstruction tasks is still a challenge to existing image-to-3D diffusion models. Generally, incorporating 3D representations into diffusion model decrease the model's speed as well as generalizability and quality. This paper proposes a general framework to generate consistent multi-view images from single image or leveraging scene representation transformer and view-conditioned diffusion model. In the model, we introduce epipolar geometry constraints and multi-view attention to enforce 3D consistency. From as few as one image input, our model is able to generate 3D meshes surpassing baselines methods in evaluation metrics, including PSNR, SSIM and LPIPS.



