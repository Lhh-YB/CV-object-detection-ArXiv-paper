# Arxiv Papers in cs.CV on 2024-05-09
### The object detection model uses combined extraction with KNN and RF classification
- **Arxiv ID**: http://arxiv.org/abs/2405.05551v1
- **DOI**: 10.11591/ijeecs.v35.i1.pp436-445
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2405.05551v1)
- **Published**: 2024-05-09 05:21:42+00:00
- **Updated**: 2024-05-09 05:21:42+00:00
- **Authors**: Florentina Tatrin Kurniati, Daniel HF Manongga, Irwan Sembiring, Sutarto Wijono, Roy Rudolf Huizen
- **Comment**: None
- **Journal**: IJEECS, pp 436-445, Vol 35, No 1 July 2024;
  https://ijeecs.iaescore.com/index.php/IJEECS/article/view/35888
- **Summary**: Object detection plays an important role in various fields. Developing detection models for 2D objects that experience rotation and texture variations is a challenge. In this research, the initial stage of the proposed model integrates the gray-level co-occurrence matrix (GLCM) and local binary patterns (LBP) texture feature extraction to obtain feature vectors. The next stage is classifying features using k-nearest neighbors (KNN) and random forest (RF), as well as voting ensemble (VE). System testing used a dataset of 4,437 2D images, the results for KNN accuracy were 92.7% and F1-score 92.5%, while RF performance was lower. Although GLCM features improve performance on both algorithms, KNN is more consistent. The VE approach provides the best performance with an accuracy of 93.9% and an F1 score of 93.8%, this shows the effectiveness of the ensemble technique in increasing object detection accuracy. This study contributes to the field of object detection with a new approach combining GLCM and LBP as feature vectors as well as VE for classification



### Depth Awakens: A Depth-perceptual Attention Fusion Network for RGB-D Camouflaged Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2405.05614v1
- **DOI**: 10.1016/j.imavis.2024.104924
- **Categories**: **cs.CV**, cs.NI
- **Links**: [PDF](http://arxiv.org/pdf/2405.05614v1)
- **Published**: 2024-05-09 08:17:43+00:00
- **Updated**: 2024-05-09 08:17:43+00:00
- **Authors**: Xinran Liua, Lin Qia, Yuxuan Songa, Qi Wen
- **Comment**: None
- **Journal**: Image and Vision Computing, 143:104924, 2024
- **Summary**: Camouflaged object detection (COD) presents a persistent challenge in accurately identifying objects that seamlessly blend into their surroundings. However, most existing COD models overlook the fact that visual systems operate within a genuine 3D environment. The scene depth inherent in a single 2D image provides rich spatial clues that can assist in the detection of camouflaged objects. Therefore, we propose a novel depth-perception attention fusion network that leverages the depth map as an auxiliary input to enhance the network's ability to perceive 3D information, which is typically challenging for the human eye to discern from 2D images. The network uses a trident-branch encoder to extract chromatic and depth information and their communications. Recognizing that certain regions of a depth map may not effectively highlight the camouflaged object, we introduce a depth-weighted cross-attention fusion module to dynamically adjust the fusion weights on depth and RGB feature maps. To keep the model simple without compromising effectiveness, we design a straightforward feature aggregation decoder that adaptively fuses the enhanced aggregated features. Experiments demonstrate the significant superiority of our proposed method over other states of the arts, which further validates the contribution of depth information in camouflaged object detection. The code will be available at https://github.com/xinran-liu00/DAF-Net.



### ASGrasp: Generalizable Transparent Object Reconstruction and Grasping from RGB-D Active Stereo Camera
- **Arxiv ID**: http://arxiv.org/abs/2405.05648v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2405.05648v1)
- **Published**: 2024-05-09 09:44:51+00:00
- **Updated**: 2024-05-09 09:44:51+00:00
- **Authors**: Jun Shi, Yong A, Yixiang Jin, Dingzhe Li, Haoyu Niu, Zhezhu Jin, He Wang
- **Comment**: IEEE International Conference on Robotics and Automation (ICRA), 2024
- **Journal**: None
- **Summary**: In this paper, we tackle the problem of grasping transparent and specular objects. This issue holds importance, yet it remains unsolved within the field of robotics due to failure of recover their accurate geometry by depth cameras. For the first time, we propose ASGrasp, a 6-DoF grasp detection network that uses an RGB-D active stereo camera. ASGrasp utilizes a two-layer learning-based stereo network for the purpose of transparent object reconstruction, enabling material-agnostic object grasping in cluttered environments. In contrast to existing RGB-D based grasp detection methods, which heavily depend on depth restoration networks and the quality of depth maps generated by depth cameras, our system distinguishes itself by its ability to directly utilize raw IR and RGB images for transparent object geometry reconstruction. We create an extensive synthetic dataset through domain randomization, which is based on GraspNet-1Billion. Our experiments demonstrate that ASGrasp can achieve over 90% success rate for generalizable transparent object grasping in both simulation and the real via seamless sim-to-real transfer. Our method significantly outperforms SOTA networks and even surpasses the performance upper bound set by perfect visible point cloud inputs.Project page: https://pku-epic.github.io/ASGrasp



### Free-Moving Object Reconstruction and Pose Estimation with Virtual Camera
- **Arxiv ID**: http://arxiv.org/abs/2405.05858v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2405.05858v2)
- **Published**: 2024-05-09 15:45:08+00:00
- **Updated**: 2024-05-10 15:57:13+00:00
- **Authors**: Haixin Shi, Yinlin Hu, Daniel Koguciuk, Juan-Ting Lin, Mathieu Salzmann, David Ferstl
- **Comment**: None
- **Journal**: None
- **Summary**: We propose an approach for reconstructing free-moving object from a monocular RGB video. Most existing methods either assume scene prior, hand pose prior, object category pose prior, or rely on local optimization with multiple sequence segments. We propose a method that allows free interaction with the object in front of a moving camera without relying on any prior, and optimizes the sequence globally without any segments. We progressively optimize the object shape and pose simultaneously based on an implicit neural representation. A key aspect of our method is a virtual camera system that reduces the search space of the optimization significantly. We evaluate our method on the standard HO3D dataset and a collection of egocentric RGB sequences captured with a head-mounted device. We demonstrate that our approach outperforms most methods significantly, and is on par with recent techniques that assume prior information.



