# Arxiv Papers in cs.CV on 2024-12-04
### Point-GR: Graph Residual Point Cloud Network for 3D Object Classification and Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2412.03052v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2412.03052v1)
- **Published**: 2024-12-04 06:12:19+00:00
- **Updated**: 2024-12-04 06:12:19+00:00
- **Authors**: Md Meraz, Md Afzal Ansari, Mohammed Javed, Pavan Chakraborty
- **Comment**: ICPR 2024 G2SP-CV Workshop, Dec 1-5, 2024 Kolkata, India
- **Journal**: None
- **Summary**: In recent years, the challenge of 3D shape analysis within point cloud data has gathered significant attention in computer vision. Addressing the complexities of effective 3D information representation and meaningful feature extraction for classification tasks remains crucial. This paper presents Point-GR, a novel deep learning architecture designed explicitly to transform unordered raw point clouds into higher dimensions while preserving local geometric features. It introduces residual-based learning within the network to mitigate the point permutation issues in point cloud data. The proposed Point-GR network significantly reduced the number of network parameters in Classification and Part-Segmentation compared to baseline graph-based networks. Notably, the Point-GR model achieves a state-of-the-art scene segmentation mean IoU of 73.47% on the S3DIS benchmark dataset, showcasing its effectiveness. Furthermore, the model shows competitive results in Classification and Part-Segmentation tasks.



### ObjectFinder: Open-Vocabulary Assistive System for Interactive Object Search by Blind People
- **Arxiv ID**: http://arxiv.org/abs/2412.03118v1
- **DOI**: None
- **Categories**: **cs.HC**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2412.03118v1)
- **Published**: 2024-12-04 08:38:45+00:00
- **Updated**: 2024-12-04 08:38:45+00:00
- **Authors**: Ruiping Liu, Jiaming Zhang, Angela Schön, Karin Müller, Junwei Zheng, Kailun Yang, Kathrin Gerling, Rainer Stiefelhagen
- **Comment**: None
- **Journal**: None
- **Summary**: Assistive technology can be leveraged by blind people when searching for objects in their daily lives. We created ObjectFinder, an open-vocabulary interactive object-search prototype, which combines object detection with scene description and navigation. It enables blind persons to detect and navigate to objects of their choice. Our approach used co-design for the development of the prototype. We further conducted need-finding interviews to better understand challenges in object search, followed by a study with the ObjectFinder prototype in a laboratory setting simulating a living room and an office, with eight blind users. Additionally, we compared the prototype with BeMyEyes and Lookout for object search. We found that most participants felt more independent with ObjectFinder and preferred it over the baselines when deployed on more efficient hardware, as it enhances mental mapping and allows for active target definition. Moreover, we identified factors for future directions for the development of object-search systems.



### Data Fusion of Semantic and Depth Information in the Context of Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2412.03490v1
- **DOI**: 10.1109/ICoICI62503.2024.10696627
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2412.03490v1)
- **Published**: 2024-12-04 17:26:30+00:00
- **Updated**: 2024-12-04 17:26:30+00:00
- **Authors**: Md Abu Yusuf, Md Rezaul Karim Khan, Partha Pratim Saha, Mohammed Mahbubur Rahaman
- **Comment**: None
- **Journal**: None
- **Summary**: Considerable study has already been conducted regarding autonomous driving in modern era. An autonomous driving system must be extremely good at detecting objects surrounding the car to ensure safety. In this paper, classification, and estimation of an object's (pedestrian) position (concerning an ego 3D coordinate system) are studied and the distance between the ego vehicle and the object in the context of autonomous driving is measured. To classify the object, faster Region-based Convolution Neural Network (R-CNN) with inception v2 is utilized. First, a network is trained with customized dataset to estimate the reference position of objects as well as the distance from the vehicle. From camera calibration to computing the distance, cutting-edge technologies of computer vision algorithms in a series of processes are applied to generate a 3D reference point of the region of interest. The foremost step in this process is generating a disparity map using the concept of stereo vision.



### Style3D: Attention-guided Multi-view Style Transfer for 3D Object Generation
- **Arxiv ID**: http://arxiv.org/abs/2412.03571v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2412.03571v1)
- **Published**: 2024-12-04 18:59:38+00:00
- **Updated**: 2024-12-04 18:59:38+00:00
- **Authors**: Bingjie Song, Xin Huang, Ruting Xie, Xue Wang, Qing Wang
- **Comment**: None
- **Journal**: None
- **Summary**: We present Style3D, a novel approach for generating stylized 3D objects from a content image and a style image. Unlike most previous methods that require case- or style-specific training, Style3D supports instant 3D object stylization. Our key insight is that 3D object stylization can be decomposed into two interconnected processes: multi-view dual-feature alignment and sparse-view spatial reconstruction. We introduce MultiFusion Attention, an attention-guided technique to achieve multi-view stylization from the content-style pair. Specifically, the query features from the content image preserve geometric consistency across multiple views, while the key and value features from the style image are used to guide the stylistic transfer. This dual-feature alignment ensures that spatial coherence and stylistic fidelity are maintained across multi-view images. Finally, a large 3D reconstruction model is introduced to generate coherent stylized 3D objects. By establishing an interplay between structural and stylistic features across multiple views, our approach enables a holistic 3D stylization process. Extensive experiments demonstrate that Style3D offers a more flexible and scalable solution for generating style-consistent 3D assets, surpassing existing methods in both computational efficiency and visual quality.



