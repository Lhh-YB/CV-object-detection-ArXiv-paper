# Arxiv Papers in cs.CV on 2024-12-01
### Visual Modality Prompt for Adapting Vision-Language Object Detectors
- **Arxiv ID**: http://arxiv.org/abs/2412.00622v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2412.00622v1)
- **Published**: 2024-12-01 00:19:59+00:00
- **Updated**: 2024-12-01 00:19:59+00:00
- **Authors**: Heitor R. Medeiros, Atif Belal, Srikanth Muralidharan, Eric Granger, Marco Pedersoli
- **Comment**: None
- **Journal**: None
- **Summary**: The zero-shot performance of object detectors degrades when tested on different modalities, such as infrared and depth. While recent work has explored image translation techniques to adapt detectors to new modalities, these methods are limited to a single modality and apply only to traditional detectors. Recently, vision-language detectors, such as YOLO-World and Grounding DINO, have shown promising zero-shot capabilities, however, they have not yet been adapted for other visual modalities. Traditional fine-tuning approaches tend to compromise the zero-shot capabilities of the detectors. The visual prompt strategies commonly used for classification with vision-language models apply the same linear prompt translation to each image making them less effective. To address these limitations, we propose ModPrompt, a visual prompt strategy to adapt vision-language detectors to new modalities without degrading zero-shot performance. In particular, an encoder-decoder visual prompt strategy is proposed, further enhanced by the integration of inference-friendly task residuals, facilitating more robust adaptation. Empirically, we benchmark our method for modality adaptation on two vision-language detectors, YOLO-World and Grounding DINO, and on challenging infrared (LLVIP, FLIR) and depth (NYUv2) data, achieving performance comparable to full fine-tuning while preserving the model's zero-shot capability. Our code is available at: https://github.com/heitorrapela/ModPrompt



### Explaining Object Detectors via Collective Contribution of Pixels
- **Arxiv ID**: http://arxiv.org/abs/2412.00666v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2412.00666v1)
- **Published**: 2024-12-01 04:04:32+00:00
- **Updated**: 2024-12-01 04:04:32+00:00
- **Authors**: Toshinori Yamauchi, Hiroshi Kera, Kazuhiko Kawamoto
- **Comment**: 11+14 pages, 15 figures, 8 tables
- **Journal**: None
- **Summary**: Visual explanations for object detectors are crucial for enhancing their reliability. Since object detectors identify and localize instances by assessing multiple features collectively, generating explanations that capture these collective contributions is critical. However, existing methods focus solely on individual pixel contributions, ignoring the collective contribution of multiple pixels. To address this, we proposed a method for object detectors that considers the collective contribution of multiple pixels. Our approach leverages game-theoretic concepts, specifically Shapley values and interactions, to provide explanations. These explanations cover both bounding box generation and class determination, considering both individual and collective pixel contributions. Extensive quantitative and qualitative experiments demonstrate that the proposed method more accurately identifies important regions in detection results compared to current state-of-the-art methods. The code will be publicly available soon.



### Refine3DNet: Scaling Precision in 3D Object Reconstruction from Multi-View RGB Images using Attention
- **Arxiv ID**: http://arxiv.org/abs/2412.00731v1
- **DOI**: 10.1145/3702250.3702292
- **Categories**: **cs.CV**, I.5
- **Links**: [PDF](http://arxiv.org/pdf/2412.00731v1)
- **Published**: 2024-12-01 08:53:39+00:00
- **Updated**: 2024-12-01 08:53:39+00:00
- **Authors**: Ajith Balakrishnan, Sreeja S, Linu Shine
- **Comment**: ICVGIP-2024, 8 pages
- **Journal**: None
- **Summary**: Generating 3D models from multi-view 2D RGB images has gained significant attention, extending the capabilities of technologies like Virtual Reality, Robotic Vision, and human-machine interaction. In this paper, we introduce a hybrid strategy combining CNNs and transformers, featuring a visual auto-encoder with self-attention mechanisms and a 3D refiner network, trained using a novel Joint Train Separate Optimization (JTSO) algorithm. Encoded features from unordered inputs are transformed into an enhanced feature map by the self-attention layer, decoded into an initial 3D volume, and further refined. Our network generates 3D voxels from single or multiple 2D images from arbitrary viewpoints. Performance evaluations using the ShapeNet datasets show that our approach, combined with JTSO, outperforms state-of-the-art techniques in single and multi-view 3D reconstruction, achieving the highest mean intersection over union (IOU) scores, surpassing other models by 4.2% in single-view reconstruction.



### DyMO: Training-Free Diffusion Model Alignment with Dynamic Multi-Objective Scheduling
- **Arxiv ID**: http://arxiv.org/abs/2412.00759v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2412.00759v2)
- **Published**: 2024-12-01 10:32:47+00:00
- **Updated**: 2024-12-03 04:00:09+00:00
- **Authors**: Xin Xie, Dong Gong
- **Comment**: None
- **Journal**: None
- **Summary**: Text-to-image diffusion model alignment is critical for improving the alignment between the generated images and human preferences. While training-based methods are constrained by high computational costs and dataset requirements, training-free alignment methods remain underexplored and are often limited by inaccurate guidance. We propose a plug-and-play training-free alignment method, DyMO, for aligning the generated images and human preferences during inference. Apart from text-aware human preference scores, we introduce a semantic alignment objective for enhancing the semantic alignment in the early stages of diffusion, relying on the fact that the attention maps are effective reflections of the semantics in noisy images. We propose dynamic scheduling of multiple objectives and intermediate recurrent steps to reflect the requirements at different steps. Experiments with diverse pre-trained diffusion models and metrics demonstrate the effectiveness and robustness of the proposed method.



### Explorations in Self-Supervised Learning: Dataset Composition Testing for Object Classification
- **Arxiv ID**: http://arxiv.org/abs/2412.00770v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2412.00770v1)
- **Published**: 2024-12-01 11:21:01+00:00
- **Updated**: 2024-12-01 11:21:01+00:00
- **Authors**: Raynor Kirkson E. Chavez, Kyle Gabriel M. Reynoso
- **Comment**: None
- **Journal**: None
- **Summary**: This paper investigates the impact of sampling and pretraining using datasets with different image characteristics on the performance of self-supervised learning (SSL) models for object classification. To do this, we sample two apartment datasets from the Omnidata platform based on modality, luminosity, image size, and camera field of view and use them to pretrain a SimCLR model. The encodings generated from the pretrained model are then transferred to a supervised Resnet-50 model for object classification. Through A/B testing, we find that depth pretrained models are more effective on low resolution images, while RGB pretrained models perform better on higher resolution images. We also discover that increasing the luminosity of training images can improve the performance of models on low resolution images without negatively affecting their performance on higher resolution images.



### LUIEO: A Lightweight Model for Integrating Underwater Image Enhancement and Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2412.07009v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2412.07009v1)
- **Published**: 2024-12-01 14:01:30+00:00
- **Updated**: 2024-12-01 14:01:30+00:00
- **Authors**: Bin Li, Li Li, Zhenwei Zhang, Yuping Duan
- **Comment**: None
- **Journal**: None
- **Summary**: Underwater optical images inevitably suffer from various degradation factors such as blurring, low contrast, and color distortion, which hinder the accuracy of object detection tasks. Due to the lack of paired underwater/clean images, most research methods adopt a strategy of first enhancing and then detecting, resulting in a lack of feature communication between the two learning tasks. On the other hand, due to the contradiction between the diverse degradation factors of underwater images and the limited number of samples, existing underwater enhancement methods are difficult to effectively enhance degraded images of unknown water bodies, thereby limiting the improvement of object detection accuracy. Therefore, most underwater target detection results are still displayed on degraded images, making it difficult to visually judge the correctness of the detection results. To address the above issues, this paper proposes a multi-task learning method that simultaneously enhances underwater images and improves detection accuracy. Compared with single-task learning, the integrated model allows for the dynamic adjustment of information communication and sharing between different tasks. Due to the fact that real underwater images can only provide annotated object labels, this paper introduces physical constraints to ensure that object detection tasks do not interfere with image enhancement tasks. Therefore, this article introduces a physical module to decompose underwater images into clean images, background light, and transmission images and uses a physical model to calculate underwater images for self-supervision. Numerical experiments demonstrate that the proposed model achieves satisfactory results in visual performance, object detection accuracy, and detection efficiency compared to state-of-the-art comparative methods.



### Particle-based 6D Object Pose Estimation from Point Clouds using Diffusion Models
- **Arxiv ID**: http://arxiv.org/abs/2412.00835v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2412.00835v1)
- **Published**: 2024-12-01 14:52:44+00:00
- **Updated**: 2024-12-01 14:52:44+00:00
- **Authors**: Christian Möller, Niklas Funk, Jan Peters
- **Comment**: None
- **Journal**: None
- **Summary**: Object pose estimation from a single view remains a challenging problem. In particular, partial observability, occlusions, and object symmetries eventually result in pose ambiguity. To account for this multimodality, this work proposes training a diffusion-based generative model for 6D object pose estimation. During inference, the trained generative model allows for sampling multiple particles, i.e., pose hypotheses. To distill this information into a single pose estimate, we propose two novel and effective pose selection strategies that do not require any additional training or computationally intensive operations. Moreover, while many existing methods for pose estimation primarily focus on the image domain and only incorporate depth information for final pose refinement, our model solely operates on point cloud data. The model thereby leverages recent advancements in point cloud processing and operates upon an SE(3)-equivariant latent space that forms the basis for the particle selection strategies and allows for improved inference times. Our thorough experimental results demonstrate the competitive performance of our approach on the Linemod dataset and showcase the effectiveness of our design choices. Code is available at https://github.com/zitronian/6DPoseDiffusion .



