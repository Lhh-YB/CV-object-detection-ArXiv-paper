# Arxiv Papers in cs.CV on 2024-04-25
### A Multi-objective Optimization Benchmark Test Suite for Real-time Semantic Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2404.16266v2
- **DOI**: 10.1145/3638530.3654389
- **Categories**: **cs.CV**, cs.NE
- **Links**: [PDF](http://arxiv.org/pdf/2404.16266v2)
- **Published**: 2024-04-25 00:30:03+00:00
- **Updated**: 2024-04-29 01:39:37+00:00
- **Authors**: Yifan Zhao, Zhenyu Liang, Zhichao Lu, Ran Cheng
- **Comment**: GECCO 2024
- **Journal**: None
- **Summary**: As one of the emerging challenges in Automated Machine Learning, the Hardware-aware Neural Architecture Search (HW-NAS) tasks can be treated as black-box multi-objective optimization problems (MOPs). An important application of HW-NAS is real-time semantic segmentation, which plays a pivotal role in autonomous driving scenarios. The HW-NAS for real-time semantic segmentation inherently needs to balance multiple optimization objectives, including model accuracy, inference speed, and hardware-specific considerations. Despite its importance, benchmarks have yet to be developed to frame such a challenging task as multi-objective optimization. To bridge the gap, we introduce a tailored streamline to transform the task of HW-NAS for real-time semantic segmentation into standard MOPs. Building upon the streamline, we present a benchmark test suite, CitySeg/MOP, comprising fifteen MOPs derived from the Cityscapes dataset. The CitySeg/MOP test suite is integrated into the EvoXBench platform to provide seamless interfaces with various programming languages (e.g., Python and MATLAB) for instant fitness evaluations. We comprehensively assessed the CitySeg/MOP test suite on various multi-objective evolutionary algorithms, showcasing its versatility and practicality. Source codes are available at https://github.com/EMI-Group/evoxbench.



### CFMW: Cross-modality Fusion Mamba for Multispectral Object Detection under Adverse Weather Conditions
- **Arxiv ID**: http://arxiv.org/abs/2404.16302v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM, cs.RO, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2404.16302v1)
- **Published**: 2024-04-25 02:54:11+00:00
- **Updated**: 2024-04-25 02:54:11+00:00
- **Authors**: Haoyuan Li, Qi Hu, You Yao, Kailun Yang, Peng Chen
- **Comment**: The dataset and source code will be made publicly available at
  https://github.com/lhy-zjut/CFMW
- **Journal**: None
- **Summary**: Cross-modality images that integrate visible-infrared spectra cues can provide richer complementary information for object detection. Despite this, existing visible-infrared object detection methods severely degrade in severe weather conditions. This failure stems from the pronounced sensitivity of visible images to environmental perturbations, such as rain, haze, and snow, which frequently cause false negatives and false positives in detection. To address this issue, we introduce a novel and challenging task, termed visible-infrared object detection under adverse weather conditions. To foster this task, we have constructed a new Severe Weather Visible-Infrared Dataset (SWVID) with diverse severe weather scenes. Furthermore, we introduce the Cross-modality Fusion Mamba with Weather-removal (CFMW) to augment detection accuracy in adverse weather conditions. Thanks to the proposed Weather Removal Diffusion Model (WRDM) and Cross-modality Fusion Mamba (CFM) modules, CFMW is able to mine more essential information of pedestrian features in cross-modality fusion, thus could transfer to other rarer scenarios with high efficiency and has adequate availability on those platforms with low computing power. To the best of our knowledge, this is the first study that targeted improvement and integrated both Diffusion and Mamba modules in cross-modality object detection, successfully expanding the practical application of this type of model with its higher accuracy and more advanced architecture. Extensive experiments on both well-recognized and self-created datasets conclusively demonstrate that our CFMW achieves state-of-the-art detection performance, surpassing existing benchmarks. The dataset and source code will be made publicly available at https://github.com/lhy-zjut/CFMW.



### Object criticality for safer navigation
- **Arxiv ID**: http://arxiv.org/abs/2406.10232v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2406.10232v1)
- **Published**: 2024-04-25 09:02:22+00:00
- **Updated**: 2024-04-25 09:02:22+00:00
- **Authors**: Andrea Ceccarelli, Leonardo Montecchi
- **Comment**: position paper with initial results
- **Journal**: None
- **Summary**: Object detection in autonomous driving consists in perceiving and locating instances of objects in multi-dimensional data, such as images or lidar scans. Very recently, multiple works are proposing to evaluate object detectors by measuring their ability to detect the objects that are most likely to interfere with the driving task. Detectors are then ranked according to their ability to detect the most relevant objects, rather than the highest number of objects. However there is little evidence so far that the relevance of predicted object may contribute to the safety and reliability improvement of the driving task. This position paper elaborates on a strategy, together with partial results, to i) configure and deploy object detectors that successfully extract knowledge on object relevance, and ii) use such knowledge to improve the trajectory planning task. We show that, given an object detector, filtering objects based on their relevance, in combination with the traditional confidence threshold, reduces the risk of missing relevant objects, decreases the likelihood of dangerous trajectories, and improves the quality of trajectories in general.



### COBRA -- COnfidence score Based on shape Regression Analysis for method-independent quality assessment of object pose estimation from single images
- **Arxiv ID**: http://arxiv.org/abs/2404.16471v6
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2404.16471v6)
- **Published**: 2024-04-25 09:55:35+00:00
- **Updated**: 2024-12-16 09:21:03+00:00
- **Authors**: Panagiotis Sapoutzoglou, Georgios Giapitzakis, Georgios Floros, George Terzakis, Maria Pateraki
- **Comment**: None
- **Journal**: None
- **Summary**: We propose a generic procedure for assessing 6D object pose estimates. Our approach relies on the evaluation of discrepancies in the geometry of the observed object, in particular its respective estimated back-projection in 3D, against a putative functional shape representation comprising mixtures of Gaussian Processes, that act as a template. Each Gaussian Process is trained to yield a fragment of the object's surface in a radial fashion with respect to designated reference points. We further define a pose confidence measure as the average probability of pixel back-projections in the Gaussian mixture. The goal of our experiments is two-fold. a) We demonstrate that our functional representation is sufficiently accurate as a shape template on which the probability of back-projected object points can be evaluated, and, b) we show that the resulting confidence scores based on these probabilities are indeed a consistent quality measure of pose.



### Commonsense Prototype for Outdoor Unsupervised 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2404.16493v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2404.16493v3)
- **Published**: 2024-04-25 10:38:33+00:00
- **Updated**: 2024-06-26 10:51:51+00:00
- **Authors**: Hai Wu, Shijia Zhao, Xun Huang, Chenglu Wen, Xin Li, Cheng Wang
- **Comment**: Accepted by CVPR 2024
- **Journal**: None
- **Summary**: The prevalent approaches of unsupervised 3D object detection follow cluster-based pseudo-label generation and iterative self-training processes. However, the challenge arises due to the sparsity of LiDAR scans, which leads to pseudo-labels with erroneous size and position, resulting in subpar detection performance. To tackle this problem, this paper introduces a Commonsense Prototype-based Detector, termed CPD, for unsupervised 3D object detection. CPD first constructs Commonsense Prototype (CProto) characterized by high-quality bounding box and dense points, based on commonsense intuition. Subsequently, CPD refines the low-quality pseudo-labels by leveraging the size prior from CProto. Furthermore, CPD enhances the detection accuracy of sparsely scanned objects by the geometric knowledge from CProto. CPD outperforms state-of-the-art unsupervised 3D detectors on Waymo Open Dataset (WOD), PandaSet, and KITTI datasets by a large margin. Besides, by training CPD on WOD and testing on KITTI, CPD attains 90.85% and 81.01% 3D Average Precision on easy and moderate car classes, respectively. These achievements position CPD in close proximity to fully supervised detectors, highlighting the significance of our method. The code will be available at https://github.com/hailanyi/CPD.



### Make-it-Real: Unleashing Large Multimodal Model for Painting 3D Objects with Realistic Materials
- **Arxiv ID**: http://arxiv.org/abs/2404.16829v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2404.16829v3)
- **Published**: 2024-04-25 17:59:58+00:00
- **Updated**: 2024-05-23 19:12:51+00:00
- **Authors**: Ye Fang, Zeyi Sun, Tong Wu, Jiaqi Wang, Ziwei Liu, Gordon Wetzstein, Dahua Lin
- **Comment**: Project Page: https://sunzey.github.io/Make-it-Real/
- **Journal**: None
- **Summary**: Physically realistic materials are pivotal in augmenting the realism of 3D assets across various applications and lighting conditions. However, existing 3D assets and generative models often lack authentic material properties. Manual assignment of materials using graphic software is a tedious and time-consuming task. In this paper, we exploit advancements in Multimodal Large Language Models (MLLMs), particularly GPT-4V, to present a novel approach, Make-it-Real: 1) We demonstrate that GPT-4V can effectively recognize and describe materials, allowing the construction of a detailed material library. 2) Utilizing a combination of visual cues and hierarchical text prompts, GPT-4V precisely identifies and aligns materials with the corresponding components of 3D objects. 3) The correctly matched materials are then meticulously applied as reference for the new SVBRDF material generation according to the original albedo map, significantly enhancing their visual authenticity. Make-it-Real offers a streamlined integration into the 3D content creation workflow, showcasing its utility as an essential tool for developers of 3D assets.



### Constellation Dataset: Benchmarking High-Altitude Object Detection for an Urban Intersection
- **Arxiv ID**: http://arxiv.org/abs/2404.16944v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2404.16944v1)
- **Published**: 2024-04-25 18:00:24+00:00
- **Updated**: 2024-04-25 18:00:24+00:00
- **Authors**: Mehmet Kerem Turkcan, Sanjeev Narasimhan, Chengbo Zang, Gyung Hyun Je, Bo Yu, Mahshid Ghasemi, Javad Ghaderi, Gil Zussman, Zoran Kostic
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce Constellation, a dataset of 13K images suitable for research on detection of objects in dense urban streetscapes observed from high-elevation cameras, collected for a variety of temporal conditions. The dataset addresses the need for curated data to explore problems in small object detection exemplified by the limited pixel footprint of pedestrians observed tens of meters from above. It enables the testing of object detection models for variations in lighting, building shadows, weather, and scene dynamics. We evaluate contemporary object detection architectures on the dataset, observing that state-of-the-art methods have lower performance in detecting small pedestrians compared to vehicles, corresponding to a 10% difference in average precision (AP). Using structurally similar datasets for pretraining the models results in an increase of 1.8% mean AP (mAP). We further find that incorporating domain-specific data augmentations helps improve model performance. Using pseudo-labeled data, obtained from inference outcomes of the best-performing models, improves the performance of the models. Finally, comparing the models trained using the data collected in two different time intervals, we find a performance drift in models due to the changes in intersection conditions over time. The best-performing model achieves a pedestrian AP of 92.0% with 11.5 ms inference time on NVIDIA A100 GPUs, and an mAP of 95.4%.



