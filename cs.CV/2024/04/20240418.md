# Arxiv Papers in cs.CV on 2024-04-18
### Simultaneous Detection and Interaction Reasoning for Object-Centric Action Recognition
- **Arxiv ID**: http://arxiv.org/abs/2404.11903v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2404.11903v1)
- **Published**: 2024-04-18 05:06:12+00:00
- **Updated**: 2024-04-18 05:06:12+00:00
- **Authors**: Xunsong Li, Pengzhan Sun, Yangcen Liu, Lixin Duan, Wen Li
- **Comment**: 12 pages, 5 figures, submitted to IEEE Transactions on Multimedia
- **Journal**: None
- **Summary**: The interactions between human and objects are important for recognizing object-centric actions. Existing methods usually adopt a two-stage pipeline, where object proposals are first detected using a pretrained detector, and then are fed to an action recognition model for extracting video features and learning the object relations for action recognition. However, since the action prior is unknown in the object detection stage, important objects could be easily overlooked, leading to inferior action recognition performance. In this paper, we propose an end-to-end object-centric action recognition framework that simultaneously performs Detection And Interaction Reasoning in one stage. Particularly, after extracting video features with a base network, we create three modules for concurrent object detection and interaction reasoning. First, a Patch-based Object Decoder generates proposals from video patch tokens. Then, an Interactive Object Refining and Aggregation identifies important objects for action recognition, adjusts proposal scores based on position and appearance, and aggregates object-level info into a global video representation. Lastly, an Object Relation Modeling module encodes object relations. These three modules together with the video feature extractor can be trained jointly in an end-to-end fashion, thus avoiding the heavy reliance on an off-the-shelf object detector, and reducing the multi-stage training burden. We conduct experiments on two datasets, Something-Else and Ikea-Assembly, to evaluate the performance of our proposed approach on conventional, compositional, and few-shot action recognition tasks. Through in-depth experimental analysis, we show the crucial role of interactive objects in learning for action recognition, and we can outperform state-of-the-art methods on both datasets.



### The devil is in the object boundary: towards annotation-free instance segmentation using Foundation Models
- **Arxiv ID**: http://arxiv.org/abs/2404.11957v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2404.11957v1)
- **Published**: 2024-04-18 07:22:38+00:00
- **Updated**: 2024-04-18 07:22:38+00:00
- **Authors**: Cheng Shi, Sibei Yang
- **Comment**: ICLR2024, Code is released at
  https://github.com/ChengShiest/Zip-Your-CLIP
- **Journal**: None
- **Summary**: Foundation models, pre-trained on a large amount of data have demonstrated impressive zero-shot capabilities in various downstream tasks. However, in object detection and instance segmentation, two fundamental computer vision tasks heavily reliant on extensive human annotations, foundation models such as SAM and DINO struggle to achieve satisfactory performance. In this study, we reveal that the devil is in the object boundary, \textit{i.e.}, these foundation models fail to discern boundaries between individual objects. For the first time, we probe that CLIP, which has never accessed any instance-level annotations, can provide a highly beneficial and strong instance-level boundary prior in the clustering results of its particular intermediate layer. Following this surprising observation, we propose $\textbf{Zip}$ which $\textbf{Z}$ips up CL$\textbf{ip}$ and SAM in a novel classification-first-then-discovery pipeline, enabling annotation-free, complex-scene-capable, open-vocabulary object detection and instance segmentation. Our Zip significantly boosts SAM's mask AP on COCO dataset by 12.5% and establishes state-of-the-art performance in various settings, including training-free, self-training, and label-efficient finetuning. Furthermore, annotation-free Zip even achieves comparable performance to the best-performing open-vocabulary object detecters using base annotations. Code is released at https://github.com/ChengShiest/Zip-Your-CLIP



### Food Portion Estimation via 3D Object Scaling
- **Arxiv ID**: http://arxiv.org/abs/2404.12257v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.MM, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2404.12257v2)
- **Published**: 2024-04-18 15:23:37+00:00
- **Updated**: 2024-10-10 20:02:07+00:00
- **Authors**: Gautham Vinod, Jiangpeng He, Zeman Shao, Fengqing Zhu
- **Comment**: None
- **Journal**: None
- **Summary**: Image-based methods to analyze food images have alleviated the user burden and biases associated with traditional methods. However, accurate portion estimation remains a major challenge due to the loss of 3D information in the 2D representation of foods captured by smartphone cameras or wearable devices. In this paper, we propose a new framework to estimate both food volume and energy from 2D images by leveraging the power of 3D food models and physical reference in the eating scene. Our method estimates the pose of the camera and the food object in the input image and recreates the eating occasion by rendering an image of a 3D model of the food with the estimated poses. We also introduce a new dataset, SimpleFood45, which contains 2D images of 45 food items and associated annotations including food volume, weight, and energy. Our method achieves an average error of 31.10 kCal (17.67%) on this dataset, outperforming existing portion estimation methods. The dataset can be accessed at: https://lorenz.ecn.purdue.edu/~gvinod/simplefood45/ and the code can be accessed at: https://gitlab.com/viper-purdue/monocular-food-volume-3d



### Customizing Text-to-Image Diffusion with Object Viewpoint Control
- **Arxiv ID**: http://arxiv.org/abs/2404.12333v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2404.12333v2)
- **Published**: 2024-04-18 16:59:51+00:00
- **Updated**: 2024-12-02 21:15:29+00:00
- **Authors**: Nupur Kumari, Grace Su, Richard Zhang, Taesung Park, Eli Shechtman, Jun-Yan Zhu
- **Comment**: project page: https://customdiffusion360.github.io
- **Journal**: None
- **Summary**: Model customization introduces new concepts to existing text-to-image models, enabling the generation of these new concepts/objects in novel contexts. However, such methods lack accurate camera view control with respect to the new object, and users must resort to prompt engineering (e.g., adding ``top-view'') to achieve coarse view control. In this work, we introduce a new task -- enabling explicit control of the object viewpoint in the customization of text-to-image diffusion models. This allows us to modify the custom object's properties and generate it in various background scenes via text prompts, all while incorporating the object viewpoint as an additional control. This new task presents significant challenges, as one must harmoniously merge a 3D representation from the multi-view images with the 2D pre-trained model. To bridge this gap, we propose to condition the diffusion process on the 3D object features rendered from the target viewpoint. During training, we fine-tune the 3D feature prediction modules to reconstruct the object's appearance and geometry, while reducing overfitting to the input multi-view images. Our method outperforms existing image editing and model customization baselines in preserving the custom object's identity while following the target object viewpoint and the text prompt.



### Inverse Neural Rendering for Explainable Multi-Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2404.12359v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2404.12359v1)
- **Published**: 2024-04-18 17:37:53+00:00
- **Updated**: 2024-04-18 17:37:53+00:00
- **Authors**: Julian Ost, Tanushree Banerjee, Mario Bijelic, Felix Heide
- **Comment**: None
- **Journal**: None
- **Summary**: Today, most methods for image understanding tasks rely on feed-forward neural networks. While this approach has allowed for empirical accuracy, efficiency, and task adaptation via fine-tuning, it also comes with fundamental disadvantages. Existing networks often struggle to generalize across different datasets, even on the same task. By design, these networks ultimately reason about high-dimensional scene features, which are challenging to analyze. This is true especially when attempting to predict 3D information based on 2D images. We propose to recast 3D multi-object tracking from RGB cameras as an \emph{Inverse Rendering (IR)} problem, by optimizing via a differentiable rendering pipeline over the latent space of pre-trained 3D object representations and retrieve the latents that best represent object instances in a given input image. To this end, we optimize an image loss over generative latent spaces that inherently disentangle shape and appearance properties. We investigate not only an alternate take on tracking but our method also enables examining the generated objects, reasoning about failure situations, and resolving ambiguous cases. We validate the generalization and scaling capabilities of our method by learning the generative prior exclusively from synthetic data and assessing camera-based 3D tracking on the nuScenes and Waymo datasets. Both these datasets are completely unseen to our method and do not require fine-tuning. Videos and code are available at https://light.princeton.edu/inverse-rendering-tracking/.



### G-HOP: Generative Hand-Object Prior for Interaction Reconstruction and Grasp Synthesis
- **Arxiv ID**: http://arxiv.org/abs/2404.12383v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2404.12383v1)
- **Published**: 2024-04-18 17:59:28+00:00
- **Updated**: 2024-04-18 17:59:28+00:00
- **Authors**: Yufei Ye, Abhinav Gupta, Kris Kitani, Shubham Tulsiani
- **Comment**: accepted to CVPR2024; project page at
  https://judyye.github.io/ghop-www
- **Journal**: None
- **Summary**: We propose G-HOP, a denoising diffusion based generative prior for hand-object interactions that allows modeling both the 3D object and a human hand, conditioned on the object category. To learn a 3D spatial diffusion model that can capture this joint distribution, we represent the human hand via a skeletal distance field to obtain a representation aligned with the (latent) signed distance field for the object. We show that this hand-object prior can then serve as generic guidance to facilitate other tasks like reconstruction from interaction clip and human grasp synthesis. We believe that our model, trained by aggregating seven diverse real-world interaction datasets spanning across 155 categories, represents a first approach that allows jointly generating both hand and object. Our empirical evaluations demonstrate the benefit of this joint prior in video-based reconstruction and human grasp synthesis, outperforming current task-specific baselines.   Project website: https://judyye.github.io/ghop-www



### Moving Object Segmentation: All You Need Is SAM (and Flow)
- **Arxiv ID**: http://arxiv.org/abs/2404.12389v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2404.12389v2)
- **Published**: 2024-04-18 17:59:53+00:00
- **Updated**: 2024-11-21 20:28:33+00:00
- **Authors**: Junyu Xie, Charig Yang, Weidi Xie, Andrew Zisserman
- **Comment**: Project Page: https://www.robots.ox.ac.uk/~vgg/research/flowsam/
- **Journal**: None
- **Summary**: The objective of this paper is motion segmentation -- discovering and segmenting the moving objects in a video. This is a much studied area with numerous careful, and sometimes complex, approaches and training schemes including: self-supervised learning, learning from synthetic datasets, object-centric representations, amodal representations, and many more. Our interest in this paper is to determine if the Segment Anything model (SAM) can contribute to this task. We investigate two models for combining SAM with optical flow that harness the segmentation power of SAM with the ability of flow to discover and group moving objects. In the first model, we adapt SAM to take optical flow, rather than RGB, as an input. In the second, SAM takes RGB as an input, and flow is used as a segmentation prompt. These surprisingly simple methods, without any further modifications, outperform all previous approaches by a considerable margin in both single and multi-object benchmarks. We also extend these frame-level segmentations to sequence-level segmentations that maintain object identity. Again, this simple model achieves outstanding performance across multiple moving object segmentation benchmarks.



### Spot-Compose: A Framework for Open-Vocabulary Object Retrieval and Drawer Manipulation in Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2404.12440v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, I.2.9; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2404.12440v1)
- **Published**: 2024-04-18 18:01:15+00:00
- **Updated**: 2024-04-18 18:01:15+00:00
- **Authors**: Oliver Lemke, Zuria Bauer, René Zurbrügg, Marc Pollefeys, Francis Engelmann, Hermann Blum
- **Comment**: Accepted at ICRA 2024 Workshops. Code and videos available at
  https://spot-compose.github.io/
- **Journal**: None
- **Summary**: In recent years, modern techniques in deep learning and large-scale datasets have led to impressive progress in 3D instance segmentation, grasp pose estimation, and robotics. This allows for accurate detection directly in 3D scenes, object- and environment-aware grasp prediction, as well as robust and repeatable robotic manipulation. This work aims to integrate these recent methods into a comprehensive framework for robotic interaction and manipulation in human-centric environments. Specifically, we leverage 3D reconstructions from a commodity 3D scanner for open-vocabulary instance segmentation, alongside grasp pose estimation, to demonstrate dynamic picking of objects, and opening of drawers. We show the performance and robustness of our model in two sets of real-world experiments including dynamic object retrieval and drawer opening, reporting a 51% and 82% success rate respectively. Code of our framework as well as videos are available on: https://spot-compose.github.io/.



### DoughNet: A Visual Predictive Model for Topological Manipulation of Deformable Objects
- **Arxiv ID**: http://arxiv.org/abs/2404.12524v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2404.12524v1)
- **Published**: 2024-04-18 21:55:23+00:00
- **Updated**: 2024-04-18 21:55:23+00:00
- **Authors**: Dominik Bauer, Zhenjia Xu, Shuran Song
- **Comment**: Under review. 17 pages, 14 figures
- **Journal**: None
- **Summary**: Manipulation of elastoplastic objects like dough often involves topological changes such as splitting and merging. The ability to accurately predict these topological changes that a specific action might incur is critical for planning interactions with elastoplastic objects. We present DoughNet, a Transformer-based architecture for handling these challenges, consisting of two components. First, a denoising autoencoder represents deformable objects of varying topology as sets of latent codes. Second, a visual predictive model performs autoregressive set prediction to determine long-horizon geometrical deformation and topological changes purely in latent space. Given a partial initial state and desired manipulation trajectories, it infers all resulting object geometries and topologies at each step. DoughNet thereby allows to plan robotic manipulation; selecting a suited tool, its pose and opening width to recreate robot- or human-made goals. Our experiments in simulated and real environments show that DoughNet is able to significantly outperform related approaches that consider deformation only as geometrical change.



