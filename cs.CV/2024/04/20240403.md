# Arxiv Papers in cs.CV on 2024-04-03
### Representation Alignment Contrastive Regularization for Multi-Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2404.02562v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2404.02562v2)
- **Published**: 2024-04-03 08:33:08+00:00
- **Updated**: 2024-04-17 07:13:27+00:00
- **Authors**: Zhonglin Liu, Shujie Chen, Jianfeng Dong, Xun Wang, Di Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Achieving high-performance in multi-object tracking algorithms heavily relies on modeling spatio-temporal relationships during the data association stage. Mainstream approaches encompass rule-based and deep learning-based methods for spatio-temporal relationship modeling. While the former relies on physical motion laws, offering wider applicability but yielding suboptimal results for complex object movements, the latter, though achieving high-performance, lacks interpretability and involves complex module designs. This work aims to simplify deep learning-based spatio-temporal relationship models and introduce interpretability into features for data association. Specifically, a lightweight single-layer transformer encoder is utilized to model spatio-temporal relationships. To make features more interpretative, two contrastive regularization losses based on representation alignment are proposed, derived from spatio-temporal consistency rules. By applying weighted summation to affinity matrices, the aligned features can seamlessly integrate into the data association stage of the original tracking workflow. Experimental results showcase that our model enhances the majority of existing tracking networks' performance without excessive complexity, with minimal increase in training overhead and nearly negligible computational and storage costs.



### Independently Keypoint Learning for Small Object Semantic Correspondence
- **Arxiv ID**: http://arxiv.org/abs/2404.02678v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2404.02678v1)
- **Published**: 2024-04-03 12:21:41+00:00
- **Updated**: 2024-04-03 12:21:41+00:00
- **Authors**: Hailong Jin, Huiying Li
- **Comment**: None
- **Journal**: None
- **Summary**: Semantic correspondence remains a challenging task for establishing correspondences between a pair of images with the same category or similar scenes due to the large intra-class appearance. In this paper, we introduce a novel problem called 'Small Object Semantic Correspondence (SOSC).' This problem is challenging due to the close proximity of keypoints associated with small objects, which results in the fusion of these respective features. It is difficult to identify the corresponding key points of the fused features, and it is also difficult to be recognized. To address this challenge, we propose the Keypoint Bounding box-centered Cropping (KBC) method, which aims to increase the spatial separation between keypoints of small objects, thereby facilitating independent learning of these keypoints. The KBC method is seamlessly integrated into our proposed inference pipeline and can be easily incorporated into other methodologies, resulting in significant performance enhancements. Additionally, we introduce a novel framework, named KBCNet, which serves as our baseline model. KBCNet comprises a Cross-Scale Feature Alignment (CSFA) module and an efficient 4D convolutional decoder. The CSFA module is designed to align multi-scale features, enriching keypoint representations by integrating fine-grained features and deep semantic features. Meanwhile, the 4D convolutional decoder, based on efficient 4D convolution, ensures efficiency and rapid convergence. To empirically validate the effectiveness of our proposed methodology, extensive experiments are conducted on three widely used benchmarks: PF-PASCAL, PF-WILLOW, and SPair-71k. Our KBC method demonstrates a substantial performance improvement of 7.5\% on the SPair-71K dataset, providing compelling evidence of its efficacy.



### DPFT: Dual Perspective Fusion Transformer for Camera-Radar-based Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2404.03015v2
- **DOI**: 10.1109/TIV.2024.3507538
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2404.03015v2)
- **Published**: 2024-04-03 18:54:27+00:00
- **Updated**: 2024-11-27 16:50:46+00:00
- **Authors**: Felix Fent, Andras Palffy, Holger Caesar
- **Comment**: Accepted to IEEE Transactions on Intelligent Vehicles
- **Journal**: None
- **Summary**: The perception of autonomous vehicles has to be efficient, robust, and cost-effective. However, cameras are not robust against severe weather conditions, lidar sensors are expensive, and the performance of radar-based perception is still inferior to the others. Camera-radar fusion methods have been proposed to address this issue, but these are constrained by the typical sparsity of radar point clouds and often designed for radars without elevation information. We propose a novel camera-radar fusion approach called Dual Perspective Fusion Transformer (DPFT), designed to overcome these limitations. Our method leverages lower-level radar data (the radar cube) instead of the processed point clouds to preserve as much information as possible and employs projections in both the camera and ground planes to effectively use radars with elevation information and simplify the fusion with camera data. As a result, DPFT has demonstrated state-of-the-art performance on the K-Radar dataset while showing remarkable robustness against adverse weather conditions and maintaining a low inference time. The code is made available as open-source software under https://github.com/TUMFTM/DPFT.



### Linear Anchored Gaussian Mixture Model for Location and Width Computations of Objects in Thick Line Shape
- **Arxiv ID**: http://arxiv.org/abs/2404.03043v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2404.03043v3)
- **Published**: 2024-04-03 20:05:00+00:00
- **Updated**: 2024-05-15 01:13:34+00:00
- **Authors**: Nafaa Nacereddine, Aicha Baya Goumeidane, Djemel Ziou
- **Comment**: 23 pages, 13 figures
- **Journal**: None
- **Summary**: Accurate detection of the centerline of a thick linear structure and good estimation of its thickness are challenging topics in many real-world applications such X-ray imaging, remote sensing and lane marking detection in road traffic. Model-based approaches using Hough and Radon transforms are often used but, are not recommended for thick line detection, whereas methods based on image derivatives need further step-by-step processing making their efficiency dependent on each step outcome. In this paper, a novel paradigm to better detect thick linear objects is presented, where the 3D image gray level representation is considered as a finite mixture model of a statistical distribution, called linear anchored Gaussian distribution and parametrized by a scale factor to describe the structure thickness and radius and angle parameters to localize the structure centerline. Expectation-Maximization algorithm (Algo1) using the original image as input data is used to estimate the model parameters. To rid the data of irrelevant information brought by nonuniform and noisy background, a modified EM algorithm (Algo2) is detailed. In Experiments, the proposed algorithms show promising results on real-world images and synthetic images corrupted by blur and noise, where Algo2, using Hessian-based angle initialization, outperforms Algo1 and Algo2 with random angle initialization, in terms of running time and structure location and thickness computation accuracy.



### Ego-Motion Aware Target Prediction Module for Robust Multi-Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2404.03110v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.9; I.2.10; I.2.9
- **Links**: [PDF](http://arxiv.org/pdf/2404.03110v1)
- **Published**: 2024-04-03 23:24:25+00:00
- **Updated**: 2024-04-03 23:24:25+00:00
- **Authors**: Navid Mahdian, Mohammad Jani, Amir M. Soufi Enayati, Homayoun Najjaran
- **Comment**: 7 pages, 4 figures, submitted to IROS2024
- **Journal**: None
- **Summary**: Multi-object tracking (MOT) is a prominent task in computer vision with application in autonomous driving, responsible for the simultaneous tracking of multiple object trajectories. Detection-based multi-object tracking (DBT) algorithms detect objects using an independent object detector and predict the imminent location of each target. Conventional prediction methods in DBT utilize Kalman Filter(KF) to extrapolate the target location in the upcoming frames by supposing a constant velocity motion model. These methods are especially hindered in autonomous driving applications due to dramatic camera motion or unavailable detections. Such limitations lead to tracking failures manifested by numerous identity switches and disrupted trajectories. In this paper, we introduce a novel KF-based prediction module called the Ego-motion Aware Target Prediction (EMAP) module by focusing on the integration of camera motion and depth information with object motion models. Our proposed method decouples the impact of camera rotational and translational velocity from the object trajectories by reformulating the Kalman Filter. This reformulation enables us to reject the disturbances caused by camera motion and maximizes the reliability of the object motion model. We integrate our module with four state-of-the-art base MOT algorithms, namely OC-SORT, Deep OC-SORT, ByteTrack, and BoT-SORT. In particular, our evaluation on the KITTI MOT dataset demonstrates that EMAP remarkably drops the number of identity switches (IDSW) of OC-SORT and Deep OC-SORT by 73% and 21%, respectively. At the same time, it elevates other performance metrics such as HOTA by more than 5%. Our source code is available at https://github.com/noyzzz/EMAP.



