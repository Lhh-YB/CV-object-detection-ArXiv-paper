# Arxiv Papers in cs.CV on 2024-04-01
### TSOM: Small Object Motion Detection Neural Network Inspired by Avian Visual Circuit
- **Arxiv ID**: http://arxiv.org/abs/2404.00855v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2404.00855v1)
- **Published**: 2024-04-01 01:49:08+00:00
- **Updated**: 2024-04-01 01:49:08+00:00
- **Authors**: Pignge Hu, Xiaoteng Zhang, Mengmeng Li, Yingjie Zhu, Li Shi
- **Comment**: None
- **Journal**: None
- **Summary**: Detecting small moving objects in complex backgrounds from an overhead perspective is a highly challenging task for machine vision systems. As an inspiration from nature, the avian visual system is capable of processing motion information in various complex aerial scenes, and its Retina-OT-Rt visual circuit is highly sensitive to capturing the motion information of small objects from high altitudes. However, more needs to be done on small object motion detection algorithms based on the avian visual system. In this paper, we conducted mathematical modeling based on extensive studies of the biological mechanisms of the Retina-OT-Rt visual circuit. Based on this, we proposed a novel tectum small object motion detection neural network (TSOM). The neural network includes the retina, SGC dendritic, SGC Soma, and Rt layers, each layer corresponding to neurons in the visual pathway. The Retina layer is responsible for accurately projecting input content, the SGC dendritic layer perceives and encodes spatial-temporal information, the SGC Soma layer computes complex motion information and extracts small objects, and the Rt layer integrates and decodes motion information from multiple directions to determine the position of small objects. Extensive experiments on pigeon neurophysiological experiments and image sequence data showed that the TSOM is biologically interpretable and effective in extracting reliable small object motion features from complex high-altitude backgrounds.



### Open-Vocabulary Object Detectors: Robustness Challenges under Distribution Shifts
- **Arxiv ID**: http://arxiv.org/abs/2405.14874v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2405.14874v4)
- **Published**: 2024-04-01 14:18:15+00:00
- **Updated**: 2024-09-06 15:11:19+00:00
- **Authors**: Prakash Chandra Chhipa, Kanjar De, Meenakshi Subhash Chippa, Rajkumar Saini, Marcus Liwicki
- **Comment**: Accepted at 2024 European Conference on Computer Vision Workshops
  (ECCVW). Project page -
  https://prakashchhipa.github.io/projects/ovod_robustness
- **Journal**: None
- **Summary**: The challenge of Out-Of-Distribution (OOD) robustness remains a critical hurdle towards deploying deep vision models. Vision-Language Models (VLMs) have recently achieved groundbreaking results. VLM-based open-vocabulary object detection extends the capabilities of traditional object detection frameworks, enabling the recognition and classification of objects beyond predefined categories. Investigating OOD robustness in recent open-vocabulary object detection is essential to increase the trustworthiness of these models. This study presents a comprehensive robustness evaluation of the zero-shot capabilities of three recent open-vocabulary (OV) foundation object detection models: OWL-ViT, YOLO World, and Grounding DINO. Experiments carried out on the robustness benchmarks COCO-O, COCO-DC, and COCO-C encompassing distribution shifts due to information loss, corruption, adversarial attacks, and geometrical deformation, highlighting the challenges of the model's robustness to foster the research for achieving robustness. Project page: https://prakashchhipa.github.io/projects/ovod_robustness



### Detect2Interact: Localizing Object Key Field in Visual Question Answering (VQA) with LLMs
- **Arxiv ID**: http://arxiv.org/abs/2404.01151v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2404.01151v1)
- **Published**: 2024-04-01 14:53:36+00:00
- **Updated**: 2024-04-01 14:53:36+00:00
- **Authors**: Jialou Wang, Manli Zhu, Yulei Li, Honglei Li, Longzhi Yang, Wai Lok Woo
- **Comment**: Accepted to IEEE Intelligent Systems
- **Journal**: None
- **Summary**: Localization plays a crucial role in enhancing the practicality and precision of VQA systems. By enabling fine-grained identification and interaction with specific parts of an object, it significantly improves the system's ability to provide contextually relevant and spatially accurate responses, crucial for applications in dynamic environments like robotics and augmented reality. However, traditional systems face challenges in accurately mapping objects within images to generate nuanced and spatially aware responses. In this work, we introduce "Detect2Interact", which addresses these challenges by introducing an advanced approach for fine-grained object visual key field detection. First, we use the segment anything model (SAM) to generate detailed spatial maps of objects in images. Next, we use Vision Studio to extract semantic object descriptions. Third, we employ GPT-4's common sense knowledge, bridging the gap between an object's semantics and its spatial map. As a result, Detect2Interact achieves consistent qualitative results on object key field detection across extensive test cases and outperforms the existing VQA system with object detection by providing a more reasonable and finer visual representation.



### Entity-Centric Reinforcement Learning for Object Manipulation from Pixels
- **Arxiv ID**: http://arxiv.org/abs/2404.01220v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2404.01220v1)
- **Published**: 2024-04-01 16:25:08+00:00
- **Updated**: 2024-04-01 16:25:08+00:00
- **Authors**: Dan Haramati, Tal Daniel, Aviv Tamar
- **Comment**: ICLR 2024 Spotlight. Videos and code are available on the project
  website: https://sites.google.com/view/entity-centric-rl
- **Journal**: None
- **Summary**: Manipulating objects is a hallmark of human intelligence, and an important task in domains such as robotics. In principle, Reinforcement Learning (RL) offers a general approach to learn object manipulation. In practice, however, domains with more than a few objects are difficult for RL agents due to the curse of dimensionality, especially when learning from raw image observations. In this work we propose a structured approach for visual RL that is suitable for representing multiple objects and their interaction, and use it to learn goal-conditioned manipulation of several objects. Key to our method is the ability to handle goals with dependencies between the objects (e.g., moving objects in a certain order). We further relate our architecture to the generalization capability of the trained agent, based on a theoretical result for compositional generalization, and demonstrate agents that learn with 3 objects but generalize to similar tasks with over 10 objects. Videos and code are available on the project website: https://sites.google.com/view/entity-centric-rl



### Object-conditioned Bag of Instances for Few-Shot Personalized Instance Recognition
- **Arxiv ID**: http://arxiv.org/abs/2404.01397v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2404.01397v1)
- **Published**: 2024-04-01 18:08:58+00:00
- **Updated**: 2024-04-01 18:08:58+00:00
- **Authors**: Umberto Michieli, Jijoong Moon, Daehyun Kim, Mete Ozay
- **Comment**: ICASSP 2024. Copyright 2024 IEEE. Personal use of this material is
  permitted. Permission from IEEE must be obtained for all other uses, in any
  current or future media, including reprinting/republishing this material for
  advertising or promotional purposes, creating new collective works, for
  resale or redistribution to servers or lists, or reuse of any copyrighted
  component of this work in other
- **Journal**: None
- **Summary**: Nowadays, users demand for increased personalization of vision systems to localize and identify personal instances of objects (e.g., my dog rather than dog) from a few-shot dataset only. Despite outstanding results of deep networks on classical label-abundant benchmarks (e.g., those of the latest YOLOv8 model for standard object detection), they struggle to maintain within-class variability to represent different instances rather than object categories only. We construct an Object-conditioned Bag of Instances (OBoI) based on multi-order statistics of extracted features, where generic object detection models are extended to search and identify personal instances from the OBoI's metric space, without need for backpropagation. By relying on multi-order statistics, OBoI achieves consistent superior accuracy in distinguishing different instances. In the results, we achieve 77.1% personal object recognition accuracy in case of 18 personal instances, showing about 12% relative gain over the state of the art.



### ContactHandover: Contact-Guided Robot-to-Human Object Handover
- **Arxiv ID**: http://arxiv.org/abs/2404.01402v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2404.01402v2)
- **Published**: 2024-04-01 18:12:09+00:00
- **Updated**: 2024-09-30 07:34:17+00:00
- **Authors**: Zixi Wang, Zeyi Liu, Nicolas Ouporov, Shuran Song
- **Comment**: Accepted to IROS 2024. Project website:
  https://clairezixiwang.github.io/ContactHandover.github.io/
- **Journal**: None
- **Summary**: Robot-to-human object handover is an important step in many human robot collaboration tasks. A successful handover requires the robot to maintain a stable grasp on the object while making sure the human receives the object in a natural and easy-to-use manner. We propose ContactHandover, a robot to human handover system that consists of two phases: a contact-guided grasping phase and an object delivery phase. During the grasping phase, ContactHandover predicts both 6-DoF robot grasp poses and a 3D affordance map of human contact points on the object. The robot grasp poses are re-ranked by penalizing those that block human contact points, and the robot executes the highest ranking grasp. During the delivery phase, the robot end effector pose is computed by maximizing human contact points close to the human while minimizing the human arm joint torques and displacements. We evaluate our system on 27 diverse household objects and show that our system achieves better visibility and reachability of human contacts to the receiver compared to several baselines. More results can be found on https://clairezixiwang.github.io/ContactHandover.github.io



### The Radar Ghost Dataset -- An Evaluation of Ghost Objects in Automotive Radar Data
- **Arxiv ID**: http://arxiv.org/abs/2404.01437v1
- **DOI**: 10.1109/IROS51168.2021.9636338
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2404.01437v1)
- **Published**: 2024-04-01 19:20:32+00:00
- **Updated**: 2024-04-01 19:20:32+00:00
- **Authors**: Florian Kraus, Nicolas Scheiner, Werner Ritter, Klaus Dietmayer
- **Comment**: None
- **Journal**: IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS), 2021, pp. 8570-8577
- **Summary**: Radar sensors have a long tradition in advanced driver assistance systems (ADAS) and also play a major role in current concepts for autonomous vehicles. Their importance is reasoned by their high robustness against meteorological effects, such as rain, snow, or fog, and the radar's ability to measure relative radial velocity differences via the Doppler effect. The cause for these advantages, namely the large wavelength, is also one of the drawbacks of radar sensors. Compared to camera or lidar sensor, a lot more surfaces in a typical traffic scenario appear flat relative to the radar's emitted signal. This results in multi-path reflections or so called ghost detections in the radar signal. Ghost objects pose a major source for potential false positive detections in a vehicle's perception pipeline. Therefore, it is important to be able to segregate multi-path reflections from direct ones. In this article, we present a dataset with detailed manual annotations for different kinds of ghost detections. Moreover, two different approaches for identifying these kinds of objects are evaluated. We hope that our dataset encourages more researchers to engage in the fields of multi-path object suppression or exploitation.



### Neural Implicit Representation for Building Digital Twins of Unknown Articulated Objects
- **Arxiv ID**: http://arxiv.org/abs/2404.01440v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2404.01440v2)
- **Published**: 2024-04-01 19:23:00+00:00
- **Updated**: 2024-06-06 23:20:55+00:00
- **Authors**: Yijia Weng, Bowen Wen, Jonathan Tremblay, Valts Blukis, Dieter Fox, Leonidas Guibas, Stan Birchfield
- **Comment**: CVPR 2024
- **Journal**: None
- **Summary**: We address the problem of building digital twins of unknown articulated objects from two RGBD scans of the object at different articulation states. We decompose the problem into two stages, each addressing distinct aspects. Our method first reconstructs object-level shape at each state, then recovers the underlying articulation model including part segmentation and joint articulations that associate the two states. By explicitly modeling point-level correspondences and exploiting cues from images, 3D reconstructions, and kinematics, our method yields more accurate and stable results compared to prior work. It also handles more than one movable part and does not rely on any object shape or structure priors. Project page: https://github.com/NVlabs/DigitalTwinArt



### Modality Translation for Object Detection Adaptation Without Forgetting Prior Knowledge
- **Arxiv ID**: http://arxiv.org/abs/2404.01492v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2404.01492v3)
- **Published**: 2024-04-01 21:28:50+00:00
- **Updated**: 2024-07-31 21:50:57+00:00
- **Authors**: Heitor Rapela Medeiros, Masih Aminbeidokhti, Fidel Guerrero Pena, David Latortue, Eric Granger, Marco Pedersoli
- **Comment**: ECCV 2024: European Conference on Computer Vision, Milan Italy
- **Journal**: None
- **Summary**: A common practice in deep learning involves training large neural networks on massive datasets to achieve high accuracy across various domains and tasks. While this approach works well in many application areas, it often fails drastically when processing data from a new modality with a significant distribution shift from the data used to pre-train the model. This paper focuses on adapting a large object detection model trained on RGB images to new data extracted from IR images with a substantial modality shift. We propose Modality Translator (ModTr) as an alternative to the common approach of fine-tuning a large model to the new modality. ModTr adapts the IR input image with a small transformation network trained to directly minimize the detection loss. The original RGB model can then work on the translated inputs without any further changes or fine-tuning to its parameters. Experimental results on translating from IR to RGB images on two well-known datasets show that our simple approach provides detectors that perform comparably or better than standard fine-tuning, without forgetting the knowledge of the original model. This opens the door to a more flexible and efficient service-based detection pipeline, where a unique and unaltered server, such as an RGB detector, runs constantly while being queried by different modalities, such as IR with the corresponding translations model. Our code is available at: https://github.com/heitorrapela/ModTr.



