# Arxiv Papers in cs.CV on 2024-09-15
### GLCONet: Learning Multi-source Perception Representation for Camouflaged Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2409.09588v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2409.09588v1)
- **Published**: 2024-09-15 02:26:17+00:00
- **Updated**: 2024-09-15 02:26:17+00:00
- **Authors**: Yanguang Sun, Hanyu Xuan, Jian Yang, Lei Luo
- **Comment**: Accepted at TNNLS 2024
- **Journal**: None
- **Summary**: Recently, biological perception has been a powerful tool for handling the camouflaged object detection (COD) task. However, most existing methods are heavily dependent on the local spatial information of diverse scales from convolutional operations to optimize initial features. A commonly neglected point in these methods is the long-range dependencies between feature pixels from different scale spaces that can help the model build a global structure of the object, inducing a more precise image representation. In this paper, we propose a novel Global-Local Collaborative Optimization Network, called GLCONet. Technically, we first design a collaborative optimization strategy from the perspective of multi-source perception to simultaneously model the local details and global long-range relationships, which can provide features with abundant discriminative information to boost the accuracy in detecting camouflaged objects. Furthermore, we introduce an adjacent reverse decoder that contains cross-layer aggregation and reverse optimization to integrate complementary information from different levels for generating high-quality representations. Extensive experiments demonstrate that the proposed GLCONet method with different backbones can effectively activate potentially significant pixels in an image, outperforming twenty state-of-the-art methods on three public COD datasets. The source code is available at: \https://github.com/CSYSI/GLCONet.



### Enhancing Weakly-Supervised Object Detection on Static Images through (Hallucinated) Motion
- **Arxiv ID**: http://arxiv.org/abs/2409.09616v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2409.09616v1)
- **Published**: 2024-09-15 05:32:48+00:00
- **Updated**: 2024-09-15 05:32:48+00:00
- **Authors**: Cagri Gungor, Adriana Kovashka
- **Comment**: None
- **Journal**: None
- **Summary**: While motion has garnered attention in various tasks, its potential as a modality for weakly-supervised object detection (WSOD) in static images remains unexplored. Our study introduces an approach to enhance WSOD methods by integrating motion information. This method involves leveraging hallucinated motion from static images to improve WSOD on image datasets, utilizing a Siamese network for enhanced representation learning with motion, addressing camera motion through motion normalization, and selectively training images based on object motion. Experimental validation on the COCO and YouTube-BB datasets demonstrates improvements over a state-of-the-art method.



### Reasoning Paths with Reference Objects Elicit Quantitative Spatial Reasoning in Large Vision-Language Models
- **Arxiv ID**: http://arxiv.org/abs/2409.09788v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2409.09788v1)
- **Published**: 2024-09-15 16:45:42+00:00
- **Updated**: 2024-09-15 16:45:42+00:00
- **Authors**: Yuan-Hong Liao, Rafid Mahmood, Sanja Fidler, David Acuna
- **Comment**: 20 pages, 13 figures
- **Journal**: None
- **Summary**: Despite recent advances demonstrating vision-language models' (VLMs) abilities to describe complex relationships in images using natural language, their capability to quantitatively reason about object sizes and distances remains underexplored. In this work, we introduce a manually annotated benchmark, Q-Spatial Bench, with 271 questions across five categories designed for quantitative spatial reasoning and systematically investigate the performance of state-of-the-art VLMs on this task. Our analysis reveals that reasoning about distances between objects is particularly challenging for SoTA VLMs; however, some VLMs significantly outperform others, with an over 40-point gap between the two best performing models. We also make the surprising observation that the success rate of the top-performing VLM increases by 19 points when a reasoning path using a reference object emerges naturally in the response. Inspired by this observation, we develop a zero-shot prompting technique, SpatialPrompt, that encourages VLMs to answer quantitative spatial questions using reference objects as visual cues. By instructing VLMs to use reference objects in their reasoning paths via SpatialPrompt, Gemini 1.5 Pro, Gemini 1.5 Flash, and GPT-4V improve their success rates by over 40, 20, and 30 points, respectively. We emphasize that these significant improvements are obtained without needing more data, model architectural modifications, or fine-tuning.



### NARF24: Estimating Articulated Object Structure for Implicit Rendering
- **Arxiv ID**: http://arxiv.org/abs/2409.09829v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2409.09829v1)
- **Published**: 2024-09-15 19:06:46+00:00
- **Updated**: 2024-09-15 19:06:46+00:00
- **Authors**: Stanley Lewis, Tom Gao, Odest Chadwicke Jenkins
- **Comment**: extended abstract as submitted to ICRA@40 anniversary conference
- **Journal**: None
- **Summary**: Articulated objects and their representations pose a difficult problem for robots. These objects require not only representations of geometry and texture, but also of the various connections and joint parameters that make up each articulation. We propose a method that learns a common Neural Radiance Field (NeRF) representation across a small number of collected scenes. This representation is combined with a parts-based image segmentation to produce an implicit space part localization, from which the connectivity and joint parameters of the articulated object can be estimated, thus enabling configuration-conditioned rendering.



