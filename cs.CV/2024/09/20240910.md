# Arxiv Papers in cs.CV on 2024-09-10
### A Likelihood Ratio-Based Approach to Segmenting Unknown Objects
- **Arxiv ID**: http://arxiv.org/abs/2409.06424v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2409.06424v1)
- **Published**: 2024-09-10 11:10:32+00:00
- **Updated**: 2024-09-10 11:10:32+00:00
- **Authors**: Nazir Nayal, Youssef Shoeb, Fatma GÃ¼ney
- **Comment**: 13 pages, 2 figures, and 4 tables
- **Journal**: None
- **Summary**: Addressing the Out-of-Distribution (OoD) segmentation task is a prerequisite for perception systems operating in an open-world environment. Large foundational models are frequently used in downstream tasks, however, their potential for OoD remains mostly unexplored. We seek to leverage a large foundational model to achieve robust representation. Outlier supervision is a widely used strategy for improving OoD detection of the existing segmentation networks. However, current approaches for outlier supervision involve retraining parts of the original network, which is typically disruptive to the model's learned feature representation. Furthermore, retraining becomes infeasible in the case of large foundational models. Our goal is to retrain for outlier segmentation without compromising the strong representation space of the foundational model. To this end, we propose an adaptive, lightweight unknown estimation module (UEM) for outlier supervision that significantly enhances the OoD segmentation performance without affecting the learned feature representation of the original network. UEM learns a distribution for outliers and a generic distribution for known classes. Using the learned distributions, we propose a likelihood-ratio-based outlier scoring function that fuses the confidence of UEM with that of the pixel-wise segmentation inlier network to detect unknown objects. We also propose an objective to optimize this score directly. Our approach achieves a new state-of-the-art across multiple datasets, outperforming the previous best method by 5.74% average precision points while having a lower false-positive rate. Importantly, strong inlier performance remains unaffected.



### Semi-Supervised 3D Object Detection with Channel Augmentation using Transformation Equivariance
- **Arxiv ID**: http://arxiv.org/abs/2409.06583v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2409.06583v2)
- **Published**: 2024-09-10 15:22:05+00:00
- **Updated**: 2024-09-22 11:06:20+00:00
- **Authors**: Minju Kang, Taehun Kong, Tae-Kyun Kim
- **Comment**: Accepted to 2024 IEEE International Conference on Image Processing
  (ICIP)
- **Journal**: None
- **Summary**: Accurate 3D object detection is crucial for autonomous vehicles and robots to navigate and interact with the environment safely and effectively. Meanwhile, the performance of 3D detector relies on the data size and annotation which is expensive. Consequently, the demand of training with limited labeled data is growing. We explore a novel teacher-student framework employing channel augmentation for 3D semi-supervised object detection. The teacher-student SSL typically adopts a weak augmentation and strong augmentation to teacher and student, respectively. In this work, we apply multiple channel augmentations to both networks using the transformation equivariance detector (TED). The TED allows us to explore different combinations of augmentation on point clouds and efficiently aggregates multi-channel transformation equivariance features. In principle, by adopting fixed channel augmentations for the teacher network, the student can train stably on reliable pseudo-labels. Adopting strong channel augmentations can enrich the diversity of data, fostering robustness to transformations and enhancing generalization performance of the student network. We use SOTA hierarchical supervision as a baseline and adapt its dual-threshold to TED, which is called channel IoU consistency. We evaluate our method with KITTI dataset, and achieved a significant performance leap, surpassing SOTA 3D semi-supervised object detection models.



### When to Extract ReID Features: A Selective Approach for Improved Multiple Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2409.06617v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2409.06617v2)
- **Published**: 2024-09-10 16:14:46+00:00
- **Updated**: 2024-11-21 20:04:33+00:00
- **Authors**: Emirhan Bayar, Cemal Aker
- **Comment**: 8 pages, 5 figures. Presents a selective approach for ReID feature
  extraction in Multiple Object Tracking, reducing computational overhead while
  maintaining accuracy. Tested on StrongSORT and Deep OC-SORT using MOT17,
  MOT20, and DanceTrack datasets. Code:
  https://github.com/emirhanbayar/Fast-StrongSORT,
  https://github.com/emirhanbayar/Fast-Deep-OC-SORT
- **Journal**: None
- **Summary**: Extracting and matching Re-Identification (ReID) features is used by many state-of-the-art (SOTA) Multiple Object Tracking (MOT) methods, particularly effective against frequent and long-term occlusions. While end-to-end object detection and tracking have been the main focus of recent research, they have yet to outperform traditional methods in benchmarks like MOT17 and MOT20. Thus, from an application standpoint, methods with separate detection and embedding remain the best option for accuracy, modularity, and ease of implementation, though they are impractical for edge devices due to the overhead involved. In this paper, we investigate a selective approach to minimize the overhead of feature extraction while preserving accuracy, modularity, and ease of implementation. This approach can be integrated into various SOTA methods. We demonstrate its effectiveness by applying it to StrongSORT and Deep OC-SORT. Experiments on MOT17, MOT20, and DanceTrack datasets show that our mechanism retains the advantages of feature extraction during occlusions while significantly reducing runtime. Additionally, it improves accuracy by preventing confusion in the feature-matching stage, particularly in cases of deformation and appearance similarity, which are common in DanceTrack. https://github.com/emirhanbayar/Fast-StrongSORT, https://github.com/emirhanbayar/Fast-Deep-OC-SORT



### Object Modeling from Underwater Forward-Scan Sonar Imagery with Sea-Surface Multipath
- **Arxiv ID**: http://arxiv.org/abs/2409.06815v1
- **DOI**: 10.1109/JOE.2024.3412268
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2409.06815v1)
- **Published**: 2024-09-10 18:46:25+00:00
- **Updated**: 2024-09-10 18:46:25+00:00
- **Authors**: Yuhan Liu, Shahriar Negaharipour
- **Comment**: Copyright 2024 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
- **Journal**: None
- **Summary**: We propose an optimization technique for 3-D underwater object modeling from 2-D forward-scan sonar images at known poses. A key contribution, for objects imaged in the proximity of the sea surface, is to resolve the multipath artifacts due to the air-water interface. Here, the object image formed by the direct target backscatter is almost always corrupted by the ghost and sometimes by the mirror components (generated by the multipath propagation). Assuming a planar air-water interface, we model, localize, and discard the corrupted object region within each view, thus avoiding the distortion of recovered 3-D shape. Additionally, complementary visual cues from the boundary of the mirror component, distinct at suitable sonar poses, are employed to enhance the 3-D modeling accuracy.   The optimization is implemented as iterative shape adjustment by displacing the vertices of triangular patches in the 3-D surface mesh model, in order to minimize the discrepancy between the data and synthesized views of the 3-D object model. To this end, we first determine 2-D motion fields that align the object regions in the data and synthesized views, then calculate the 3-D motion of triangular patch centers, and finally the model vertices. The 3-D model is initialized with the solution of an earlier space carving method applied to the same data. The same parameters are applied in various experiments with 2 real data sets, mixed real-synthetic data set, and computer-generated data guided by general findings from a real experiment, to explore the impact of non-flat air-water interface. The results confirm the generation of a refined 3-D model in about half-dozen iterations.



