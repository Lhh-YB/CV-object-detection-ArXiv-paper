# Arxiv Papers in cs.CV on 2024-09-27
### Query matching for spatio-temporal action detection with query-based object detector
- **Arxiv ID**: http://arxiv.org/abs/2409.18408v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2409.18408v1)
- **Published**: 2024-09-27 02:54:24+00:00
- **Updated**: 2024-09-27 02:54:24+00:00
- **Authors**: Shimon Hori, Kazuki Omi, Toru Tamaki
- **Comment**: None
- **Journal**: None
- **Summary**: In this paper, we propose a method that extends the query-based object detection model, DETR, to spatio-temporal action detection, which requires maintaining temporal consistency in videos. Our proposed method applies DETR to each frame and uses feature shift to incorporate temporal information. However, DETR's object queries in each frame may correspond to different objects, making a simple feature shift ineffective. To overcome this issue, we propose query matching across different frames, ensuring that queries for the same object are matched and used for the feature shift. Experimental results show that performance on the JHMDB21 dataset improves significantly when query features are shifted using the proposed query matching.



### When SAM2 Meets Video Camouflaged Object Segmentation: A Comprehensive Evaluation and Adaptation
- **Arxiv ID**: http://arxiv.org/abs/2409.18653v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2409.18653v1)
- **Published**: 2024-09-27 11:35:50+00:00
- **Updated**: 2024-09-27 11:35:50+00:00
- **Authors**: Yuli Zhou, Guolei Sun, Yawei Li, Luca Benini, Ender Konukoglu
- **Comment**: Technical report
- **Journal**: None
- **Summary**: This study investigates the application and performance of the Segment Anything Model 2 (SAM2) in the challenging task of video camouflaged object segmentation (VCOS). VCOS involves detecting objects that blend seamlessly in the surroundings for videos, due to similar colors and textures, poor light conditions, etc. Compared to the objects in normal scenes, camouflaged objects are much more difficult to detect. SAM2, a video foundation model, has shown potential in various tasks. But its effectiveness in dynamic camouflaged scenarios remains under-explored. This study presents a comprehensive study on SAM2's ability in VCOS. First, we assess SAM2's performance on camouflaged video datasets using different models and prompts (click, box, and mask). Second, we explore the integration of SAM2 with existing multimodal large language models (MLLMs) and VCOS methods. Third, we specifically adapt SAM2 by fine-tuning it on the video camouflaged dataset. Our comprehensive experiments demonstrate that SAM2 has excellent zero-shot ability of detecting camouflaged objects in videos. We also show that this ability could be further improved by specifically adjusting SAM2's parameters for VCOS. The code will be available at https://github.com/zhoustan/SAM2-VCOS



### Gaussian Heritage: 3D Digitization of Cultural Heritage with Integrated Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2409.19039v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2409.19039v1)
- **Published**: 2024-09-27 12:20:39+00:00
- **Updated**: 2024-09-27 12:20:39+00:00
- **Authors**: Mahtab Dahaghin, Myrna Castillo, Kourosh Riahidehkordi, Matteo Toso, Alessio Del Bue
- **Comment**: None
- **Journal**: None
- **Summary**: The creation of digital replicas of physical objects has valuable applications for the preservation and dissemination of tangible cultural heritage. However, existing methods are often slow, expensive, and require expert knowledge. We propose a pipeline to generate a 3D replica of a scene using only RGB images (e.g. photos of a museum) and then extract a model for each item of interest (e.g. pieces in the exhibit). We do this by leveraging the advancements in novel view synthesis and Gaussian Splatting, modified to enable efficient 3D segmentation. This approach does not need manual annotation, and the visual inputs can be captured using a standard smartphone, making it both affordable and easy to deploy. We provide an overview of the method and baseline evaluation of the accuracy of object segmentation. The code is available at https://mahtaabdn.github.io/gaussian_heritage.github.io/.



### MCUBench: A Benchmark of Tiny Object Detectors on MCUs
- **Arxiv ID**: http://arxiv.org/abs/2409.18866v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2409.18866v1)
- **Published**: 2024-09-27 16:02:56+00:00
- **Updated**: 2024-09-27 16:02:56+00:00
- **Authors**: Sudhakar Sah, Darshan C. Ganji, Matteo Grimaldi, Ravish Kumar, Alexander Hoffman, Honnesh Rohmetra, Ehsan Saboori
- **Comment**: Code and data are available at
  https://github.com/Deeplite/deeplite-torch-zoo
- **Journal**: None
- **Summary**: We introduce MCUBench, a benchmark featuring over 100 YOLO-based object detection models evaluated on the VOC dataset across seven different MCUs. This benchmark provides detailed data on average precision, latency, RAM, and Flash usage for various input resolutions and YOLO-based one-stage detectors. By conducting a controlled comparison with a fixed training pipeline, we collect comprehensive performance metrics. Our Pareto-optimal analysis shows that integrating modern detection heads and training techniques allows various YOLO architectures, including legacy models like YOLOv3, to achieve a highly efficient tradeoff between mean Average Precision (mAP) and latency. MCUBench serves as a valuable tool for benchmarking the MCU performance of contemporary object detectors and aids in model selection based on specific constraints.



### S2O: Static to Openable Enhancement for Articulated 3D Objects
- **Arxiv ID**: http://arxiv.org/abs/2409.18896v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2409.18896v1)
- **Published**: 2024-09-27 16:34:13+00:00
- **Updated**: 2024-09-27 16:34:13+00:00
- **Authors**: Denys Iliash, Hanxiao Jiang, Yiming Zhang, Manolis Savva, Angel X. Chang
- **Comment**: None
- **Journal**: None
- **Summary**: Despite much progress in large 3D datasets there are currently few interactive 3D object datasets, and their scale is limited due to the manual effort required in their construction. We introduce the static to openable (S2O) task which creates interactive articulated 3D objects from static counterparts through openable part detection, motion prediction, and interior geometry completion. We formulate a unified framework to tackle this task, and curate a challenging dataset of openable 3D objects that serves as a test bed for systematic evaluation. Our experiments benchmark methods from prior work and simple yet effective heuristics for the S2O task. We find that turning static 3D objects into interactively openable counterparts is possible but that all methods struggle to generalize to realistic settings of the task, and we highlight promising future work directions.



### Improving Visual Object Tracking through Visual Prompting
- **Arxiv ID**: http://arxiv.org/abs/2409.18901v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.MM, eess.IV, 68, I.4; I.2; I.5; I.4.1; I.4.8; I.4.9; I.4.10
- **Links**: [PDF](http://arxiv.org/pdf/2409.18901v1)
- **Published**: 2024-09-27 16:39:50+00:00
- **Updated**: 2024-09-27 16:39:50+00:00
- **Authors**: Shih-Fang Chen, Jun-Cheng Chen, I-Hong Jhuo, Yen-Yu Lin
- **Comment**: Accepted and to appear in IEEE Transactions on Multimedia
- **Journal**: None
- **Summary**: Learning a discriminative model to distinguish a target from its surrounding distractors is essential to generic visual object tracking. Dynamic target representation adaptation against distractors is challenging due to the limited discriminative capabilities of prevailing trackers. We present a new visual Prompting mechanism for generic Visual Object Tracking (PiVOT) to address this issue. PiVOT proposes a prompt generation network with the pre-trained foundation model CLIP to automatically generate and refine visual prompts, enabling the transfer of foundation model knowledge for tracking. While CLIP offers broad category-level knowledge, the tracker, trained on instance-specific data, excels at recognizing unique object instances. Thus, PiVOT first compiles a visual prompt highlighting potential target locations. To transfer the knowledge of CLIP to the tracker, PiVOT leverages CLIP to refine the visual prompt based on the similarities between candidate objects and the reference templates across potential targets. Once the visual prompt is refined, it can better highlight potential target locations, thereby reducing irrelevant prompt information. With the proposed prompting mechanism, the tracker can generate improved instance-aware feature maps through the guidance of the visual prompt, thus effectively reducing distractors. The proposed method does not involve CLIP during training, thereby keeping the same training complexity and preserving the generalization capability of the pretrained foundation model. Extensive experiments across multiple benchmarks indicate that PiVOT, using the proposed prompting method can suppress distracting objects and enhance the tracker.



