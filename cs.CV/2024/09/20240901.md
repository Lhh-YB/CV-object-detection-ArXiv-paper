# Arxiv Papers in cs.CV on 2024-09-01
### COMOGen: A Controllable Text-to-3D Multi-object Generation Framework
- **Arxiv ID**: http://arxiv.org/abs/2409.00590v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2409.00590v1)
- **Published**: 2024-09-01 02:50:38+00:00
- **Updated**: 2024-09-01 02:50:38+00:00
- **Authors**: Shaorong Sun, Shuchao Pang, Yazhou Yao, Xiaoshui Huang
- **Comment**: None
- **Journal**: None
- **Summary**: The controllability of 3D object generation methods is achieved through input text. Existing text-to-3D object generation methods primarily focus on generating a single object based on a single object description. However, these methods often face challenges in producing results that accurately correspond to our desired positions when the input text involves multiple objects. To address the issue of controllability in generating multiple objects, this paper introduces COMOGen, a COntrollable text-to-3D Multi-Object Generation framework. COMOGen enables the simultaneous generation of multiple 3D objects by the distillation of layout and multi-view prior knowledge. The framework consists of three modules: the layout control module, the multi-view consistency control module, and the 3D content enhancement module. Moreover, to integrate these three modules as an integral framework, we propose Layout Multi-view Score Distillation, which unifies two prior knowledge and further enhances the diversity and quality of generated 3D content. Comprehensive experiments demonstrate the effectiveness of our approach compared to the state-of-the-art methods, which represents a significant step forward in enabling more controlled and versatile text-based 3D content generation.



### Study of Dropout in PointPillars with 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2409.00673v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2409.00673v1)
- **Published**: 2024-09-01 09:30:54+00:00
- **Updated**: 2024-09-01 09:30:54+00:00
- **Authors**: Xiaoxiang Sun, Geoffrey Fox
- **Comment**: None
- **Journal**: None
- **Summary**: 3D object detection is critical for autonomous driving, leveraging deep learning techniques to interpret LiDAR data. The PointPillars architecture is a prominent model in this field, distinguished by its efficient use of LiDAR data. This study provides an analysis of enhancing the performance of PointPillars model under various dropout rates to address overfitting and improve model generalization. Dropout, a regularization technique, involves randomly omitting neurons during training, compelling the network to learn robust and diverse features. We systematically compare the effects of different enhancement techniques on the model's regression performance during training and its accuracy, measured by Average Precision (AP) and Average Orientation Similarity (AOS). Our findings offer insights into the optimal enhancements, contributing to improved 3D object detection in autonomous driving applications.



### Decoupled and Interactive Regression Modeling for High-performance One-stage 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2409.00690v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2409.00690v1)
- **Published**: 2024-09-01 10:47:22+00:00
- **Updated**: 2024-09-01 10:47:22+00:00
- **Authors**: Weiping Xiao, Yiqiang Wu, Chang Liu, Yu Qin, Xiaomao Li, Liming Xin
- **Comment**: None
- **Journal**: None
- **Summary**: Inadequate bounding box modeling in regression tasks constrains the performance of one-stage 3D object detection. Our study reveals that the primary reason lies in two aspects: (1) The limited center-offset prediction seriously impairs the bounding box localization since many highest response positions significantly deviate from object centers. (2) The low-quality sample ignored in regression tasks significantly impacts the bounding box prediction since it produces unreliable quality (IoU) rectification. To tackle these problems, we propose Decoupled and Interactive Regression Modeling (DIRM) for one-stage detection. Specifically, Decoupled Attribute Regression (DAR) is implemented to facilitate long regression range modeling for the center attribute through an adaptive multi-sample assignment strategy that deeply decouples bounding box attributes. On the other hand, to enhance the reliability of IoU predictions for low-quality results, Interactive Quality Prediction (IQP) integrates the classification task, proficient in modeling negative samples, with quality prediction for joint optimization. Extensive experiments on Waymo and ONCE datasets demonstrate that DIRM significantly improves the performance of several state-of-the-art methods with minimal additional inference latency. Notably, DIRM achieves state-of-the-art detection performance on both the Waymo and ONCE datasets.



### ReMOVE: A Reference-free Metric for Object Erasure
- **Arxiv ID**: http://arxiv.org/abs/2409.00707v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2409.00707v1)
- **Published**: 2024-09-01 12:26:14+00:00
- **Updated**: 2024-09-01 12:26:14+00:00
- **Authors**: Aditya Chandrasekar, Goirik Chakrabarty, Jai Bardhan, Ramya Hebbalaguppe, Prathosh AP
- **Comment**: Accepted at The First Workshop on the Evaluation of Generative
  Foundation Models (EvGENFM) at CVPR 2024
- **Journal**: None
- **Summary**: We introduce $\texttt{ReMOVE}$, a novel reference-free metric for assessing object erasure efficacy in diffusion-based image editing models post-generation. Unlike existing measures such as LPIPS and CLIPScore, $\texttt{ReMOVE}$ addresses the challenge of evaluating inpainting without a reference image, common in practical scenarios. It effectively distinguishes between object removal and replacement. This is a key issue in diffusion models due to stochastic nature of image generation. Traditional metrics fail to align with the intuitive definition of inpainting, which aims for (1) seamless object removal within masked regions (2) while preserving the background continuity. $\texttt{ReMOVE}$ not only correlates with state-of-the-art metrics and aligns with human perception but also captures the nuanced aspects of the inpainting process, providing a finer-grained evaluation of the generated outputs.



### Entropy Loss: An Interpretability Amplifier of 3D Object Detection Network for Intelligent Driving
- **Arxiv ID**: http://arxiv.org/abs/2409.00839v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.IT, math.IT
- **Links**: [PDF](http://arxiv.org/pdf/2409.00839v1)
- **Published**: 2024-09-01 20:55:50+00:00
- **Updated**: 2024-09-01 20:55:50+00:00
- **Authors**: Haobo Yang, Shiyan Zhang, Zhuoyi Yang, Xinyu Zhang, Li Wang, Yifan Tang, Jilong Guo, Jun Li
- **Comment**: None
- **Journal**: None
- **Summary**: With the increasing complexity of the traffic environment, the significance of safety perception in intelligent driving is intensifying. Traditional methods in the field of intelligent driving perception rely on deep learning, which suffers from limited interpretability, often described as a "black box." This paper introduces a novel type of loss function, termed "Entropy Loss," along with an innovative training strategy. Entropy Loss is formulated based on the functionality of feature compression networks within the perception model. Drawing inspiration from communication systems, the information transmission process in a feature compression network is expected to demonstrate steady changes in information volume and a continuous decrease in information entropy. By modeling network layer outputs as continuous random variables, we construct a probabilistic model that quantifies changes in information volume. Entropy Loss is then derived based on these expectations, guiding the update of network parameters to enhance network interpretability. Our experiments indicate that the Entropy Loss training strategy accelerates the training process. Utilizing the same 60 training epochs, the accuracy of 3D object detection models using Entropy Loss on the KITTI test set improved by up to 4.47\% compared to models without Entropy Loss, underscoring the method's efficacy. The implementation code is available at \url{https://github.com/yhbcode000/Eloss-Interpretability}.



### Detection, Recognition and Pose Estimation of Tabletop Objects
- **Arxiv ID**: http://arxiv.org/abs/2409.00869v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2409.00869v1)
- **Published**: 2024-09-01 23:31:13+00:00
- **Updated**: 2024-09-01 23:31:13+00:00
- **Authors**: Sanjuksha Nirgude, Kevin DuCharme, Namrita Madhusoodanan
- **Comment**: None
- **Journal**: None
- **Summary**: The problem of cleaning a messy table using Deep Neural Networks is a very interesting problem in both social and industrial robotics. This project focuses on the social application of this technology. A neural network model that is capable of detecting and recognizing common tabletop objects, such as a mug, mouse, or stapler is developed. The model also predicts the angle at which these objects are placed on a table,with respect to some reference. Assuming each object has a fixed intended position and orientation on the tabletop, the orientation of a particular object predicted by the deep learning model can be used to compute the transformation matrix to move the object from its initial position to the intended position. This can be fed to a pick and place robot to carry out the transfer.This paper talks about the deep learning approaches used in this project for object detection and orientation estimation.



