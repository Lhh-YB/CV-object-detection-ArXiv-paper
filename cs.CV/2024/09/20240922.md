# Arxiv Papers in cs.CV on 2024-09-22
### PISR: Polarimetric Neural Implicit Surface Reconstruction for Textureless and Specular Objects
- **Arxiv ID**: http://arxiv.org/abs/2409.14331v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2409.14331v1)
- **Published**: 2024-09-22 06:31:11+00:00
- **Updated**: 2024-09-22 06:31:11+00:00
- **Authors**: Guangcheng Chen, Yicheng He, Li He, Hong Zhang
- **Comment**: Accepted to ECCV 2024
- **Journal**: None
- **Summary**: Neural implicit surface reconstruction has achieved remarkable progress recently. Despite resorting to complex radiance modeling, state-of-the-art methods still struggle with textureless and specular surfaces. Different from RGB images, polarization images can provide direct constraints on the azimuth angles of the surface normals. In this paper, we present PISR, a novel method that utilizes a geometrically accurate polarimetric loss to refine shape independently of appearance. In addition, PISR smooths surface normals in image space to eliminate severe shape distortions and leverages the hash-grid-based neural signed distance function to accelerate the reconstruction. Experimental results demonstrate that PISR achieves higher accuracy and robustness, with an L1 Chamfer distance of 0.5 mm and an F-score of 99.5% at 1 mm, while converging 4~30 times faster than previous polarimetric surface reconstruction methods.



### Memory Matching is not Enough: Jointly Improving Memory Matching and Decoding for Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2409.14343v1
- **DOI**: None
- **Categories**: **cs.CV**, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2409.14343v1)
- **Published**: 2024-09-22 07:08:59+00:00
- **Updated**: 2024-09-22 07:08:59+00:00
- **Authors**: Jintu Zheng, Yun Liang, Yuqing Zhang, Wanchao Su
- **Comment**: Accepted to ICPR2024
- **Journal**: None
- **Summary**: Memory-based video object segmentation methods model multiple objects over long temporal-spatial spans by establishing memory bank, which achieve the remarkable performance. However, they struggle to overcome the false matching and are prone to lose critical information, resulting in confusion among different objects. In this paper, we propose an effective approach which jointly improving the matching and decoding stages to alleviate the false matching issue.For the memory matching stage, we present a cost aware mechanism that suppresses the slight errors for short-term memory and a shunted cross-scale matching for long-term memory which establish a wide filed matching spaces for various object scales. For the readout decoding stage, we implement a compensatory mechanism aims at recovering the essential information where missing at the matching stage. Our approach achieves the outstanding performance in several popular benchmarks (i.e., DAVIS 2016&2017 Val (92.4%&88.1%), and DAVIS 2017 Test (83.9%)), and achieves 84.8%&84.6% on YouTubeVOS 2018&2019 Val.



### TrackNetV4: Enhancing Fast Sports Object Tracking with Motion Attention Maps
- **Arxiv ID**: http://arxiv.org/abs/2409.14543v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2409.14543v1)
- **Published**: 2024-09-22 17:58:09+00:00
- **Updated**: 2024-09-22 17:58:09+00:00
- **Authors**: Arjun Raj, Lei Wang, Tom Gedeon
- **Comment**: Research report
- **Journal**: None
- **Summary**: Accurately detecting and tracking high-speed, small objects, such as balls in sports videos, is challenging due to factors like motion blur and occlusion. Although recent deep learning frameworks like TrackNetV1, V2, and V3 have advanced tennis ball and shuttlecock tracking, they often struggle in scenarios with partial occlusion or low visibility. This is primarily because these models rely heavily on visual features without explicitly incorporating motion information, which is crucial for precise tracking and trajectory prediction. In this paper, we introduce an enhancement to the TrackNet family by fusing high-level visual features with learnable motion attention maps through a motion-aware fusion mechanism, effectively emphasizing the moving ball's location and improving tracking performance. Our approach leverages frame differencing maps, modulated by a motion prompt layer, to highlight key motion regions over time. Experimental results on the tennis ball and shuttlecock datasets show that our method enhances the tracking performance of both TrackNetV2 and V3. We refer to our lightweight, plug-and-play solution, built on top of the existing TrackNet, as TrackNetV4.



### SOS: Segment Object System for Open-World Instance Segmentation With Object Priors
- **Arxiv ID**: http://arxiv.org/abs/2409.14627v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2409.14627v1)
- **Published**: 2024-09-22 23:35:31+00:00
- **Updated**: 2024-09-22 23:35:31+00:00
- **Authors**: Christian Wilms, Tim Rolff, Maris Hillemann, Robert Johanson, Simone Frintrop
- **Comment**: Accepted at ECCV 2024. Code available at
  https://github.com/chwilms/SOS
- **Journal**: None
- **Summary**: We propose an approach for Open-World Instance Segmentation (OWIS), a task that aims to segment arbitrary unknown objects in images by generalizing from a limited set of annotated object classes during training. Our Segment Object System (SOS) explicitly addresses the generalization ability and the low precision of state-of-the-art systems, which often generate background detections. To this end, we generate high-quality pseudo annotations based on the foundation model SAM. We thoroughly study various object priors to generate prompts for SAM, explicitly focusing the foundation model on objects. The strongest object priors were obtained by self-attention maps from self-supervised Vision Transformers, which we utilize for prompting SAM. Finally, the post-processed segments from SAM are used as pseudo annotations to train a standard instance segmentation system. Our approach shows strong generalization capabilities on COCO, LVIS, and ADE20k datasets and improves on the precision by up to 81.6% compared to the state-of-the-art. Source code is available at: https://github.com/chwilms/SOS



