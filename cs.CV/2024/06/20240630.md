# Arxiv Papers in cs.CV on 2024-06-30
### Learning Granularity-Aware Affordances from Human-Object Interaction for Tool-Based Functional Grasping in Dexterous Robotics
- **Arxiv ID**: http://arxiv.org/abs/2407.00614v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2407.00614v1)
- **Published**: 2024-06-30 07:42:57+00:00
- **Updated**: 2024-06-30 07:42:57+00:00
- **Authors**: Fan Yang, Wenrui Chen, Kailun Yang, Haoran Lin, DongSheng Luo, Conghui Tang, Zhiyong Li, Yaonan Wang
- **Comment**: The source code and the established dataset will be made publicly
  available at https://github.com/yangfan293/GAAF-DEX
- **Journal**: None
- **Summary**: To enable robots to use tools, the initial step is teaching robots to employ dexterous gestures for touching specific areas precisely where tasks are performed. Affordance features of objects serve as a bridge in the functional interaction between agents and objects. However, leveraging these affordance cues to help robots achieve functional tool grasping remains unresolved. To address this, we propose a granularity-aware affordance feature extraction method for locating functional affordance areas and predicting dexterous coarse gestures. We study the intrinsic mechanisms of human tool use. On one hand, we use fine-grained affordance features of object-functional finger contact areas to locate functional affordance regions. On the other hand, we use highly activated coarse-grained affordance features in hand-object interaction regions to predict grasp gestures. Additionally, we introduce a model-based post-processing module that includes functional finger coordinate localization, finger-to-end coordinate transformation, and force feedback-based coarse-to-fine grasping. This forms a complete dexterous robotic functional grasping framework GAAF-Dex, which learns Granularity-Aware Affordances from human-object interaction for tool-based Functional grasping in Dexterous Robotics. Unlike fully-supervised methods that require extensive data annotation, we employ a weakly supervised approach to extract relevant cues from exocentric (Exo) images of hand-object interactions to supervise feature extraction in egocentric (Ego) images. We have constructed a small-scale dataset, FAH, which includes near 6K images of functional hand-object interaction Exo- and Ego images of 18 commonly used tools performing 6 tasks. Extensive experiments on the dataset demonstrate our method outperforms state-of-the-art methods. The code will be made publicly available at https://github.com/yangfan293/GAAF-DEX.



### CAMON: Cooperative Agents for Multi-Object Navigation with LLM-based Conversations
- **Arxiv ID**: http://arxiv.org/abs/2407.00632v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CL, cs.CV, cs.MA
- **Links**: [PDF](http://arxiv.org/pdf/2407.00632v1)
- **Published**: 2024-06-30 09:14:33+00:00
- **Updated**: 2024-06-30 09:14:33+00:00
- **Authors**: Pengying Wu, Yao Mu, Kangjie Zhou, Ji Ma, Junting Chen, Chang Liu
- **Comment**: Accepted to the RSS 2024 Workshop: GROUND
- **Journal**: None
- **Summary**: Visual navigation tasks are critical for household service robots. As these tasks become increasingly complex, effective communication and collaboration among multiple robots become imperative to ensure successful completion. In recent years, large language models (LLMs) have exhibited remarkable comprehension and planning abilities in the context of embodied agents. However, their application in household scenarios, specifically in the use of multiple agents collaborating to complete complex navigation tasks through communication, remains unexplored. Therefore, this paper proposes a framework for decentralized multi-agent navigation, leveraging LLM-enabled communication and collaboration. By designing the communication-triggered dynamic leadership organization structure, we achieve faster team consensus with fewer communication instances, leading to better navigation effectiveness and collaborative exploration efficiency. With the proposed novel communication scheme, our framework promises to be conflict-free and robust in multi-object navigation tasks, even when there is a surge in team size.



### Engineering an Efficient Object Tracker for Non-Linear Motion
- **Arxiv ID**: http://arxiv.org/abs/2407.00738v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, I.2.10; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2407.00738v1)
- **Published**: 2024-06-30 15:50:54+00:00
- **Updated**: 2024-06-30 15:50:54+00:00
- **Authors**: Momir Adžemović, Predrag Tadić, Andrija Petrović, Mladen Nikolić
- **Comment**: 3 figures, 20 tables
- **Journal**: None
- **Summary**: The goal of multi-object tracking is to detect and track all objects in a scene while maintaining unique identifiers for each, by associating their bounding boxes across video frames. This association relies on matching motion and appearance patterns of detected objects. This task is especially hard in case of scenarios involving dynamic and non-linear motion patterns. In this paper, we introduce DeepMoveSORT, a novel, carefully engineered multi-object tracker designed specifically for such scenarios. In addition to standard methods of appearance-based association, we improve motion-based association by employing deep learnable filters (instead of the most commonly used Kalman filter) and a rich set of newly proposed heuristics. Our improvements to motion-based association methods are severalfold. First, we propose a new transformer-based filter architecture, TransFilter, which uses an object's motion history for both motion prediction and noise filtering. We further enhance the filter's performance by careful handling of its motion history and accounting for camera motion. Second, we propose a set of heuristics that exploit cues from the position, shape, and confidence of detected bounding boxes to improve association performance. Our experimental evaluation demonstrates that DeepMoveSORT outperforms existing trackers in scenarios featuring non-linear motion, surpassing state-of-the-art results on three such datasets. We also perform a thorough ablation study to evaluate the contributions of different tracker components which we proposed. Based on our study, we conclude that using a learnable filter instead of the Kalman filter, along with appearance-based association is key to achieving strong general tracking performance.



