# Arxiv Papers in cs.CV on 2024-06-24
### High-resolution open-vocabulary object 6D pose estimation
- **Arxiv ID**: http://arxiv.org/abs/2406.16384v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2406.16384v2)
- **Published**: 2024-06-24 07:53:46+00:00
- **Updated**: 2024-07-11 17:03:29+00:00
- **Authors**: Jaime Corsetti, Davide Boscaini, Francesco Giuliari, Changjae Oh, Andrea Cavallaro, Fabio Poiesi
- **Comment**: Technical report. Extension of CVPR paper "Open-vocabulary object 6D
  pose estimation". Project page: https://jcorsetti.github.io/oryon
- **Journal**: None
- **Summary**: The generalisation to unseen objects in the 6D pose estimation task is very challenging. While Vision-Language Models (VLMs) enable using natural language descriptions to support 6D pose estimation of unseen objects, these solutions underperform compared to model-based methods. In this work we present Horyon, an open-vocabulary VLM-based architecture that addresses relative pose estimation between two scenes of an unseen object, described by a textual prompt only. We use the textual prompt to identify the unseen object in the scenes and then obtain high-resolution multi-scale features. These features are used to extract cross-scene matches for registration. We evaluate our model on a benchmark with a large variety of unseen objects across four datasets, namely REAL275, Toyota-Light, Linemod, and YCB-Video. Our method achieves state-of-the-art performance on all datasets, outperforming by 12.6 in Average Recall the previous best-performing approach.



### Exploring Test-Time Adaptation for Object Detection in Continually Changing Environments
- **Arxiv ID**: http://arxiv.org/abs/2406.16439v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2406.16439v4)
- **Published**: 2024-06-24 08:30:03+00:00
- **Updated**: 2024-11-13 09:41:00+00:00
- **Authors**: Shilei Cao, Yan Liu, Juepeng Zheng, Weijia Li, Runmin Dong, Haohuan Fu
- **Comment**: None
- **Journal**: None
- **Summary**: Real-world application models are commonly deployed in dynamic environments, where the target domain distribution undergoes temporal changes. Continual Test-Time Adaptation (CTTA) has recently emerged as a promising technique to gradually adapt a source-trained model to continually changing target domains. Despite recent advancements in addressing CTTA, two critical issues remain: 1) Fixed thresholds for pseudo-labeling in existing methodologies lead to low-quality pseudo-labels, as model confidence varies across categories and domains; 2) Stochastic parameter restoration methods for mitigating catastrophic forgetting fail to preserve critical information effectively, due to their intrinsic randomness. To tackle these challenges for detection models in CTTA scenarios, we present AMROD, featuring three core components. Firstly, the object-level contrastive learning module extracts object-level features for contrastive learning to refine the feature representation in the target domain. Secondly, the adaptive monitoring module dynamically skips unnecessary adaptation and updates the category-specific threshold based on predicted confidence scores to enable efficiency and improve the quality of pseudo-labels. Lastly, the adaptive randomized restoration mechanism selectively reset inactive parameters with higher possibilities, ensuring the retention of essential knowledge. We demonstrate the effectiveness of AMROD on four CTTA object detection tasks, where AMROD outperforms existing methods, especially achieving a 3.2 mAP improvement and a 20% increase in efficiency on the Cityscapes-to-Cityscapes-C CTTA task. The code will be released.



### Articulate your NeRF: Unsupervised articulated object modeling via conditional view synthesis
- **Arxiv ID**: http://arxiv.org/abs/2406.16623v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2406.16623v1)
- **Published**: 2024-06-24 13:13:31+00:00
- **Updated**: 2024-06-24 13:13:31+00:00
- **Authors**: Jianning Deng, Kartic Subr, Hakan Bilen
- **Comment**: 9 pages for the maincontent, excluding references and supplementaries
- **Journal**: None
- **Summary**: We propose a novel unsupervised method to learn the pose and part-segmentation of articulated objects with rigid parts. Given two observations of an object in different articulation states, our method learns the geometry and appearance of object parts by using an implicit model from the first observation, distils the part segmentation and articulation from the second observation while rendering the latter observation. Additionally, to tackle the complexities in the joint optimization of part segmentation and articulation, we propose a voxel grid-based initialization strategy and a decoupled optimization procedure. Compared to the prior unsupervised work, our model obtains significantly better performance, and generalizes to objects with multiple parts while it can be efficiently from few views for the latter observation.



### The Progression of Transformers from Language to Vision to MOT: A Literature Review on Multi-Object Tracking with Transformers
- **Arxiv ID**: http://arxiv.org/abs/2406.16784v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2406.16784v1)
- **Published**: 2024-06-24 16:45:28+00:00
- **Updated**: 2024-06-24 16:45:28+00:00
- **Authors**: Abhi Kamboj
- **Comment**: This report was written in November 2022, and may not contain more
  recent works since then
- **Journal**: None
- **Summary**: The transformer neural network architecture allows for autoregressive sequence-to-sequence modeling through the use of attention layers. It was originally created with the application of machine translation but has revolutionized natural language processing. Recently, transformers have also been applied across a wide variety of pattern recognition tasks, particularly in computer vision. In this literature review, we describe major advances in computer vision utilizing transformers. We then focus specifically on Multi-Object Tracking (MOT) and discuss how transformers are increasingly becoming competitive in state-of-the-art MOT works, yet still lag behind traditional deep learning methods.



