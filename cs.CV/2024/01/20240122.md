# Arxiv Papers in cs.CV on 2024-01-22
### MsSVT++: Mixed-scale Sparse Voxel Transformer with Center Voting for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2401.11718v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2401.11718v1)
- **Published**: 2024-01-22 06:42:23+00:00
- **Updated**: 2024-01-22 06:42:23+00:00
- **Authors**: Jianan Li, Shaocong Dong, Lihe Ding, Tingfa Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Accurate 3D object detection in large-scale outdoor scenes, characterized by considerable variations in object scales, necessitates features rich in both long-range and fine-grained information. While recent detectors have utilized window-based transformers to model long-range dependencies, they tend to overlook fine-grained details. To bridge this gap, we propose MsSVT++, an innovative Mixed-scale Sparse Voxel Transformer that simultaneously captures both types of information through a divide-and-conquer approach. This approach involves explicitly dividing attention heads into multiple groups, each responsible for attending to information within a specific range. The outputs of these groups are subsequently merged to obtain final mixed-scale features. To mitigate the computational complexity associated with applying a window-based transformer in 3D voxel space, we introduce a novel Chessboard Sampling strategy and implement voxel sampling and gathering operations sparsely using a hash map. Moreover, an important challenge stems from the observation that non-empty voxels are primarily located on the surface of objects, which impedes the accurate estimation of bounding boxes. To overcome this challenge, we introduce a Center Voting module that integrates newly voted voxels enriched with mixed-scale contextual information towards the centers of the objects, thereby improving precise object localization. Extensive experiments demonstrate that our single-stage detector, built upon the foundation of MsSVT++, consistently delivers exceptional performance across diverse datasets.



### Concealed Object Segmentation with Hierarchical Coherence Modeling
- **Arxiv ID**: http://arxiv.org/abs/2401.11767v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2401.11767v1)
- **Published**: 2024-01-22 09:02:52+00:00
- **Updated**: 2024-01-22 09:02:52+00:00
- **Authors**: Fengyang Xiao, Pan Zhang, Chunming He, Runze Hu, Yutao Liu
- **Comment**: Accepted to CICAI 2023. 13 pages, 6 figures, 4 tables
- **Journal**: None
- **Summary**: Concealed object segmentation (COS) is a challenging task that involves localizing and segmenting those concealed objects that are visually blended with their surrounding environments. Despite achieving remarkable success, existing COS segmenters still struggle to achieve complete segmentation results in extremely concealed scenarios. In this paper, we propose a Hierarchical Coherence Modeling (HCM) segmenter for COS, aiming to address this incomplete segmentation limitation. In specific, HCM promotes feature coherence by leveraging the intra-stage coherence and cross-stage coherence modules, exploring feature correlations at both the single-stage and contextual levels. Additionally, we introduce the reversible re-calibration decoder to detect previously undetected parts in low-confidence regions, resulting in further enhancing segmentation performance. Extensive experiments conducted on three COS tasks, including camouflaged object detection, polyp image segmentation, and transparent object detection, demonstrate the promising results achieved by the proposed HCM segmenter.



### Large receptive field strategy and important feature extraction strategy in 3D object detection
- **Arxiv ID**: http://arxiv.org/abs/2401.11913v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2401.11913v2)
- **Published**: 2024-01-22 13:01:28+00:00
- **Updated**: 2024-03-10 10:37:21+00:00
- **Authors**: Leichao Cui, Xiuxian Li, Min Meng, Guangyu Jia
- **Comment**: None
- **Journal**: None
- **Summary**: The enhancement of 3D object detection is pivotal for precise environmental perception and improved task execution capabilities in autonomous driving. LiDAR point clouds, offering accurate depth information, serve as a crucial information for this purpose. Our study focuses on key challenges in 3D target detection. To tackle the challenge of expanding the receptive field of a 3D convolutional kernel, we introduce the Dynamic Feature Fusion Module (DFFM). This module achieves adaptive expansion of the 3D convolutional kernel's receptive field, balancing the expansion with acceptable computational loads. This innovation reduces operations, expands the receptive field, and allows the model to dynamically adjust to different object requirements. Simultaneously, we identify redundant information in 3D features. Employing the Feature Selection Module (FSM) quantitatively evaluates and eliminates non-important features, achieving the separation of output box fitting and feature extraction. This innovation enables the detector to focus on critical features, resulting in model compression, reduced computational burden, and minimized candidate frame interference. Extensive experiments confirm that both DFFM and FSM not only enhance current benchmarks, particularly in small target detection, but also accelerate network performance. Importantly, these modules exhibit effective complementarity.



### A Saliency Enhanced Feature Fusion based multiscale RGB-D Salient Object Detection Network
- **Arxiv ID**: http://arxiv.org/abs/2401.11914v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2401.11914v1)
- **Published**: 2024-01-22 13:01:35+00:00
- **Updated**: 2024-01-22 13:01:35+00:00
- **Authors**: Rui Huang, Qingyi Zhao, Yan Xing, Sihua Gao, Weifeng Xu, Yuxiang Zhang, Wei Fan
- **Comment**: Accpeted by 2024 IEEE International Conference on Acoustics, Speech,
  and Signal Processing (ICASSP 2024)
- **Journal**: None
- **Summary**: Multiscale convolutional neural network (CNN) has demonstrated remarkable capabilities in solving various vision problems. However, fusing features of different scales alwaysresults in large model sizes, impeding the application of multiscale CNNs in RGB-D saliency detection. In this paper, we propose a customized feature fusion module, called Saliency Enhanced Feature Fusion (SEFF), for RGB-D saliency detection. SEFF utilizes saliency maps of the neighboring scales to enhance the necessary features for fusing, resulting in more representative fused features. Our multiscale RGB-D saliency detector uses SEFF and processes images with three different scales. SEFF is used to fuse the features of RGB and depth images, as well as the features of decoders at different scales. Extensive experiments on five benchmark datasets have demonstrated the superiority of our method over ten SOTA saliency detectors.



