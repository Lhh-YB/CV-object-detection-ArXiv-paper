# Arxiv Papers in cs.CV on 2024-01-02
### Real-Time Object Detection in Occluded Environment with Background Cluttering Effects Using Deep Learning
- **Arxiv ID**: http://arxiv.org/abs/2401.00986v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2401.00986v1)
- **Published**: 2024-01-02 01:30:03+00:00
- **Updated**: 2024-01-02 01:30:03+00:00
- **Authors**: Syed Muhammad Aamir, Hongbin Ma, Malak Abid Ali Khan, Muhammad Aaqib
- **Comment**: None
- **Journal**: None
- **Summary**: Detection of small, undetermined moving objects or objects in an occluded environment with a cluttered background is the main problem of computer vision. This greatly affects the detection accuracy of deep learning models. To overcome these problems, we concentrate on deep learning models for real-time detection of cars and tanks in an occluded environment with a cluttered background employing SSD and YOLO algorithms and improved precision of detection and reduce problems faced by these models. The developed method makes the custom dataset and employs a preprocessing technique to clean the noisy dataset. For training the developed model we apply the data augmentation technique to balance and diversify the data. We fine-tuned, trained, and evaluated these models on the established dataset by applying these techniques and highlighting the results we got more accurately than without applying these techniques. The accuracy and frame per second of the SSD-Mobilenet v2 model are higher than YOLO V3 and YOLO V4. Furthermore, by employing various techniques like data enhancement, noise reduction, parameter optimization, and model fusion we improve the effectiveness of detection and recognition. We further added a counting algorithm, and target attributes experimental comparison, and made a graphical user interface system for the developed model with features of object counting, alerts, status, resolution, and frame per second. Subsequently, to justify the importance of the developed method analysis of YOLO V3, V4, and SSD were incorporated. Which resulted in the overall completion of the proposed method.



### Shrinking Your TimeStep: Towards Low-Latency Neuromorphic Object Recognition with Spiking Neural Network
- **Arxiv ID**: http://arxiv.org/abs/2401.01912v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, eess.IV
- **Links**: [PDF](http://arxiv.org/pdf/2401.01912v1)
- **Published**: 2024-01-02 02:05:05+00:00
- **Updated**: 2024-01-02 02:05:05+00:00
- **Authors**: Yongqi Ding, Lin Zuo, Mengmeng Jing, Pei He, Yongjun Xiao
- **Comment**: Accepted by AAAI 2024
- **Journal**: None
- **Summary**: Neuromorphic object recognition with spiking neural networks (SNNs) is the cornerstone of low-power neuromorphic computing. However, existing SNNs suffer from significant latency, utilizing 10 to 40 timesteps or more, to recognize neuromorphic objects. At low latencies, the performance of existing SNNs is drastically degraded. In this work, we propose the Shrinking SNN (SSNN) to achieve low-latency neuromorphic object recognition without reducing performance. Concretely, we alleviate the temporal redundancy in SNNs by dividing SNNs into multiple stages with progressively shrinking timesteps, which significantly reduces the inference latency. During timestep shrinkage, the temporal transformer smoothly transforms the temporal scale and preserves the information maximally. Moreover, we add multiple early classifiers to the SNN during training to mitigate the mismatch between the surrogate gradient and the true gradient, as well as the gradient vanishing/exploding, thus eliminating the performance degradation at low latency. Extensive experiments on neuromorphic datasets, CIFAR10-DVS, N-Caltech101, and DVS-Gesture have revealed that SSNN is able to improve the baseline accuracy by 6.55% ~ 21.41%. With only 5 average timesteps and without any data augmentation, SSNN is able to achieve an accuracy of 73.63% on CIFAR10-DVS. This work presents a heterogeneous temporal scale SNN and provides valuable insights into the development of high-performance, low-latency SNNs.



### Depth-discriminative Metric Learning for Monocular 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2401.01075v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2401.01075v1)
- **Published**: 2024-01-02 07:34:09+00:00
- **Updated**: 2024-01-02 07:34:09+00:00
- **Authors**: Wonhyeok Choi, Mingyu Shin, Sunghoon Im
- **Comment**: Accepted at NeurIPS 2023
- **Journal**: None
- **Summary**: Monocular 3D object detection poses a significant challenge due to the lack of depth information in RGB images. Many existing methods strive to enhance the object depth estimation performance by allocating additional parameters for object depth estimation, utilizing extra modules or data. In contrast, we introduce a novel metric learning scheme that encourages the model to extract depth-discriminative features regardless of the visual attributes without increasing inference time and model size. Our method employs the distance-preserving function to organize the feature space manifold in relation to ground-truth object depth. The proposed (K, B, eps)-quasi-isometric loss leverages predetermined pairwise distance restriction as guidance for adjusting the distance among object descriptors without disrupting the non-linearity of the natural feature manifold. Moreover, we introduce an auxiliary head for object-wise depth estimation, which enhances depth quality while maintaining the inference time. The broad applicability of our method is demonstrated through experiments that show improvements in overall performance when integrated into various baselines. The results show that our method consistently improves the performance of various baselines by 23.51% and 5.78% on average across KITTI and Waymo, respectively.



### Hybrid Pooling and Convolutional Network for Improving Accuracy and Training Convergence Speed in Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2401.01134v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2401.01134v1)
- **Published**: 2024-01-02 10:22:06+00:00
- **Updated**: 2024-01-02 10:22:06+00:00
- **Authors**: Shiwen Zhao, Wei Wang, Junhui Hou, Hai Wu
- **Comment**: 10 pages,5 figures, conference
- **Journal**: None
- **Summary**: This paper introduces HPC-Net, a high-precision and rapidly convergent object detection network.



### Image Sculpting: Precise Object Editing with 3D Geometry Control
- **Arxiv ID**: http://arxiv.org/abs/2401.01702v1
- **DOI**: None
- **Categories**: **cs.GR**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2401.01702v1)
- **Published**: 2024-01-02 18:59:35+00:00
- **Updated**: 2024-01-02 18:59:35+00:00
- **Authors**: Jiraphon Yenphraphai, Xichen Pan, Sainan Liu, Daniele Panozzo, Saining Xie
- **Comment**: Code and project page: https://image-sculpting.github.io
- **Journal**: None
- **Summary**: We present Image Sculpting, a new framework for editing 2D images by incorporating tools from 3D geometry and graphics. This approach differs markedly from existing methods, which are confined to 2D spaces and typically rely on textual instructions, leading to ambiguity and limited control. Image Sculpting converts 2D objects into 3D, enabling direct interaction with their 3D geometry. Post-editing, these objects are re-rendered into 2D, merging into the original image to produce high-fidelity results through a coarse-to-fine enhancement process. The framework supports precise, quantifiable, and physically-plausible editing options such as pose editing, rotation, translation, 3D composition, carving, and serial addition. It marks an initial step towards combining the creative freedom of generative models with the precision of graphics pipelines.



