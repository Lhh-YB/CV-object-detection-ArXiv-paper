# Arxiv Papers in cs.CV on 2024-01-15
### CascadeV-Det: Cascade Point Voting for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2401.07477v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2401.07477v1)
- **Published**: 2024-01-15 05:10:27+00:00
- **Updated**: 2024-01-15 05:10:27+00:00
- **Authors**: Yingping Liang, Ying Fu
- **Comment**: None
- **Journal**: None
- **Summary**: Anchor-free object detectors are highly efficient in performing point-based prediction without the need for extra post-processing of anchors. However, different from the 2D grids, the 3D points used in these detectors are often far from the ground truth center, making it challenging to accurately regress the bounding boxes. To address this issue, we propose a Cascade Voting (CascadeV) strategy that provides high-quality 3D object detection with point-based prediction. Specifically, CascadeV performs cascade detection using a novel Cascade Voting decoder that combines two new components: Instance Aware Voting (IA-Voting) and a Cascade Point Assignment (CPA) module. The IA-Voting module updates the object features of updated proposal points within the bounding box using conditional inverse distance weighting. This approach prevents features from being aggregated outside the instance and helps improve the accuracy of object detection. Additionally, since model training can suffer from a lack of proposal points with high centerness, we have developed the CPA module to narrow down the positive assignment threshold with cascade stages. This approach relaxes the dependence on proposal centerness in the early stages while ensuring an ample quantity of positives with high centerness in the later stages. Experiments show that FCAF3D with our CascadeV achieves state-of-the-art 3D object detection results with 70.4\% mAP@0.25 and 51.6\% mAP@0.5 on SUN RGB-D and competitive results on ScanNet. Code will be released at https://github.com/Sharpiless/CascadeV-Det



### Discriminative Consensus Mining with A Thousand Groups for More Accurate Co-Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2403.12057v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2403.12057v1)
- **Published**: 2024-01-15 06:02:24+00:00
- **Updated**: 2024-01-15 06:02:24+00:00
- **Authors**: Peng Zheng
- **Comment**: Master's thesis of the M.S. program at Aalto University and
  University of Trento, v1. The source codes, the dataset, and the demos will
  not be released until the conference/journal version is finished and made
  public
- **Journal**: None
- **Summary**: Co-Salient Object Detection (CoSOD) is a rapidly growing task, extended from Salient Object Detection (SOD) and Common Object Segmentation (Co-Segmentation). It is aimed at detecting the co-occurring salient object in the given image group. Many effective approaches have been proposed on the basis of existing datasets. However, there is still no standard and efficient training set in CoSOD, which makes it chaotic to choose training sets in the recently proposed CoSOD methods. First, the drawbacks of existing training sets in CoSOD are analyzed in a comprehensive way, and potential improvements are provided to solve existing problems to some extent. In particular, in this thesis, a new CoSOD training set is introduced, named Co-Saliency of ImageNet (CoSINe) dataset. The proposed CoSINe is the largest number of groups among all existing CoSOD datasets. The images obtained here span a wide variety in terms of categories, object sizes, etc. In experiments, models trained on CoSINe can achieve significantly better performance with fewer images compared to all existing datasets. Second, to make the most of the proposed CoSINe, a novel CoSOD approach named Hierarchical Instance-aware COnsensus MinEr (HICOME) is proposed, which efficiently mines the consensus feature from different feature levels and discriminates objects of different classes in an object-aware contrastive way. As extensive experiments show, the proposed HICOME achieves SoTA performance on all the existing CoSOD test sets. Several useful training tricks suitable for training CoSOD models are also provided. Third, practical applications are given using the CoSOD technique to show the effectiveness. Finally, the remaining challenges and potential improvements of CoSOD are discussed to inspire related work in the future. The source code, the dataset, and the online demo will be publicly available at github.com/ZhengPeng7/CoSINe.



### Compositional Oil Spill Detection Based on Object Detector and Adapted Segment Anything Model from SAR Images
- **Arxiv ID**: http://arxiv.org/abs/2401.07502v2
- **DOI**: 10.1109/LGRS.2024.3382970
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2401.07502v2)
- **Published**: 2024-01-15 07:03:10+00:00
- **Updated**: 2024-12-22 06:35:53+00:00
- **Authors**: Wenhui Wu, Man Sing Wong, Xinyu Yu, Guoqiang Shi, Coco Yin Tung Kwok, Kang Zou
- **Comment**: 5 pages, 4 figures, published to IEEE Geoscience and Remote Sensing
  Letters
- **Journal**: IEEE Geoscience and Remote Sensing Letters, vol. 21, pp. 1-5,
  2024, Art no. 4007505
- **Summary**: Semantic segmentation-based methods have attracted extensive attention in oil spill detection from SAR images. However, the existing approaches require a large number of finely annotated segmentation samples in the training stage. To alleviate this issue, we propose a composite oil spill detection framework, SAM-OIL, comprising an object detector (e.g., YOLOv8), an Adapted Segment Anything Model (SAM), and an Ordered Mask Fusion (OMF) module. SAM-OIL is the first application of the powerful SAM in oil spill detection. Specifically, the SAM-OIL strategy uses YOLOv8 to obtain the categories and bounding boxes of oil spill-related objects, then inputs bounding boxes into the Adapted SAM to retrieve category-agnostic masks, and finally adopts the OMF module to fuse the masks and categories. The Adapted SAM, combining a frozen SAM with a learnable Adapter module, can enhance SAM's ability to segment ambiguous objects. The OMF module, a parameter-free method, can effectively resolve pixel category conflicts within SAM. Experimental results demonstrate that SAM-OIL surpasses existing semantic segmentation-based oil spill detection methods, achieving mIoU of 69.52\%. The results also indicated that both OMF and Adapter modules can effectively improve the accuracy in SAM-OIL.



### Geo-locating Road Objects using Inverse Haversine Formula with NVIDIA Driveworks
- **Arxiv ID**: http://arxiv.org/abs/2401.07582v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2401.07582v1)
- **Published**: 2024-01-15 10:38:07+00:00
- **Updated**: 2024-01-15 10:38:07+00:00
- **Authors**: Mamoona Birkhez Shami, Gabriel Kiss, Trond Arve Haakonsen, Frank Lindseth
- **Comment**: None
- **Journal**: Norsk IKT-konferanse for forskning og utdanning. No. 1. (2023)
- **Summary**: Geolocation is integral to the seamless functioning of autonomous vehicles and advanced traffic monitoring infrastructures. This paper introduces a methodology to geolocate road objects using a monocular camera, leveraging the NVIDIA DriveWorks platform. We use the Centimeter Positioning Service (CPOS) and the inverse Haversine formula to geo-locate road objects accurately. The real-time algorithm processing capability of the NVIDIA DriveWorks platform enables instantaneous object recognition and spatial localization for Advanced Driver Assistance Systems (ADAS) and autonomous driving platforms. We present a measurement pipeline suitable for autonomous driving (AD) platforms and provide detailed guidelines for calibrating cameras using NVIDIA DriveWorks. Experiments were carried out to validate the accuracy of the proposed method for geolocating targets in both controlled and dynamic settings. We show that our approach can locate targets with less than 1m error when the AD platform is stationary and less than 4m error at higher speeds (i.e. up to 60km/h) within a 15m radius.



### Fine-Grained Prototypes Distillation for Few-Shot Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2401.07629v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2401.07629v2)
- **Published**: 2024-01-15 12:12:48+00:00
- **Updated**: 2024-03-12 04:09:02+00:00
- **Authors**: Zichen Wang, Bo Yang, Haonan Yue, Zhenghao Ma
- **Comment**: Accepted by AAAI2024
- **Journal**: None
- **Summary**: Few-shot object detection (FSOD) aims at extending a generic detector for novel object detection with only a few training examples. It attracts great concerns recently due to the practical meanings. Meta-learning has been demonstrated to be an effective paradigm for this task. In general, methods based on meta-learning employ an additional support branch to encode novel examples (a.k.a. support images) into class prototypes, which are then fused with query branch to facilitate the model prediction. However, the class-level prototypes are difficult to precisely generate, and they also lack detailed information, leading to instability in performance.New methods are required to capture the distinctive local context for more robust novel object detection. To this end, we propose to distill the most representative support features into fine-grained prototypes. These prototypes are then assigned into query feature maps based on the matching results, modeling the detailed feature relations between two branches. This process is realized by our Fine-Grained Feature Aggregation (FFA) module. Moreover, in terms of high-level feature fusion, we propose Balanced Class-Agnostic Sampling (B-CAS) strategy and Non-Linear Fusion (NLF) module from differenct perspectives. They are complementary to each other and depict the high-level feature relations more effectively. Extensive experiments on PASCAL VOC and MS COCO benchmarks show that our method sets a new state-of-the-art performance in most settings. Our code is available at https://github.com/wangchen1801/FPD.



### VeCAF: Vision-language Collaborative Active Finetuning with Training Objective Awareness
- **Arxiv ID**: http://arxiv.org/abs/2401.07853v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2401.07853v2)
- **Published**: 2024-01-15 17:28:37+00:00
- **Updated**: 2024-04-13 10:56:49+00:00
- **Authors**: Rongyu Zhang, Zefan Cai, Huanrui Yang, Zidong Liu, Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata, Kurt Keutzer, Baobao Chang, Yuan Du, Li Du, Shanghang Zhang
- **Comment**: 13 pages
- **Journal**: None
- **Summary**: Finetuning a pretrained vision model (PVM) is a common technique for learning downstream vision tasks. However, the conventional finetuning process with randomly sampled data points results in diminished training efficiency. To address this drawback, we propose a novel approach, Vision-language Collaborative Active Finetuning (VeCAF). With the emerging availability of labels and natural language annotations of images through web-scale crawling or controlled generation, VeCAF makes use of these information to perform parametric data selection for PVM finetuning. VeCAF incorporates the finetuning objective to select significant data points that effectively guide the PVM towards faster convergence to meet the performance goal. This process is assisted by the inherent semantic richness of the text embedding space which we use to augment image features. Furthermore, the flexibility of text-domain augmentation allows VeCAF to handle out-of-distribution scenarios without external data. Extensive experiments show the leading performance and high computational efficiency of VeCAF that is superior to baselines in both in-distribution and out-of-distribution image classification tasks. On ImageNet, VeCAF uses up to 3.3x less training batches to reach the target performance compared to full finetuning, and achieves an accuracy improvement of 2.7% over the state-of-the-art active finetuning method with the same number of batches.



### Information hiding cameras: optical concealment of object information into ordinary images
- **Arxiv ID**: http://arxiv.org/abs/2401.07856v1
- **DOI**: 10.1126/sciadv.adn9420
- **Categories**: **physics.optics**, cs.CV, physics.app-ph
- **Links**: [PDF](http://arxiv.org/pdf/2401.07856v1)
- **Published**: 2024-01-15 17:37:27+00:00
- **Updated**: 2024-01-15 17:37:27+00:00
- **Authors**: Bijie Bai, Ryan Lee, Yuhang Li, Tianyi Gan, Yuntian Wang, Mona Jarrahi, Aydogan Ozcan
- **Comment**: 26 Pages, 8 Figures
- **Journal**: Science Advances (2024)
- **Summary**: Data protection methods like cryptography, despite being effective, inadvertently signal the presence of secret communication, thereby drawing undue attention. Here, we introduce an optical information hiding camera integrated with an electronic decoder, optimized jointly through deep learning. This information hiding-decoding system employs a diffractive optical processor as its front-end, which transforms and hides input images in the form of ordinary-looking patterns that deceive/mislead human observers. This information hiding transformation is valid for infinitely many combinations of secret messages, all of which are transformed into ordinary-looking output patterns, achieved all-optically through passive light-matter interactions within the optical processor. By processing these ordinary-looking output images, a jointly-trained electronic decoder neural network accurately reconstructs the original information hidden within the deceptive output pattern. We numerically demonstrated our approach by designing an information hiding diffractive camera along with a jointly-optimized convolutional decoder neural network. The efficacy of this system was demonstrated under various lighting conditions and noise levels, showing its robustness. We further extended this information hiding camera to multi-spectral operation, allowing the concealment and decoding of multiple images at different wavelengths, all performed simultaneously in a single feed-forward operation. The feasibility of our framework was also demonstrated experimentally using THz radiation. This optical encoder-electronic decoder-based co-design provides a novel information hiding camera interface that is both high-speed and energy-efficient, offering an intriguing solution for visual information security.



### Machine Learning Based Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2401.07929v1
- **DOI**: None
- **Categories**: **cs.CV**, physics.app-ph
- **Links**: [PDF](http://arxiv.org/pdf/2401.07929v1)
- **Published**: 2024-01-15 19:46:05+00:00
- **Updated**: 2024-01-15 19:46:05+00:00
- **Authors**: Md Rakibul Karim Akanda, Joshua Reynolds, Treylin Jackson, Milijah Gray
- **Comment**: None
- **Journal**: None
- **Summary**: Machine learning based object detection as well as tracking that object have been performed in this paper. The authors were able to set a range of interest (ROI) around an object using Open Computer Vision, better known as OpenCV. Next a tracking algorithm has been used to maintain tracking on an object while simultaneously operating two servo motors to keep the object centered in the frame. Detailed procedure and code are included in this paper.



