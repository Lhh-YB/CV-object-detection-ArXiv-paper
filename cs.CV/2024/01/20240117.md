# Arxiv Papers in cs.CV on 2024-01-17
### OCTO+: A Suite for Automatic Open-Vocabulary Object Placement in Mixed Reality
- **Arxiv ID**: http://arxiv.org/abs/2401.08973v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL
- **Links**: [PDF](http://arxiv.org/pdf/2401.08973v1)
- **Published**: 2024-01-17 04:52:40+00:00
- **Updated**: 2024-01-17 04:52:40+00:00
- **Authors**: Aditya Sharma, Luke Yoffe, Tobias HÃ¶llerer
- **Comment**: 2024 IEEE International Conference on Artificial Intelligence and
  eXtended and Virtual Reality (AIXVR)
- **Journal**: None
- **Summary**: One key challenge in Augmented Reality is the placement of virtual content in natural locations. Most existing automated techniques can only work with a closed-vocabulary, fixed set of objects. In this paper, we introduce and evaluate several methods for automatic object placement using recent advances in open-vocabulary vision-language models. Through a multifaceted evaluation, we identify a new state-of-the-art method, OCTO+. We also introduce a benchmark for automatically evaluating the placement of virtual objects in augmented reality, alleviating the need for costly user studies. Through this, in addition to human evaluations, we find that OCTO+ places objects in a valid region over 70% of the time, outperforming other methods on a range of metrics.



### Enhancing Lidar-based Object Detection in Adverse Weather using Offset Sequences in Time
- **Arxiv ID**: http://arxiv.org/abs/2401.09049v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2401.09049v1)
- **Published**: 2024-01-17 08:31:58+00:00
- **Updated**: 2024-01-17 08:31:58+00:00
- **Authors**: Raphael van Kempen, Tim Rehbronn, Abin Jose, Johannes Stegmaier, Bastian Lampe, Timo Woopen, Lutz Eckstein
- **Comment**: Published as part of the III. International Conference on Electrical,
  Computer and Energy Technologies (ICECET 2023), Cape Town, South Africa,
  November 16-17, 2023
- **Journal**: None
- **Summary**: Automated vehicles require an accurate perception of their surroundings for safe and efficient driving. Lidar-based object detection is a widely used method for environment perception, but its performance is significantly affected by adverse weather conditions such as rain and fog. In this work, we investigate various strategies for enhancing the robustness of lidar-based object detection by processing sequential data samples generated by lidar sensors. Our approaches leverage temporal information to improve a lidar object detection model, without the need for additional filtering or pre-processing steps. We compare $10$ different neural network architectures that process point cloud sequences including a novel augmentation strategy introducing a temporal offset between frames of a sequence during training and evaluate the effectiveness of all strategies on lidar point clouds under adverse weather conditions through experiments. Our research provides a comprehensive study of effective methods for mitigating the effects of adverse weather on the reliability of lidar-based object detection using sequential data that are evaluated using public datasets such as nuScenes, Dense, and the Canadian Adverse Driving Conditions Dataset. Our findings demonstrate that our novel method, involving temporal offset augmentation through randomized frame skipping in sequences, enhances object detection accuracy compared to both the baseline model (Pillar-based Object Detection) and no augmentation.



### Objects With Lighting: A Real-World Dataset for Evaluating Reconstruction and Rendering for Object Relighting
- **Arxiv ID**: http://arxiv.org/abs/2401.09126v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2401.09126v2)
- **Published**: 2024-01-17 11:02:52+00:00
- **Updated**: 2024-04-13 16:43:01+00:00
- **Authors**: Benjamin Ummenhofer, Sanskar Agrawal, Rene Sepulveda, Yixing Lao, Kai Zhang, Tianhang Cheng, Stephan Richter, Shenlong Wang, German Ros
- **Comment**: Accepted at 3DV 2024, Oral presentation. For the project page see
  https://github.com/isl-org/objects-with-lighting
- **Journal**: None
- **Summary**: Reconstructing an object from photos and placing it virtually in a new environment goes beyond the standard novel view synthesis task as the appearance of the object has to not only adapt to the novel viewpoint but also to the new lighting conditions and yet evaluations of inverse rendering methods rely on novel view synthesis data or simplistic synthetic datasets for quantitative analysis. This work presents a real-world dataset for measuring the reconstruction and rendering of objects for relighting. To this end, we capture the environment lighting and ground truth images of the same objects in multiple environments allowing to reconstruct the objects from images taken in one environment and quantify the quality of the rendered views for the unseen lighting environments. Further, we introduce a simple baseline composed of off-the-shelf methods and test several state-of-the-art methods on the relighting task and show that novel view synthesis is not a reliable proxy to measure performance. Code and dataset are available at https://github.com/isl-org/objects-with-lighting .



### SM$^3$: Self-Supervised Multi-task Modeling with Multi-view 2D Images for Articulated Objects
- **Arxiv ID**: http://arxiv.org/abs/2401.09133v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2401.09133v1)
- **Published**: 2024-01-17 11:15:09+00:00
- **Updated**: 2024-01-17 11:15:09+00:00
- **Authors**: Haowen Wang, Zhen Zhao, Zhao Jin, Zhengping Che, Liang Qiao, Yakun Huang, Zhipeng Fan, Xiuquan Qiao, Jian Tang
- **Comment**: None
- **Journal**: None
- **Summary**: Reconstructing real-world objects and estimating their movable joint structures are pivotal technologies within the field of robotics. Previous research has predominantly focused on supervised approaches, relying on extensively annotated datasets to model articulated objects within limited categories. However, this approach falls short of effectively addressing the diversity present in the real world. To tackle this issue, we propose a self-supervised interaction perception method, referred to as SM$^3$, which leverages multi-view RGB images captured before and after interaction to model articulated objects, identify the movable parts, and infer the parameters of their rotating joints. By constructing 3D geometries and textures from the captured 2D images, SM$^3$ achieves integrated optimization of movable part and joint parameters during the reconstruction process, obviating the need for annotations. Furthermore, we introduce the MMArt dataset, an extension of PartNet-Mobility, encompassing multi-view and multi-modal data of articulated objects spanning diverse categories. Evaluations demonstrate that SM$^3$ surpasses existing benchmarks across various categories and objects, while its adaptability in real-world scenarios has been thoroughly validated.



