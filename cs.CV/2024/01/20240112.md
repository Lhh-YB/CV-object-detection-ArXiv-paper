# Arxiv Papers in cs.CV on 2024-01-12
### Improving the Detection of Small Oriented Objects in Aerial Images
- **Arxiv ID**: http://arxiv.org/abs/2401.06503v1
- **DOI**: 10.1109/WACVW58289.2023.00023
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2401.06503v1)
- **Published**: 2024-01-12 11:00:07+00:00
- **Updated**: 2024-01-12 11:00:07+00:00
- **Authors**: Chandler Timm C. Doloriel, Rhandley D. Cajote
- **Comment**: C. T. C. Doloriel and R. D. Cajote, "Improving the Detection of Small
  Oriented Objects in Aerial Images," 2023 IEEE/CVF Winter Conference on
  Applications of Computer Vision Workshops (WACVW), Waikoloa, HI, USA, 2023,
  pp. 176-185, doi: 10.1109/WACVW58289.2023.00023
- **Journal**: None
- **Summary**: Small oriented objects that represent tiny pixel-area in large-scale aerial images are difficult to detect due to their size and orientation. Existing oriented aerial detectors have shown promising results but are mainly focused on orientation modeling with less regard to the size of the objects. In this work, we proposed a method to accurately detect small oriented objects in aerial images by enhancing the classification and regression tasks of the oriented object detection model. We designed the Attention-Points Network consisting of two losses: Guided-Attention Loss (GALoss) and Box-Points Loss (BPLoss). GALoss uses an instance segmentation mask as ground-truth to learn the attention features needed to improve the detection of small objects. These attention features are then used to predict box points for BPLoss, which determines the points' position relative to the target oriented bounding box. Experimental results show the effectiveness of our Attention-Points Network on a standard oriented aerial dataset with small object instances (DOTA-v1.5) and on a maritime-related dataset (HRSC2016). The code is publicly available.



### Robustness-Aware 3D Object Detection in Autonomous Driving: A Review and Outlook
- **Arxiv ID**: http://arxiv.org/abs/2401.06542v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2401.06542v3)
- **Published**: 2024-01-12 12:35:45+00:00
- **Updated**: 2024-08-15 14:07:04+00:00
- **Authors**: Ziying Song, Lin Liu, Feiyang Jia, Yadan Luo, Guoxin Zhang, Lei Yang, Li Wang, Caiyan Jia
- **Comment**: None
- **Journal**: None
- **Summary**: In the realm of modern autonomous driving, the perception system is indispensable for accurately assessing the state of the surrounding environment, thereby enabling informed prediction and planning. The key step to this system is related to 3D object detection that utilizes vehicle-mounted sensors such as LiDAR and cameras to identify the size, the category, and the location of nearby objects. Despite the surge in 3D object detection methods aimed at enhancing detection precision and efficiency, there is a gap in the literature that systematically examines their resilience against environmental variations, noise, and weather changes. This study emphasizes the importance of robustness, alongside accuracy and latency, in evaluating perception systems under practical scenarios. Our work presents an extensive survey of camera-only, LiDAR-only, and multi-modal 3D object detection algorithms, thoroughly evaluating their trade-off between accuracy, latency, and robustness, particularly on datasets like KITTI-C and nuScenes-C to ensure fair comparisons. Among these, multi-modal 3D detection approaches exhibit superior robustness, and a novel taxonomy is introduced to reorganize the literature for enhanced clarity. This survey aims to offer a more practical perspective on the current capabilities and the constraints of 3D object detection algorithms in real-world applications, thus steering future research towards robustness-centric advancements.



