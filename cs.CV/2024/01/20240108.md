# Arxiv Papers in cs.CV on 2024-01-08
### A Flying Bird Object Detection Method for Surveillance Video
- **Arxiv ID**: http://arxiv.org/abs/2401.03749v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2401.03749v3)
- **Published**: 2024-01-08 09:20:46+00:00
- **Updated**: 2024-08-29 08:52:40+00:00
- **Authors**: Ziwei Sun, Zexi Hua, Hengchao Li, Yan Li
- **Comment**: None
- **Journal**: in IEEE Transactions on Instrumentation and Measurement, vol. 73,
  pp. 1-14, 2024
- **Summary**: Aiming at the specific characteristics of flying bird objects in surveillance video, such as the typically non-obvious features in single-frame images, small size in most instances, and asymmetric shapes, this paper proposes a Flying Bird Object Detection method for Surveillance Video (FBOD-SV). Firstly, a new feature aggregation module, the Correlation Attention Feature Aggregation (Co-Attention-FA) module, is designed to aggregate the features of the flying bird object according to the bird object's correlation on multiple consecutive frames of images. Secondly, a Flying Bird Object Detection Network (FBOD-Net) with down-sampling followed by up-sampling is designed, which utilizes a large feature layer that fuses fine spatial information and large receptive field information to detect special multi-scale (mostly small-scale) bird objects. Finally, the SimOTA dynamic label allocation method is applied to One-Category object detection, and the SimOTA-OC dynamic label strategy is proposed to solve the difficult problem of label allocation caused by irregular flying bird objects. In this paper, the performance of the FBOD-SV is validated using experimental datasets of flying bird objects in traction substation surveillance videos. The experimental results show that the FBOD-SV effectively improves the detection performance of flying bird objects in surveillance video.



### UFO: Unidentified Foreground Object Detection in 3D Point Cloud
- **Arxiv ID**: http://arxiv.org/abs/2401.03846v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2401.03846v1)
- **Published**: 2024-01-08 12:16:06+00:00
- **Updated**: 2024-01-08 12:16:06+00:00
- **Authors**: Hyunjun Choi, Hawook Jeong, Jin Young Choi
- **Comment**: Under review
- **Journal**: None
- **Summary**: In this paper, we raise a new issue on Unidentified Foreground Object (UFO) detection in 3D point clouds, which is a crucial technology in autonomous driving in the wild. UFO detection is challenging in that existing 3D object detectors encounter extremely hard challenges in both 3D localization and Out-of-Distribution (OOD) detection. To tackle these challenges, we suggest a new UFO detection framework including three tasks: evaluation protocol, methodology, and benchmark. The evaluation includes a new approach to measure the performance on our goal, i.e. both localization and OOD detection of UFOs. The methodology includes practical techniques to enhance the performance of our goal. The benchmark is composed of the KITTI Misc benchmark and our additional synthetic benchmark for modeling a more diverse range of UFOs. The proposed framework consistently enhances performance by a large margin across all four baseline detectors: SECOND, PointPillars, PV-RCNN, and PartA2, giving insight for future work on UFO detection in the wild.



### Robustness Assessment of a Runway Object Classifier for Safe Aircraft Taxiing
- **Arxiv ID**: http://arxiv.org/abs/2402.00035v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, cs.LO
- **Links**: [PDF](http://arxiv.org/pdf/2402.00035v4)
- **Published**: 2024-01-08 12:19:46+00:00
- **Updated**: 2024-08-06 17:36:06+00:00
- **Authors**: Yizhak Elboher, Raya Elsaleh, Omri Isac, Mélanie Ducoffe, Audrey Galametz, Guillaume Povéda, Ryma Boumazouza, Noémie Cohen, Guy Katz
- **Comment**: This is a preprint version of the paper in the proceedings of 43rd
  Digital Avionics Systems Conference (DASC)
- **Journal**: None
- **Summary**: As deep neural networks (DNNs) are becoming the prominent solution for many computational problems, the aviation industry seeks to explore their potential in alleviating pilot workload and in improving operational safety. However, the use of DNNs in this type of safety-critical applications requires a thorough certification process. This need can be addressed through formal verification, which provides rigorous assurances -- e.g.,~by proving the absence of certain mispredictions. In this case-study paper, we demonstrate this process using an image-classifier DNN currently under development at Airbus and intended for use during the aircraft taxiing phase. We use formal methods to assess this DNN's robustness to three common image perturbation types: noise, brightness and contrast, and some of their combinations. This process entails multiple invocations of the underlying verifier, which might be computationally expensive; and we therefore propose a method that leverages the monotonicity of these robustness properties, as well as the results of past verification queries, in order to reduce the overall number of verification queries required by nearly 60%. Our results provide an indication of the level of robustness achieved by the DNN classifier under study, and indicate that it is considerably more vulnerable to noise than to brightness or contrast perturbations.



### A New Dataset and a Distractor-Aware Architecture for Transparent Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2401.03872v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2401.03872v1)
- **Published**: 2024-01-08 13:04:28+00:00
- **Updated**: 2024-01-08 13:04:28+00:00
- **Authors**: Alan Lukezic, Ziga Trojer, Jiri Matas, Matej Kristan
- **Comment**: Under the review. arXiv admin note: substantial text overlap with
  arXiv:2210.03436
- **Journal**: None
- **Summary**: Performance of modern trackers degrades substantially on transparent objects compared to opaque objects. This is largely due to two distinct reasons. Transparent objects are unique in that their appearance is directly affected by the background. Furthermore, transparent object scenes often contain many visually similar objects (distractors), which often lead to tracking failure. However, development of modern tracking architectures requires large training sets, which do not exist in transparent object tracking. We present two contributions addressing the aforementioned issues. We propose the first transparent object tracking training dataset Trans2k that consists of over 2k sequences with 104,343 images overall, annotated by bounding boxes and segmentation masks. Standard trackers trained on this dataset consistently improve by up to 16%. Our second contribution is a new distractor-aware transparent object tracker (DiTra) that treats localization accuracy and target identification as separate tasks and implements them by a novel architecture. DiTra sets a new state-of-the-art in transparent object tracking and generalizes well to opaque objects.



### RoboFusion: Towards Robust Multi-Modal 3D Object Detection via SAM
- **Arxiv ID**: http://arxiv.org/abs/2401.03907v4
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2401.03907v4)
- **Published**: 2024-01-08 14:10:24+00:00
- **Updated**: 2024-04-23 12:48:23+00:00
- **Authors**: Ziying Song, Guoxing Zhang, Lin Liu, Lei Yang, Shaoqing Xu, Caiyan Jia, Feiyang Jia, Li Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-modal 3D object detectors are dedicated to exploring secure and reliable perception systems for autonomous driving (AD).Although achieving state-of-the-art (SOTA) performance on clean benchmark datasets, they tend to overlook the complexity and harsh conditions of real-world environments. With the emergence of visual foundation models (VFMs), opportunities and challenges are presented for improving the robustness and generalization of multi-modal 3D object detection in AD. Therefore, we propose RoboFusion, a robust framework that leverages VFMs like SAM to tackle out-of-distribution (OOD) noise scenarios. We first adapt the original SAM for AD scenarios named SAM-AD. To align SAM or SAM-AD with multi-modal methods, we then introduce AD-FPN for upsampling the image features extracted by SAM. We employ wavelet decomposition to denoise the depth-guided images for further noise reduction and weather interference. At last, we employ self-attention mechanisms to adaptively reweight the fused features, enhancing informative features while suppressing excess noise. In summary, RoboFusion significantly reduces noise by leveraging the generalization and robustness of VFMs, thereby enhancing the resilience of multi-modal 3D object detection. Consequently, RoboFusion achieves SOTA performance in noisy scenarios, as demonstrated by the KITTI-C and nuScenes-C benchmarks. Code is available at https://github.com/adept-thu/RoboFusion.



### SOAP: Cross-sensor Domain Adaptation for 3D Object Detection Using Stationary Object Aggregation Pseudo-labelling
- **Arxiv ID**: http://arxiv.org/abs/2401.04230v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2401.04230v1)
- **Published**: 2024-01-08 20:41:08+00:00
- **Updated**: 2024-01-08 20:41:08+00:00
- **Authors**: Chengjie Huang, Vahdat Abdelzad, Sean Sedwards, Krzysztof Czarnecki
- **Comment**: Accepted by WACV 2024
- **Journal**: None
- **Summary**: We consider the problem of cross-sensor domain adaptation in the context of LiDAR-based 3D object detection and propose Stationary Object Aggregation Pseudo-labelling (SOAP) to generate high quality pseudo-labels for stationary objects. In contrast to the current state-of-the-art in-domain practice of aggregating just a few input scans, SOAP aggregates entire sequences of point clouds at the input level to reduce the sensor domain gap. Then, by means of what we call quasi-stationary training and spatial consistency post-processing, the SOAP model generates accurate pseudo-labels for stationary objects, closing a minimum of 30.3% domain gap compared to few-frame detectors. Our results also show that state-of-the-art domain adaptation approaches can achieve even greater performance in combination with SOAP, in both the unsupervised and semi-supervised settings.



