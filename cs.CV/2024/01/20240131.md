# Arxiv Papers in cs.CV on 2024-01-31
### Do Object Detection Localization Errors Affect Human Performance and Trust?
- **Arxiv ID**: http://arxiv.org/abs/2401.17821v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.HC
- **Links**: [PDF](http://arxiv.org/pdf/2401.17821v1)
- **Published**: 2024-01-31 13:24:51+00:00
- **Updated**: 2024-01-31 13:24:51+00:00
- **Authors**: Sven de Witte, Ombretta Strafforello, Jan van Gemert
- **Comment**: None
- **Journal**: None
- **Summary**: Bounding boxes are often used to communicate automatic object detection results to humans, aiding humans in a multitude of tasks. We investigate the relationship between bounding box localization errors and human task performance. We use observer performance studies on a visual multi-object counting task to measure both human trust and performance with different levels of bounding box accuracy. The results show that localization errors have no significant impact on human accuracy or trust in the system. Recall and precision errors impact both human performance and trust, suggesting that optimizing algorithms based on the F1 score is more beneficial in human-computer tasks. Lastly, the paper offers an improvement on bounding boxes in multi-object counting tasks with center dots, showing improved performance and better resilience to localization inaccuracy.



### VR-based generation of photorealistic synthetic data for training hand-object tracking models
- **Arxiv ID**: http://arxiv.org/abs/2401.17874v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2401.17874v2)
- **Published**: 2024-01-31 14:32:56+00:00
- **Updated**: 2024-02-02 00:21:31+00:00
- **Authors**: Chengyan Zhang, Rahul Chaudhari
- **Comment**: None
- **Journal**: None
- **Summary**: Supervised learning models for precise tracking of hand-object interactions (HOI) in 3D require large amounts of annotated data for training. Moreover, it is not intuitive for non-experts to label 3D ground truth (e.g. 6DoF object pose) on 2D images. To address these issues, we present "blender-hoisynth", an interactive synthetic data generator based on the Blender software. Blender-hoisynth can scalably generate and automatically annotate visual HOI training data. Other competing approaches usually generate synthetic HOI data compeletely without human input. While this may be beneficial in some scenarios, HOI applications inherently necessitate direct control over the HOIs as an expression of human intent. With blender-hoisynth, it is possible for users to interact with objects via virtual hands using standard Virtual Reality hardware. The synthetically generated data are characterized by a high degree of photorealism and contain visually plausible and physically realistic videos of hands grasping objects and moving them around in 3D. To demonstrate the efficacy of our data generation, we replace large parts of the training data in the well-known DexYCB dataset with hoisynth data and train a state-of-the-art HOI reconstruction model with it. We show that there is no significant degradation in the model performance despite the data replacement.



### Source-free Domain Adaptive Object Detection in Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/2401.17916v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2401.17916v1)
- **Published**: 2024-01-31 15:32:44+00:00
- **Updated**: 2024-01-31 15:32:44+00:00
- **Authors**: Weixing Liu, Jun Liu, Xin Su, Han Nie, Bin Luo
- **Comment**: 14 pages, 11 figures
- **Journal**: None
- **Summary**: Recent studies have used unsupervised domain adaptive object detection (UDAOD) methods to bridge the domain gap in remote sensing (RS) images. However, UDAOD methods typically assume that the source domain data can be accessed during the domain adaptation process. This setting is often impractical in the real world due to RS data privacy and transmission difficulty. To address this challenge, we propose a practical source-free object detection (SFOD) setting for RS images, which aims to perform target domain adaptation using only the source pre-trained model. We propose a new SFOD method for RS images consisting of two parts: perturbed domain generation and alignment. The proposed multilevel perturbation constructs the perturbed domain in a simple yet efficient form by perturbing the domain-variant features at the image level and feature level according to the color and style bias. The proposed multilevel alignment calculates feature and label consistency between the perturbed domain and the target domain across the teacher-student network, and introduces the distillation of feature prototype to mitigate the noise of pseudo-labels. By requiring the detector to be consistent in the perturbed domain and the target domain, the detector is forced to focus on domaininvariant features. Extensive results of three synthetic-to-real experiments and three cross-sensor experiments have validated the effectiveness of our method which does not require access to source domain RS images. Furthermore, experiments on computer vision datasets show that our method can be extended to other fields as well. Our code will be available at: https://weixliu.github.io/ .



### MelNet: A Real-Time Deep Learning Algorithm for Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2401.17972v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2401.17972v1)
- **Published**: 2024-01-31 16:27:47+00:00
- **Updated**: 2024-01-31 16:27:47+00:00
- **Authors**: Yashar Azadvatan, Murat Kurt
- **Comment**: 11 pages, 9 figures, 5 tables
- **Journal**: None
- **Summary**: In this study, a novel deep learning algorithm for object detection, named MelNet, was introduced. MelNet underwent training utilizing the KITTI dataset for object detection. Following 300 training epochs, MelNet attained an mAP (mean average precision) score of 0.732. Additionally, three alternative models -YOLOv5, EfficientDet, and Faster-RCNN-MobileNetv3- were trained on the KITTI dataset and juxtaposed with MelNet for object detection.   The outcomes underscore the efficacy of employing transfer learning in certain instances. Notably, preexisting models trained on prominent datasets (e.g., ImageNet, COCO, and Pascal VOC) yield superior results. Another finding underscores the viability of creating a new model tailored to a specific scenario and training it on a specific dataset. This investigation demonstrates that training MelNet exclusively on the KITTI dataset also surpasses EfficientDet after 150 epochs. Consequently, post-training, MelNet's performance closely aligns with that of other pre-trained models.



### Real-time Traffic Object Detection for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2402.00128v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2402.00128v2)
- **Published**: 2024-01-31 19:12:56+00:00
- **Updated**: 2024-02-29 18:58:43+00:00
- **Authors**: Abdul Hannan Khan, Syed Tahseen Raza Rizvi, Andreas Dengel
- **Comment**: \c{opyright} 20XX IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
- **Journal**: None
- **Summary**: With recent advances in computer vision, it appears that autonomous driving will be part of modern society sooner rather than later. However, there are still a significant number of concerns to address. Although modern computer vision techniques demonstrate superior performance, they tend to prioritize accuracy over efficiency, which is a crucial aspect of real-time applications. Large object detection models typically require higher computational power, which is achieved by using more sophisticated onboard hardware. For autonomous driving, these requirements translate to increased fuel costs and, ultimately, a reduction in mileage. Further, despite their computational demands, the existing object detectors are far from being real-time. In this research, we assess the robustness of our previously proposed, highly efficient pedestrian detector LSFM on well-established autonomous driving benchmarks, including diverse weather conditions and nighttime scenes. Moreover, we extend our LSFM model for general object detection to achieve real-time object detection in traffic scenes. We evaluate its performance, low latency, and generalizability on traffic object detection datasets. Furthermore, we discuss the inadequacy of the current key performance indicator employed by object detection systems in the context of autonomous driving and propose a more suitable alternative that incorporates real-time requirements.



### Improving Object Detection Quality in Football Through Super-Resolution Techniques
- **Arxiv ID**: http://arxiv.org/abs/2402.00163v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2402.00163v1)
- **Published**: 2024-01-31 20:37:35+00:00
- **Updated**: 2024-01-31 20:37:35+00:00
- **Authors**: Karolina Seweryn, Gabriel Chęć, Szymon Łukasik, Anna Wróblewska
- **Comment**: None
- **Journal**: None
- **Summary**: This study explores the potential of super-resolution techniques in enhancing object detection accuracy in football. Given the sport's fast-paced nature and the critical importance of precise object (e.g. ball, player) tracking for both analysis and broadcasting, super-resolution could offer significant improvements. We investigate how advanced image processing through super-resolution impacts the accuracy and reliability of object detection algorithms in processing football match footage.   Our methodology involved applying state-of-the-art super-resolution techniques to a diverse set of football match videos from SoccerNet, followed by object detection using Faster R-CNN. The performance of these algorithms, both with and without super-resolution enhancement, was rigorously evaluated in terms of detection accuracy.   The results indicate a marked improvement in object detection accuracy when super-resolution preprocessing is applied. The improvement of object detection through the integration of super-resolution techniques yields significant benefits, especially for low-resolution scenarios, with a notable 12\% increase in mean Average Precision (mAP) at an IoU (Intersection over Union) range of 0.50:0.95 for 320x240 size images when increasing the resolution fourfold using RLFN. As the dimensions increase, the magnitude of improvement becomes more subdued; however, a discernible improvement in the quality of detection is consistently evident. Additionally, we discuss the implications of these findings for real-time sports analytics, player tracking, and the overall viewing experience. The study contributes to the growing field of sports technology by demonstrating the practical benefits and limitations of integrating super-resolution techniques in football analytics and broadcasting.



### MOD-CL: Multi-label Object Detection with Constrained Loss
- **Arxiv ID**: http://arxiv.org/abs/2403.07885v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2403.07885v1)
- **Published**: 2024-01-31 23:13:20+00:00
- **Updated**: 2024-01-31 23:13:20+00:00
- **Authors**: Sota Moriyama, Koji Watanabe, Katsumi Inoue, Akihiro Takemura
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce MOD-CL, a multi-label object detection framework that utilizes constrained loss in the training process to produce outputs that better satisfy the given requirements. In this paper, we use $\mathrm{MOD_{YOLO}}$, a multi-label object detection model built upon the state-of-the-art object detection model YOLOv8, which has been published in recent years. In Task 1, we introduce the Corrector Model and Blender Model, two new models that follow after the object detection process, aiming to generate a more constrained output. For Task 2, constrained losses have been incorporated into the $\mathrm{MOD_{YOLO}}$ architecture using Product T-Norm. The results show that these implementations are instrumental to improving the scores for both Task 1 and Task 2.



### Capacity Constraint Analysis Using Object Detection for Smart Manufacturing
- **Arxiv ID**: http://arxiv.org/abs/2402.00243v1
- **DOI**: None
- **Categories**: **cs.CV**, I.2.10; I.4.8; I.4.9; I.5.1; I.5.4
- **Links**: [PDF](http://arxiv.org/pdf/2402.00243v1)
- **Published**: 2024-01-31 23:52:14+00:00
- **Updated**: 2024-01-31 23:52:14+00:00
- **Authors**: Hafiz Mughees Ahmad, Afshin Rahimi, Khizer Hayat
- **Comment**: None
- **Journal**: None
- **Summary**: The increasing popularity of Deep Learning (DL) based Object Detection (OD) methods and their real-world applications have opened new venues in smart manufacturing. Traditional industries struck by capacity constraints after Coronavirus Disease (COVID-19) require non-invasive methods for in-depth operations' analysis to optimize and increase their revenue. In this study, we have initially developed a Convolutional Neural Network (CNN) based OD model to tackle this issue. This model is trained to accurately identify the presence of chairs and individuals on the production floor. The identified objects are then passed to the CNN based tracker, which tracks them throughout their life cycle in the workstation. The extracted meta-data is further processed through a novel framework for the capacity constraint analysis. We identified that the Station C is only 70.6% productive through 6 months. Additionally, the time spent at each station is recorded and aggregated for each object. This data proves helpful in conducting annual audits and effectively managing labor and material over time.



