# Arxiv Papers in cs.CV on 2024-03-06
### A Density-Guided Temporal Attention Transformer for Indiscernible Object Counting in Underwater Video
- **Arxiv ID**: http://arxiv.org/abs/2403.03461v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2403.03461v1)
- **Published**: 2024-03-06 04:54:00+00:00
- **Updated**: 2024-03-06 04:54:00+00:00
- **Authors**: Cheng-Yen Yang, Hsiang-Wei Huang, Zhongyu Jiang, Hao Wang, Farron Wallace, Jenq-Neng Hwang
- **Comment**: Accepted by ICASSP 2024 (IEEE International Conference on Acoustics,
  Speech, and Signal Processing)
- **Journal**: None
- **Summary**: Dense object counting or crowd counting has come a long way thanks to the recent development in the vision community. However, indiscernible object counting, which aims to count the number of targets that are blended with respect to their surroundings, has been a challenge. Image-based object counting datasets have been the mainstream of the current publicly available datasets. Therefore, we propose a large-scale dataset called YoutubeFish-35, which contains a total of 35 sequences of high-definition videos with high frame-per-second and more than 150,000 annotated center points across a selected variety of scenes. For benchmarking purposes, we select three mainstream methods for dense object counting and carefully evaluate them on the newly collected dataset. We propose TransVidCount, a new strong baseline that combines density and regression branches along the temporal domain in a unified framework and can effectively tackle indiscernible object counting with state-of-the-art performance on YoutubeFish-35 dataset.



### Continual Segmentation with Disentangled Objectness Learning and Class Recognition
- **Arxiv ID**: http://arxiv.org/abs/2403.03477v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2403.03477v3)
- **Published**: 2024-03-06 05:33:50+00:00
- **Updated**: 2024-04-01 03:35:25+00:00
- **Authors**: Yizheng Gong, Siyue Yu, Xiaoyang Wang, Jimin Xiao
- **Comment**: Accepted to CVPR 2024
- **Journal**: None
- **Summary**: Most continual segmentation methods tackle the problem as a per-pixel classification task. However, such a paradigm is very challenging, and we find query-based segmenters with built-in objectness have inherent advantages compared with per-pixel ones, as objectness has strong transfer ability and forgetting resistance. Based on these findings, we propose CoMasTRe by disentangling continual segmentation into two stages: forgetting-resistant continual objectness learning and well-researched continual classification. CoMasTRe uses a two-stage segmenter learning class-agnostic mask proposals at the first stage and leaving recognition to the second stage. During continual learning, a simple but effective distillation is adopted to strengthen objectness. To further mitigate the forgetting of old classes, we design a multi-label class distillation strategy suited for segmentation. We assess the effectiveness of CoMasTRe on PASCAL VOC and ADE20K. Extensive experiments show that our method outperforms per-pixel and query-based methods on both datasets. Code will be available at https://github.com/jordangong/CoMasTRe.



### VastTrack: Vast Category Visual Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2403.03493v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2403.03493v1)
- **Published**: 2024-03-06 06:39:43+00:00
- **Updated**: 2024-03-06 06:39:43+00:00
- **Authors**: Liang Peng, Junyuan Gao, Xinran Liu, Weihong Li, Shaohua Dong, Zhipeng Zhang, Heng Fan, Libo Zhang
- **Comment**: Tech. report
- **Journal**: None
- **Summary**: In this paper, we introduce a novel benchmark, dubbed VastTrack, towards facilitating the development of more general visual tracking via encompassing abundant classes and videos. VastTrack possesses several attractive properties: (1) Vast Object Category. In particular, it covers target objects from 2,115 classes, largely surpassing object categories of existing popular benchmarks (e.g., GOT-10k with 563 classes and LaSOT with 70 categories). With such vast object classes, we expect to learn more general object tracking. (2) Larger scale. Compared with current benchmarks, VastTrack offers 50,610 sequences with 4.2 million frames, which makes it to date the largest benchmark regarding the number of videos, and thus could benefit training even more powerful visual trackers in the deep learning era. (3) Rich Annotation. Besides conventional bounding box annotations, VastTrack also provides linguistic descriptions for the videos. The rich annotations of VastTrack enables development of both the vision-only and the vision-language tracking. To ensure precise annotation, all videos are manually labeled with multiple rounds of careful inspection and refinement. To understand performance of existing trackers and to provide baselines for future comparison, we extensively assess 25 representative trackers. The results, not surprisingly, show significant drops compared to those on current datasets due to lack of abundant categories and videos from diverse scenarios for training, and more efforts are required to improve general tracking. Our VastTrack and all the evaluation results will be made publicly available https://github.com/HengLan/VastTrack.



### 3D Object Visibility Prediction in Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2403.03681v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2403.03681v1)
- **Published**: 2024-03-06 13:07:42+00:00
- **Updated**: 2024-03-06 13:07:42+00:00
- **Authors**: Chuanyu Luo, Nuo Cheng, Ren Zhong, Haipeng Jiang, Wenyu Chen, Aoli Wang, Pu Li
- **Comment**: None
- **Journal**: None
- **Summary**: With the rapid advancement of hardware and software technologies, research in autonomous driving has seen significant growth. The prevailing framework for multi-sensor autonomous driving encompasses sensor installation, perception, path planning, decision-making, and motion control. At the perception phase, a common approach involves utilizing neural networks to infer 3D bounding box (Bbox) attributes from raw sensor data, including classification, size, and orientation. In this paper, we present a novel attribute and its corresponding algorithm: 3D object visibility. By incorporating multi-task learning, the introduction of this attribute, visibility, negligibly affects the model's effectiveness and efficiency. Our proposal of this attribute and its computational strategy aims to expand the capabilities for downstream tasks, thereby enhancing the safety and reliability of real-time autonomous driving in real-world scenarios.



### CMDA: Cross-Modal and Domain Adversarial Adaptation for LiDAR-Based 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2403.03721v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2403.03721v2)
- **Published**: 2024-03-06 14:12:38+00:00
- **Updated**: 2024-03-07 02:20:27+00:00
- **Authors**: Gyusam Chang, Wonseok Roh, Sujin Jang, Dongwook Lee, Daehyun Ji, Gyeongrok Oh, Jinsun Park, Jinkyu Kim, Sangpil Kim
- **Comment**: Accepted by AAAI 2024
- **Journal**: None
- **Summary**: Recent LiDAR-based 3D Object Detection (3DOD) methods show promising results, but they often do not generalize well to target domains outside the source (or training) data distribution. To reduce such domain gaps and thus to make 3DOD models more generalizable, we introduce a novel unsupervised domain adaptation (UDA) method, called CMDA, which (i) leverages visual semantic cues from an image modality (i.e., camera images) as an effective semantic bridge to close the domain gap in the cross-modal Bird's Eye View (BEV) representations. Further, (ii) we also introduce a self-training-based learning strategy, wherein a model is adversarially trained to generate domain-invariant features, which disrupt the discrimination of whether a feature instance comes from a source or an unseen target domain. Overall, our CMDA framework guides the 3DOD model to generate highly informative and domain-adaptive features for novel data distributions. In our extensive experiments with large-scale benchmarks, such as nuScenes, Waymo, and KITTI, those mentioned above provide significant performance gains for UDA tasks, achieving state-of-the-art performance.



### Learning 3D object-centric representation through prediction
- **Arxiv ID**: http://arxiv.org/abs/2403.03730v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, I.2.10; I.4.8; I.4.6; I.4.10; I.2.6
- **Links**: [PDF](http://arxiv.org/pdf/2403.03730v1)
- **Published**: 2024-03-06 14:19:11+00:00
- **Updated**: 2024-03-06 14:19:11+00:00
- **Authors**: John Day, Tushar Arora, Jirui Liu, Li Erran Li, Ming Bo Cai
- **Comment**: 21 pages, 11 figures. Project webpage can be found at
  https://jday54.github.io/opple_site/
- **Journal**: None
- **Summary**: As part of human core knowledge, the representation of objects is the building block of mental representation that supports high-level concepts and symbolic reasoning. While humans develop the ability of perceiving objects situated in 3D environments without supervision, models that learn the same set of abilities with similar constraints faced by human infants are lacking. Towards this end, we developed a novel network architecture that simultaneously learns to 1) segment objects from discrete images, 2) infer their 3D locations, and 3) perceive depth, all while using only information directly available to the brain as training data, namely: sequences of images and self-motion. The core idea is treating objects as latent causes of visual input which the brain uses to make efficient predictions of future scenes. This results in object representations being learned as an essential byproduct of learning to predict.



### Investigation of the Impact of Synthetic Training Data in the Industrial Application of Terminal Strip Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2403.04809v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2403.04809v1)
- **Published**: 2024-03-06 18:33:27+00:00
- **Updated**: 2024-03-06 18:33:27+00:00
- **Authors**: Nico Baumgart, Markus Lange-Hegermann, Mike Mücke
- **Comment**: None
- **Journal**: None
- **Summary**: In industrial manufacturing, numerous tasks of visually inspecting or detecting specific objects exist that are currently performed manually or by classical image processing methods. Therefore, introducing recent deep learning models to industrial environments holds the potential to increase productivity and enable new applications. However, gathering and labeling sufficient data is often intractable, complicating the implementation of such projects. Hence, image synthesis methods are commonly used to generate synthetic training data from 3D models and annotate them automatically, although it results in a sim-to-real domain gap. In this paper, we investigate the sim-to-real generalization performance of standard object detectors on the complex industrial application of terminal strip object detection. Combining domain randomization and domain knowledge, we created an image synthesis pipeline for automatically generating the training data. Moreover, we manually annotated 300 real images of terminal strips for the evaluation. The results show the cruciality of the objects of interest to have the same scale in either domain. Nevertheless, under optimized scaling conditions, the sim-to-real performance difference in mean average precision amounts to 2.69 % for RetinaNet and 0.98 % for Faster R-CNN, qualifying this approach for industrial requirements.



### Multi-Object Tracking with Camera-LiDAR Fusion for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2403.04112v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2403.04112v2)
- **Published**: 2024-03-06 23:49:16+00:00
- **Updated**: 2024-05-12 17:25:55+00:00
- **Authors**: Riccardo Pieroni, Simone Specchia, Matteo Corno, Sergio Matteo Savaresi
- **Comment**: Published at IEEE European Control Conference 2024
- **Journal**: None
- **Summary**: This paper presents a novel multi-modal Multi-Object Tracking (MOT) algorithm for self-driving cars that combines camera and LiDAR data. Camera frames are processed with a state-of-the-art 3D object detector, whereas classical clustering techniques are used to process LiDAR observations. The proposed MOT algorithm comprises a three-step association process, an Extended Kalman filter for estimating the motion of each detected dynamic obstacle, and a track management phase. The EKF motion model requires the current measured relative position and orientation of the observed object and the longitudinal and angular velocities of the ego vehicle as inputs. Unlike most state-of-the-art multi-modal MOT approaches, the proposed algorithm does not rely on maps or knowledge of the ego global pose. Moreover, it uses a 3D detector exclusively for cameras and is agnostic to the type of LiDAR sensor used. The algorithm is validated both in simulation and with real-world data, with satisfactory results.



