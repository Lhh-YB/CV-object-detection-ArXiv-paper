# Arxiv Papers in cs.CV on 2024-03-20
### ManiPose: A Comprehensive Benchmark for Pose-aware Object Manipulation in Robotics
- **Arxiv ID**: http://arxiv.org/abs/2403.13365v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2403.13365v1)
- **Published**: 2024-03-20 07:48:32+00:00
- **Updated**: 2024-03-20 07:48:32+00:00
- **Authors**: Qiaojun Yu, Ce Hao, Junbo Wang, Wenhai Liu, Liu Liu, Yao Mu, Yang You, Hengxu Yan, Cewu Lu
- **Comment**: 8 pages, 7 figures, submitted to 2024 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS 2024)
- **Journal**: None
- **Summary**: Robotic manipulation in everyday scenarios, especially in unstructured environments, requires skills in pose-aware object manipulation (POM), which adapts robots' grasping and handling according to an object's 6D pose. Recognizing an object's position and orientation is crucial for effective manipulation. For example, if a mug is lying on its side, it's more effective to grasp it by the rim rather than the handle. Despite its importance, research in POM skills remains limited, because learning manipulation skills requires pose-varying simulation environments and datasets. This paper introduces ManiPose, a pioneering benchmark designed to advance the study of pose-varying manipulation tasks. ManiPose encompasses: 1) Simulation environments for POM feature tasks ranging from 6D pose-specific pick-and-place of single objects to cluttered scenes, further including interactions with articulated objects. 2) A comprehensive dataset featuring geometrically consistent and manipulation-oriented 6D pose labels for 2936 real-world scanned rigid objects and 100 articulated objects across 59 categories. 3) A baseline for POM, leveraging the inferencing abilities of LLM (e.g., ChatGPT) to analyze the relationship between 6D pose and task-specific requirements, offers enhanced pose-aware grasp prediction and motion planning capabilities. Our benchmark demonstrates notable advancements in pose estimation, pose-aware manipulation, and real-robot skill transfer, setting new standards for POM research. We will open-source the ManiPose benchmark with the final version paper, inviting the community to engage with our resources, available at our website:https://sites.google.com/view/manipose.



### Few-shot Oriented Object Detection with Memorable Contrastive Learning in Remote Sensing Images
- **Arxiv ID**: http://arxiv.org/abs/2403.13375v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2403.13375v1)
- **Published**: 2024-03-20 08:15:18+00:00
- **Updated**: 2024-03-20 08:15:18+00:00
- **Authors**: Jiawei Zhou, Wuzhou Li, Yi Cao, Hongtao Cai, Xiang Li
- **Comment**: 13 pages, 8 tables, 10 figures
- **Journal**: None
- **Summary**: Few-shot object detection (FSOD) has garnered significant research attention in the field of remote sensing due to its ability to reduce the dependency on large amounts of annotated data. However, two challenges persist in this area: (1) axis-aligned proposals, which can result in misalignment for arbitrarily oriented objects, and (2) the scarcity of annotated data still limits the performance for unseen object categories. To address these issues, we propose a novel FSOD method for remote sensing images called Few-shot Oriented object detection with Memorable Contrastive learning (FOMC). Specifically, we employ oriented bounding boxes instead of traditional horizontal bounding boxes to learn a better feature representation for arbitrary-oriented aerial objects, leading to enhanced detection performance. To the best of our knowledge, we are the first to address oriented object detection in the few-shot setting for remote sensing images. To address the challenging issue of object misclassification, we introduce a supervised contrastive learning module with a dynamically updated memory bank. This module enables the use of large batches of negative samples and enhances the model's capability to learn discriminative features for unseen classes. We conduct comprehensive experiments on the DOTA and HRSC2016 datasets, and our model achieves state-of-the-art performance on the few-shot oriented object detection task. Code and pretrained models will be released.



### Fast-Poly: A Fast Polyhedral Framework For 3D Multi-Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2403.13443v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2403.13443v2)
- **Published**: 2024-03-20 09:39:39+00:00
- **Updated**: 2024-07-30 03:44:19+00:00
- **Authors**: Xiaoyu Li, Dedong Liu, Yitao Wu, Xian Wu, Lijun Zhao, Jinghan Gao
- **Comment**: 1st on the NuScenes Tracking benchmark with 75.8 AMOTA and 34.2 FPS
- **Journal**: None
- **Summary**: 3D Multi-Object Tracking (MOT) captures stable and comprehensive motion states of surrounding obstacles, essential for robotic perception. However, current 3D trackers face issues with accuracy and latency consistency. In this paper, we propose Fast-Poly, a fast and effective filter-based method for 3D MOT. Building upon our previous work Poly-MOT, Fast-Poly addresses object rotational anisotropy in 3D space, enhances local computation densification, and leverages parallelization technique, improving inference speed and precision. Fast-Poly is extensively tested on two large-scale tracking benchmarks with Python implementation. On the nuScenes dataset, Fast-Poly achieves new state-of-the-art performance with 75.8% AMOTA among all methods and can run at 34.2 FPS on a personal CPU. On the Waymo dataset, Fast-Poly exhibits competitive accuracy with 63.6% MOTA and impressive inference speed (35.5 FPS). The source code is publicly available at https://github.com/lixiaoyu2000/FastPoly.



### Mask-based Invisible Backdoor Attacks on Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2405.09550v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CR, I.4.8
- **Links**: [PDF](http://arxiv.org/pdf/2405.09550v3)
- **Published**: 2024-03-20 12:27:30+00:00
- **Updated**: 2024-06-04 11:28:42+00:00
- **Authors**: Jeongjin Shin
- **Comment**: 7 pages, 3 figures
- **Journal**: None
- **Summary**: Deep learning models have achieved unprecedented performance in the domain of object detection, resulting in breakthroughs in areas such as autonomous driving and security. However, deep learning models are vulnerable to backdoor attacks. These attacks prompt models to behave similarly to standard models without a trigger; however, they act maliciously upon detecting a predefined trigger. Despite extensive research on backdoor attacks in image classification, their application to object detection remains relatively underexplored. Given the widespread application of object detection in critical real-world scenarios, the sensitivity and potential impact of these vulnerabilities cannot be overstated. In this study, we propose an effective invisible backdoor attack on object detection utilizing a mask-based approach. Three distinct attack scenarios were explored for object detection: object disappearance, object misclassification, and object generation attack. Through extensive experiments, we comprehensively examined the effectiveness of these attacks and tested certain defense methods to determine effective countermeasures. Code will be available at https://github.com/jeongjin0/invisible-backdoor-object-detection



### Find n' Propagate: Open-Vocabulary 3D Object Detection in Urban Environments
- **Arxiv ID**: http://arxiv.org/abs/2403.13556v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2403.13556v2)
- **Published**: 2024-03-20 12:51:30+00:00
- **Updated**: 2024-07-12 10:42:30+00:00
- **Authors**: Djamahl Etchegaray, Zi Huang, Tatsuya Harada, Yadan Luo
- **Comment**: To appear in ECCV 2024. Source code:
  https://github.com/djamahl99/findnpropagate
- **Journal**: None
- **Summary**: In this work, we tackle the limitations of current LiDAR-based 3D object detection systems, which are hindered by a restricted class vocabulary and the high costs associated with annotating new object classes. Our exploration of open-vocabulary (OV) learning in urban environments aims to capture novel instances using pre-trained vision-language models (VLMs) with multi-sensor data. We design and benchmark a set of four potential solutions as baselines, categorizing them into either top-down or bottom-up approaches based on their input data strategies. While effective, these methods exhibit certain limitations, such as missing novel objects in 3D box estimation or applying rigorous priors, leading to biases towards objects near the camera or of rectangular geometries. To overcome these limitations, we introduce a universal \textsc{Find n' Propagate} approach for 3D OV tasks, aimed at maximizing the recall of novel objects and propagating this detection capability to more distant areas thereby progressively capturing more. In particular, we utilize a greedy box seeker to search against 3D novel boxes of varying orientations and depth in each generated frustum and ensure the reliability of newly identified boxes by cross alignment and density ranker. Additionally, the inherent bias towards camera-proximal objects is alleviated by the proposed remote simulator, which randomly diversifies pseudo-labeled novel instances in the self-training process, combined with the fusion of base samples in the memory bank. Extensive experiments demonstrate a 53% improvement in novel recall across diverse OV settings, VLMs, and 3D detectors. Notably, we achieve up to a 3.97-fold increase in Average Precision (AP) for novel object classes. The source code is made available at https://github.com/djamahl99/findnpropagate.



### DVMNet: Computing Relative Pose for Unseen Objects Beyond Hypotheses
- **Arxiv ID**: http://arxiv.org/abs/2403.13683v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2403.13683v1)
- **Published**: 2024-03-20 15:41:32+00:00
- **Updated**: 2024-03-20 15:41:32+00:00
- **Authors**: Chen Zhao, Tong Zhang, Zheng Dang, Mathieu Salzmann
- **Comment**: Accepted by CVPR 2024
- **Journal**: None
- **Summary**: Determining the relative pose of an object between two images is pivotal to the success of generalizable object pose estimation. Existing approaches typically approximate the continuous pose representation with a large number of discrete pose hypotheses, which incurs a computationally expensive process of scoring each hypothesis at test time. By contrast, we present a Deep Voxel Matching Network (DVMNet) that eliminates the need for pose hypotheses and computes the relative object pose in a single pass. To this end, we map the two input RGB images, reference and query, to their respective voxelized 3D representations. We then pass the resulting voxels through a pose estimation module, where the voxels are aligned and the pose is computed in an end-to-end fashion by solving a least-squares problem. To enhance robustness, we introduce a weighted closest voxel algorithm capable of mitigating the impact of noisy voxels. We conduct extensive experiments on the CO3D, LINEMOD, and Objaverse datasets, demonstrating that our method delivers more accurate relative pose estimates for novel objects at a lower computational cost compared to state-of-the-art methods. Our code is released at: https://github.com/sailor-z/DVMNet/.



### EC-IoU: Orienting Safety for Object Detectors via Ego-Centric Intersection-over-Union
- **Arxiv ID**: http://arxiv.org/abs/2403.15474v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2403.15474v1)
- **Published**: 2024-03-20 16:25:49+00:00
- **Updated**: 2024-03-20 16:25:49+00:00
- **Authors**: Brian Hsuan-Cheng Liao, Chih-Hong Cheng, Hasan Esen, Alois Knoll
- **Comment**: 8 pages (IEEE double column format), 7 figures, 2 tables, submitted
  to IROS 2024
- **Journal**: None
- **Summary**: This paper presents safety-oriented object detection via a novel Ego-Centric Intersection-over-Union (EC-IoU) measure, addressing practical concerns when applying state-of-the-art learning-based perception models in safety-critical domains such as autonomous driving. Concretely, we propose a weighting mechanism to refine the widely used IoU measure, allowing it to assign a higher score to a prediction that covers closer points of a ground-truth object from the ego agent's perspective. The proposed EC-IoU measure can be used in typical evaluation processes to select object detectors with higher safety-related performance for downstream tasks. It can also be integrated into common loss functions for model fine-tuning. While geared towards safety, our experiment with the KITTI dataset demonstrates the performance of a model trained on EC-IoU can be better than that of a variant trained on IoU in terms of mean Average Precision as well.



