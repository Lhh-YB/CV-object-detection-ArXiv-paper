# Arxiv Papers in cs.CV on 2024-02-09
### Improving 2D-3D Dense Correspondences with Diffusion Models for 6D Object Pose Estimation
- **Arxiv ID**: http://arxiv.org/abs/2402.06436v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2402.06436v1)
- **Published**: 2024-02-09 14:27:40+00:00
- **Updated**: 2024-02-09 14:27:40+00:00
- **Authors**: Peter HÃ¶nig, Stefan Thalhammer, Markus Vincze
- **Comment**: Submitted to the First Austrian Symposium on AI, Robotics, and Vision
  2024
- **Journal**: None
- **Summary**: Estimating 2D-3D correspondences between RGB images and 3D space is a fundamental problem in 6D object pose estimation. Recent pose estimators use dense correspondence maps and Point-to-Point algorithms to estimate object poses. The accuracy of pose estimation depends heavily on the quality of the dense correspondence maps and their ability to withstand occlusion, clutter, and challenging material properties. Currently, dense correspondence maps are estimated using image-to-image translation models based on GANs, Autoencoders, or direct regression models. However, recent advancements in image-to-image translation have led to diffusion models being the superior choice when evaluated on benchmarking datasets. In this study, we compare image-to-image translation networks based on GANs and diffusion models for the downstream task of 6D object pose estimation. Our results demonstrate that the diffusion-based image-to-image translation model outperforms the GAN, revealing potential for further improvements in 6D object pose estimation models.



### Transfer learning with generative models for object detection on limited datasets
- **Arxiv ID**: http://arxiv.org/abs/2402.06784v2
- **DOI**: 10.1088/2632-2153/ad65b5
- **Categories**: **cs.CV**, cond-mat.dis-nn, cs.AI, cs.LG, cs.NA, math.NA, 68T05, 68T07, 68T10, 68T45,, I.2.6; I.2.0; I.4.8; I.4.9; I.5.1; I.5.0; I.5.4; J.3
- **Links**: [PDF](http://arxiv.org/pdf/2402.06784v2)
- **Published**: 2024-02-09 21:17:31+00:00
- **Updated**: 2024-06-13 10:09:51+00:00
- **Authors**: Matteo Paiano, Stefano Martina, Carlotta Giannelli, Filippo Caruso
- **Comment**: 28 pages, 16 figures, 1 table
- **Journal**: 2024 Mach. Learn.: Sci. Technol. 5 035041
- **Summary**: The availability of data is limited in some fields, especially for object detection tasks, where it is necessary to have correctly labeled bounding boxes around each object. A notable example of such data scarcity is found in the domain of marine biology, where it is useful to develop methods to automatically detect submarine species for environmental monitoring. To address this data limitation, the state-of-the-art machine learning strategies employ two main approaches. The first involves pretraining models on existing datasets before generalizing to the specific domain of interest. The second strategy is to create synthetic datasets specifically tailored to the target domain using methods like copy-paste techniques or ad-hoc simulators. The first strategy often faces a significant domain shift, while the second demands custom solutions crafted for the specific task. In response to these challenges, here we propose a transfer learning framework that is valid for a generic scenario. In this framework, generated images help to improve the performances of an object detector in a few-real data regime. This is achieved through a diffusion-based generative model that was pretrained on large generic datasets. With respect to the state-of-the-art, we find that it is not necessary to fine tune the generative model on the specific domain of interest. We believe that this is an important advance because it mitigates the labor-intensive task of manual labeling the images in object detection tasks. We validate our approach focusing on fishes in an underwater environment, and on the more common domain of cars in an urban setting. Our method achieves detection performance comparable to models trained on thousands of images, using only a few hundreds of input data. Our results pave the way for new generative AI-based protocols for machine learning applications in various domains.



### Event-to-Video Conversion for Overhead Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2402.06805v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2402.06805v1)
- **Published**: 2024-02-09 22:07:39+00:00
- **Updated**: 2024-02-09 22:07:39+00:00
- **Authors**: Darryl Hannan, Ragib Arnab, Gavin Parpart, Garrett T. Kenyon, Edward Kim, Yijing Watkins
- **Comment**: 5 pages, 1 figure, SSIAI 2024
- **Journal**: None
- **Summary**: Collecting overhead imagery using an event camera is desirable due to the energy efficiency of the image sensor compared to standard cameras. However, event cameras complicate downstream image processing, especially for complex tasks such as object detection. In this paper, we investigate the viability of event streams for overhead object detection. We demonstrate that across a number of standard modeling approaches, there is a significant gap in performance between dense event representations and corresponding RGB frames. We establish that this gap is, in part, due to a lack of overlap between the event representations and the pre-training data used to initialize the weights of the object detectors. Then, we apply event-to-video conversion models that convert event streams into gray-scale video to close this gap. We demonstrate that this approach results in a large performance increase, outperforming even event-specific object detection techniques on our overhead target task. These results suggest that better alignment between event representations and existing large pre-trained models may result in greater short-term performance gains compared to end-to-end event-specific architectural improvements.



