# Arxiv Papers in cs.CV on 2024-02-14
### Moving Object Proposals with Deep Learned Optical Flow for Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2402.08882v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG, 68Txx
- **Links**: [PDF](http://arxiv.org/pdf/2402.08882v1)
- **Published**: 2024-02-14 01:13:55+00:00
- **Updated**: 2024-02-14 01:13:55+00:00
- **Authors**: Ge Shi, Zhili Yang
- **Comment**: 7 pages, 8 figures, 1 table
- **Journal**: None
- **Summary**: Dynamic scene understanding is one of the most conspicuous field of interest among computer vision community. In order to enhance dynamic scene understanding, pixel-wise segmentation with neural networks is widely accepted. The latest researches on pixel-wise segmentation combined semantic and motion information and produced good performance. In this work, we propose a state of art architecture of neural networks to accurately and efficiently get the moving object proposals (MOP). We first train an unsupervised convolutional neural network (UnFlow) to generate optical flow estimation. Then we render the output of optical flow net to a fully convolutional SegNet model. The main contribution of our work is (1) Fine-tuning the pretrained optical flow model on the brand new DAVIS Dataset; (2) Leveraging fully convolutional neural networks with Encoder-Decoder architecture to segment objects. We developed the codes with TensorFlow, and executed the training and evaluation processes on an AWS EC2 instance.



### Efficient One-stage Video Object Detection by Exploiting Temporal Consistency
- **Arxiv ID**: http://arxiv.org/abs/2402.09241v1
- **DOI**: 10.1007/978-3-031-19833-5_1
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2402.09241v1)
- **Published**: 2024-02-14 15:32:07+00:00
- **Updated**: 2024-02-14 15:32:07+00:00
- **Authors**: Guanxiong Sun, Yang Hua, Guosheng Hu, Neil Robertson
- **Comment**: None
- **Journal**: None
- **Summary**: Recently, one-stage detectors have achieved competitive accuracy and faster speed compared with traditional two-stage detectors on image data. However, in the field of video object detection (VOD), most existing VOD methods are still based on two-stage detectors. Moreover, directly adapting existing VOD methods to one-stage detectors introduces unaffordable computational costs. In this paper, we first analyse the computational bottlenecks of using one-stage detectors for VOD. Based on the analysis, we present a simple yet efficient framework to address the computational bottlenecks and achieve efficient one-stage VOD by exploiting the temporal consistency in video frames. Specifically, our method consists of a location-prior network to filter out background regions and a size-prior network to skip unnecessary computations on low-level feature maps for specific frames. We test our method on various modern one-stage detectors and conduct extensive experiments on the ImageNet VID dataset. Excellent experimental results demonstrate the superior effectiveness, efficiency, and compatibility of our method. The code is available at https://github.com/guanxiongsun/vfe.pytorch.



### Few-Shot Object Detection with Sparse Context Transformers
- **Arxiv ID**: http://arxiv.org/abs/2402.09315v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2402.09315v1)
- **Published**: 2024-02-14 17:10:01+00:00
- **Updated**: 2024-02-14 17:10:01+00:00
- **Authors**: Jie Mei, Mingyuan Jiu, Hichem Sahbi, Xiaoheng Jiang, Mingliang Xu
- **Comment**: None
- **Journal**: None
- **Summary**: Few-shot detection is a major task in pattern recognition which seeks to localize objects using models trained with few labeled data. One of the mainstream few-shot methods is transfer learning which consists in pretraining a detection model in a source domain prior to its fine-tuning in a target domain. However, it is challenging for fine-tuned models to effectively identify new classes in the target domain, particularly when the underlying labeled training data are scarce. In this paper, we devise a novel sparse context transformer (SCT) that effectively leverages object knowledge in the source domain, and automatically learns a sparse context from only few training images in the target domain. As a result, it combines different relevant clues in order to enhance the discrimination power of the learned detectors and reduce class confusion. We evaluate the proposed method on two challenging few-shot object detection benchmarks, and empirical results show that the proposed method obtains competitive performance compared to the related state-of-the-art.



