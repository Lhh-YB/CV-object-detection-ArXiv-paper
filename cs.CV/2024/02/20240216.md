# Arxiv Papers in cs.CV on 2024-02-16
### STF: Spatio-Temporal Fusion Module for Improving Video Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2402.10752v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2402.10752v1)
- **Published**: 2024-02-16 15:19:39+00:00
- **Updated**: 2024-02-16 15:19:39+00:00
- **Authors**: Noreen Anwar, Guillaume-Alexandre Bilodeau, Wassim Bouachir
- **Comment**: 8 pages,3 figures
- **Journal**: None
- **Summary**: Consecutive frames in a video contain redundancy, but they may also contain relevant complementary information for the detection task. The objective of our work is to leverage this complementary information to improve detection. Therefore, we propose a spatio-temporal fusion framework (STF). We first introduce multi-frame and single-frame attention modules that allow a neural network to share feature maps between nearby frames to obtain more robust object representations. Second, we introduce a dual-frame fusion module that merges feature maps in a learnable manner to improve them. Our evaluation is conducted on three different benchmarks including video sequences of moving road users. The performed experiments demonstrate that the proposed spatio-temporal fusion module leads to improved detection performance compared to baseline object detectors. Code is available at https://github.com/noreenanwar/STF-module



### Multi-Model 3D Registration: Finding Multiple Moving Objects in Cluttered Point Clouds
- **Arxiv ID**: http://arxiv.org/abs/2402.10865v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2402.10865v1)
- **Published**: 2024-02-16 18:01:43+00:00
- **Updated**: 2024-02-16 18:01:43+00:00
- **Authors**: David Jin, Sushrut Karmalkar, Harry Zhang, Luca Carlone
- **Comment**: 8 pages, Accepted by ICRA 2024
- **Journal**: None
- **Summary**: We investigate a variation of the 3D registration problem, named multi-model 3D registration. In the multi-model registration problem, we are given two point clouds picturing a set of objects at different poses (and possibly including points belonging to the background) and we want to simultaneously reconstruct how all objects moved between the two point clouds. This setup generalizes standard 3D registration where one wants to reconstruct a single pose, e.g., the motion of the sensor picturing a static scene. Moreover, it provides a mathematically grounded formulation for relevant robotics applications, e.g., where a depth sensor onboard a robot perceives a dynamic scene and has the goal of estimating its own motion (from the static portion of the scene) while simultaneously recovering the motion of all dynamic objects. We assume a correspondence-based setup where we have putative matches between the two point clouds and consider the practical case where these correspondences are plagued with outliers. We then propose a simple approach based on Expectation-Maximization (EM) and establish theoretical conditions under which the EM approach converges to the ground truth. We evaluate the approach in simulated and real datasets ranging from table-top scenes to self-driving scenarios and demonstrate its effectiveness when combined with state-of-the-art scene flow methods to establish dense correspondences.



