# Arxiv Papers in cs.CV on 2024-02-08
### NCRF: Neural Contact Radiance Fields for Free-Viewpoint Rendering of Hand-Object Interaction
- **Arxiv ID**: http://arxiv.org/abs/2402.05532v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2402.05532v2)
- **Published**: 2024-02-08 10:09:12+00:00
- **Updated**: 2024-02-09 13:00:22+00:00
- **Authors**: Zhongqun Zhang, Jifei Song, Eduardo Pérez-Pellitero, Yiren Zhou, Hyung Jin Chang, Aleš Leonardis
- **Comment**: Accepted by 3DV 2024
- **Journal**: None
- **Summary**: Modeling hand-object interactions is a fundamentally challenging task in 3D computer vision. Despite remarkable progress that has been achieved in this field, existing methods still fail to synthesize the hand-object interaction photo-realistically, suffering from degraded rendering quality caused by the heavy mutual occlusions between the hand and the object, and inaccurate hand-object pose estimation. To tackle these challenges, we present a novel free-viewpoint rendering framework, Neural Contact Radiance Field (NCRF), to reconstruct hand-object interactions from a sparse set of videos. In particular, the proposed NCRF framework consists of two key components: (a) A contact optimization field that predicts an accurate contact field from 3D query points for achieving desirable contact between the hand and the object. (b) A hand-object neural radiance field to learn an implicit hand-object representation in a static canonical space, in concert with the specifically designed hand-object motion field to produce observation-to-canonical correspondences. We jointly learn these key components where they mutually help and regularize each other with visual and geometric constraints, producing a high-quality hand-object reconstruction that achieves photo-realistic novel view synthesis. Extensive experiments on HO3D and DexYCB datasets show that our approach outperforms the current state-of-the-art in terms of both rendering quality and pose estimation accuracy.



### Extending 6D Object Pose Estimators for Stereo Vision
- **Arxiv ID**: http://arxiv.org/abs/2402.05610v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2402.05610v2)
- **Published**: 2024-02-08 12:08:52+00:00
- **Updated**: 2024-09-10 13:51:00+00:00
- **Authors**: Thomas Pöllabauer, Jan Emrich, Volker Knauthe, Arjan Kuijper
- **Comment**: 4th International Conference on Pattern Recognition and Artificial
  Intelligence (ICPRAI)
- **Journal**: None
- **Summary**: Estimating the 6D pose of objects accurately, quickly, and robustly remains a difficult task. However, recent methods for directly regressing poses from RGB images using dense features have achieved state-of-the-art results. Stereo vision, which provides an additional perspective on the object, can help reduce pose ambiguity and occlusion. Moreover, stereo can directly infer the distance of an object, while mono-vision requires internalized knowledge of the object's size. To extend the state-of-the-art in 6D object pose estimation to stereo, we created a BOP compatible stereo version of the YCB-V dataset. Our method outperforms state-of-the-art 6D pose estimation algorithms by utilizing stereo vision and can easily be adopted for other dense feature-based algorithms.



### FuncGrasp: Learning Object-Centric Neural Grasp Functions from Single Annotated Example Object
- **Arxiv ID**: http://arxiv.org/abs/2402.05644v2
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2402.05644v2)
- **Published**: 2024-02-08 12:59:47+00:00
- **Updated**: 2024-02-22 11:22:29+00:00
- **Authors**: Hanzhi Chen, Binbin Xu, Stefan Leutenegger
- **Comment**: Accepted to ICRA 2024
- **Journal**: None
- **Summary**: We present FuncGrasp, a framework that can infer dense yet reliable grasp configurations for unseen objects using one annotated object and single-view RGB-D observation via categorical priors. Unlike previous works that only transfer a set of grasp poses, FuncGrasp aims to transfer infinite configurations parameterized by an object-centric continuous grasp function across varying instances. To ease the transfer process, we propose Neural Surface Grasping Fields (NSGF), an effective neural representation defined on the surface to densely encode grasp configurations. Further, we exploit function-to-function transfer using sphere primitives to establish semantically meaningful categorical correspondences, which are learned in an unsupervised fashion without any expert knowledge. We showcase the effectiveness through extensive experiments in both simulators and the real world. Remarkably, our framework significantly outperforms several strong baseline methods in terms of density and reliability for generated grasps.



### Point-VOS: Pointing Up Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2402.05917v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2402.05917v2)
- **Published**: 2024-02-08 18:52:23+00:00
- **Updated**: 2024-06-10 17:58:56+00:00
- **Authors**: Idil Esen Zulfikar, Sabarinath Mahadevan, Paul Voigtlaender, Bastian Leibe
- **Comment**: Accepted to CVPR2024!
- **Journal**: None
- **Summary**: Current state-of-the-art Video Object Segmentation (VOS) methods rely on dense per-object mask annotations both during training and testing. This requires time-consuming and costly video annotation mechanisms. We propose a novel Point-VOS task with a spatio-temporally sparse point-wise annotation scheme that substantially reduces the annotation effort. We apply our annotation scheme to two large-scale video datasets with text descriptions and annotate over 19M points across 133K objects in 32K videos. Based on our annotations, we propose a new Point-VOS benchmark, and a corresponding point-based training mechanism, which we use to establish strong baseline results. We show that existing VOS methods can easily be adapted to leverage our point annotations during training, and can achieve results close to the fully-supervised performance when trained on pseudo-masks generated from these points. In addition, we show that our data can be used to improve models that connect vision and language, by evaluating it on the Video Narrative Grounding (VNG) task. We will make our code and annotations available at https://pointvos.github.io.



### InstaGen: Enhancing Object Detection by Training on Synthetic Dataset
- **Arxiv ID**: http://arxiv.org/abs/2402.05937v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2402.05937v3)
- **Published**: 2024-02-08 18:59:53+00:00
- **Updated**: 2024-04-08 11:46:07+00:00
- **Authors**: Chengjian Feng, Yujie Zhong, Zequn Jie, Weidi Xie, Lin Ma
- **Comment**: CVPR2024
- **Journal**: None
- **Summary**: In this paper, we present a novel paradigm to enhance the ability of object detector, e.g., expanding categories or improving detection performance, by training on synthetic dataset generated from diffusion models. Specifically, we integrate an instance-level grounding head into a pre-trained, generative diffusion model, to augment it with the ability of localising instances in the generated images. The grounding head is trained to align the text embedding of category names with the regional visual feature of the diffusion model, using supervision from an off-the-shelf object detector, and a novel self-training scheme on (novel) categories not covered by the detector. We conduct thorough experiments to show that, this enhanced version of diffusion model, termed as InstaGen, can serve as a data synthesizer, to enhance object detectors by training on its generated samples, demonstrating superior performance over existing state-of-the-art methods in open-vocabulary (+4.5 AP) and data-sparse (+1.2 to 5.2 AP) scenarios. Project page with code: https://fcjian.github.io/InstaGen.



### CLIP-Loc: Multi-modal Landmark Association for Global Localization in Object-based Maps
- **Arxiv ID**: http://arxiv.org/abs/2402.06092v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2402.06092v1)
- **Published**: 2024-02-08 22:59:12+00:00
- **Updated**: 2024-02-08 22:59:12+00:00
- **Authors**: Shigemichi Matsuzaki, Takuma Sugino, Kazuhito Tanaka, Zijun Sha, Shintaro Nakaoka, Shintaro Yoshizawa, Kazuhiro Shintani
- **Comment**: 7 pages, 7 figures. Accepted to IEEE International Conference on
  Robotics and Automation (ICRA) 2024
- **Journal**: None
- **Summary**: This paper describes a multi-modal data association method for global localization using object-based maps and camera images. In global localization, or relocalization, using object-based maps, existing methods typically resort to matching all possible combinations of detected objects and landmarks with the same object category, followed by inlier extraction using RANSAC or brute-force search. This approach becomes infeasible as the number of landmarks increases due to the exponential growth of correspondence candidates. In this paper, we propose labeling landmarks with natural language descriptions and extracting correspondences based on conceptual similarity with image observations using a Vision Language Model (VLM). By leveraging detailed text information, our approach efficiently extracts correspondences compared to methods using only object categories. Through experiments, we demonstrate that the proposed method enables more accurate global localization with fewer iterations compared to baseline methods, exhibiting its efficiency.



