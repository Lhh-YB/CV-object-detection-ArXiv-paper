# Arxiv Papers in cs.CV on 2024-02-23
### EMIFF: Enhanced Multi-scale Image Feature Fusion for Vehicle-Infrastructure Cooperative 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2402.15272v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2402.15272v1)
- **Published**: 2024-02-23 11:35:48+00:00
- **Updated**: 2024-02-23 11:35:48+00:00
- **Authors**: Zhe Wang, Siqi Fan, Xiaoliang Huo, Tongda Xu, Yan Wang, Jingjing Liu, Yilun Chen, Ya-Qin Zhang
- **Comment**: 7 pages, 8 figures. Accepted by ICRA 2024. arXiv admin note: text
  overlap with arXiv:arXiv:2303.10975
- **Journal**: None
- **Summary**: In autonomous driving, cooperative perception makes use of multi-view cameras from both vehicles and infrastructure, providing a global vantage point with rich semantic context of road conditions beyond a single vehicle viewpoint. Currently, two major challenges persist in vehicle-infrastructure cooperative 3D (VIC3D) object detection: $1)$ inherent pose errors when fusing multi-view images, caused by time asynchrony across cameras; $2)$ information loss in transmission process resulted from limited communication bandwidth. To address these issues, we propose a novel camera-based 3D detection framework for VIC3D task, Enhanced Multi-scale Image Feature Fusion (EMIFF). To fully exploit holistic perspectives from both vehicles and infrastructure, we propose Multi-scale Cross Attention (MCA) and Camera-aware Channel Masking (CCM) modules to enhance infrastructure and vehicle features at scale, spatial, and channel levels to correct the pose error introduced by camera asynchrony. We also introduce a Feature Compression (FC) module with channel and spatial compression blocks for transmission efficiency. Experiments show that EMIFF achieves SOTA on DAIR-V2X-C datasets, significantly outperforming previous early-fusion and late-fusion methods with comparable transmission costs.



### Outlier detection by ensembling uncertainty with negative objectness
- **Arxiv ID**: http://arxiv.org/abs/2402.15374v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2402.15374v4)
- **Published**: 2024-02-23 15:19:37+00:00
- **Updated**: 2024-09-08 16:14:29+00:00
- **Authors**: Anja Delić, Matej Grcić, Siniša Šegvić
- **Comment**: Accepted to BMVC 2024
- **Journal**: None
- **Summary**: Outlier detection is an essential capability in safety-critical applications of supervised visual recognition. Most of the existing methods deliver best results by encouraging standard closed-set models to produce low-confidence predictions in negative training data. However, that approach conflates prediction uncertainty with recognition of the negative class. We therefore reconsider direct prediction of K+1 logits that correspond to K groundtruth classes and one outlier class. This setup allows us to formulate a novel anomaly score as an ensemble of in-distribution uncertainty and the posterior of the outlier class which we term negative objectness. Now outliers can be independently detected due to i) high prediction uncertainty or ii) similarity with negative data. We embed our method into a dense prediction architecture with mask-level recognition over K+2 classes. The training procedure encourages the novel K+2-th class to learn negative objectness at pasted negative instances. Our models outperform the current state-of-the art on standard benchmarks for image-wide and pixel-level outlier detection with and without training on real negative data.



### Multi-Objective Learning for Deformable Image Registration
- **Arxiv ID**: http://arxiv.org/abs/2402.16658v1
- **DOI**: None
- **Categories**: **eess.IV**, cs.CV, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2402.16658v1)
- **Published**: 2024-02-23 15:42:13+00:00
- **Updated**: 2024-02-23 15:42:13+00:00
- **Authors**: Monika Grewal, Henrike Westerveld, Peter A. N. Bosman, Tanja Alderliesten
- **Comment**: None
- **Journal**: None
- **Summary**: Deformable image registration (DIR) involves optimization of multiple conflicting objectives, however, not many existing DIR algorithms are multi-objective (MO). Further, while there has been progress in the design of deep learning algorithms for DIR, there is no work in the direction of MO DIR using deep learning. In this paper, we fill this gap by combining a recently proposed approach for MO training of neural networks with a well-known deep neural network for DIR and create a deep learning based MO DIR approach. We evaluate the proposed approach for DIR of pelvic magnetic resonance imaging (MRI) scans. We experimentally demonstrate that the proposed MO DIR approach -- providing multiple registration outputs for each patient that each correspond to a different trade-off between the objectives -- has additional desirable properties from a clinical use point-of-view as compared to providing a single DIR output. The experiments also show that the proposed MO DIR approach provides a better spread of DIR outputs across the entire trade-off front than simply training multiple neural networks with weights for each objective sampled from a grid of possible values.



### Improving Explainable Object-induced Model through Uncertainty for Automated Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2402.15572v1
- **DOI**: 10.1145/3610977.3634973
- **Categories**: **cs.AI**, cs.CV, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2402.15572v1)
- **Published**: 2024-02-23 19:14:57+00:00
- **Updated**: 2024-02-23 19:14:57+00:00
- **Authors**: Shihong Ling, Yue Wan, Xiaowei Jia, Na Du
- **Comment**: In Proceedings of the 2024 ACM / IEEE International Conference on
  Human-Robot Interaction (HRI '24), March 11--14, 2024, Boulder, CO, USA. ACM,
  New York, NY, USA, 9 pages
- **Journal**: None
- **Summary**: The rapid evolution of automated vehicles (AVs) has the potential to provide safer, more efficient, and comfortable travel options. However, these systems face challenges regarding reliability in complex driving scenarios. Recent explainable AV architectures neglect crucial information related to inherent uncertainties while providing explanations for actions. To overcome such challenges, our study builds upon the "object-induced" model approach that prioritizes the role of objects in scenes for decision-making and integrates uncertainty assessment into the decision-making process using an evidential deep learning paradigm with a Beta prior. Additionally, we explore several advanced training strategies guided by uncertainty, including uncertainty-guided data reweighting and augmentation. Leveraging the BDD-OIA dataset, our findings underscore that the model, through these enhancements, not only offers a clearer comprehension of AV decisions and their underlying reasoning but also surpasses existing baselines across a broad range of scenarios.



