# Arxiv Papers in cs.CV on 2024-02-24
### CLIPose: Category-Level Object Pose Estimation with Pre-trained Vision-Language Knowledge
- **Arxiv ID**: http://arxiv.org/abs/2402.15726v1
- **DOI**: 10.1109/TCSVT.2024.3397997
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2402.15726v1)
- **Published**: 2024-02-24 05:31:53+00:00
- **Updated**: 2024-02-24 05:31:53+00:00
- **Authors**: Xiao Lin, Minghao Zhu, Ronghao Dang, Guangliang Zhou, Shaolong Shu, Feng Lin, Chengju Liu, Qijun Chen
- **Comment**: 14 pages, 4 figures, 9 tables
- **Journal**: None
- **Summary**: Most of existing category-level object pose estimation methods devote to learning the object category information from point cloud modality. However, the scale of 3D datasets is limited due to the high cost of 3D data collection and annotation. Consequently, the category features extracted from these limited point cloud samples may not be comprehensive. This motivates us to investigate whether we can draw on knowledge of other modalities to obtain category information. Inspired by this motivation, we propose CLIPose, a novel 6D pose framework that employs the pre-trained vision-language model to develop better learning of object category information, which can fully leverage abundant semantic knowledge in image and text modalities. To make the 3D encoder learn category-specific features more efficiently, we align representations of three modalities in feature space via multi-modal contrastive learning. In addition to exploiting the pre-trained knowledge of the CLIP's model, we also expect it to be more sensitive with pose parameters. Therefore, we introduce a prompt tuning approach to fine-tune image encoder while we incorporate rotations and translations information in the text descriptions. CLIPose achieves state-of-the-art performance on two mainstream benchmark datasets, REAL275 and CAMERA25, and runs in real-time during inference (40FPS).



### Multi-Object Tracking by Hierarchical Visual Representations
- **Arxiv ID**: http://arxiv.org/abs/2402.15895v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2402.15895v1)
- **Published**: 2024-02-24 20:10:44+00:00
- **Updated**: 2024-02-24 20:10:44+00:00
- **Authors**: Jinkun Cao, Jiangmiao Pang, Kris Kitani
- **Comment**: 6 pages, 3 figures, 10 tables, accepted by ICRA 2024
- **Journal**: None
- **Summary**: We propose a new visual hierarchical representation paradigm for multi-object tracking. It is more effective to discriminate between objects by attending to objects' compositional visual regions and contrasting with the background contextual information instead of sticking to only the semantic visual cue such as bounding boxes. This compositional-semantic-contextual hierarchy is flexible to be integrated in different appearance-based multi-object tracking methods. We also propose an attention-based visual feature module to fuse the hierarchical visual representations. The proposed method achieves state-of-the-art accuracy and time efficiency among query-based methods on multiple multi-object tracking benchmarks.



