# Arxiv Papers in cs.CV on 2024-02-15
### Beyond Kalman Filters: Deep Learning-Based Filters for Improved Object Tracking
- **Arxiv ID**: http://arxiv.org/abs/2402.09865v1
- **DOI**: 10.1007/s00138-024-01644-x
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2402.09865v1)
- **Published**: 2024-02-15 10:47:44+00:00
- **Updated**: 2024-02-15 10:47:44+00:00
- **Authors**: Momir Adžemović, Predrag Tadić, Andrija Petrović, Mladen Nikolić
- **Comment**: 29 pages
- **Journal**: None
- **Summary**: Traditional tracking-by-detection systems typically employ Kalman filters (KF) for state estimation. However, the KF requires domain-specific design choices and it is ill-suited to handling non-linear motion patterns. To address these limitations, we propose two innovative data-driven filtering methods. Our first method employs a Bayesian filter with a trainable motion model to predict an object's future location and combines its predictions with observations gained from an object detector to enhance bounding box prediction accuracy. Moreover, it dispenses with most domain-specific design choices characteristic of the KF. The second method, an end-to-end trainable filter, goes a step further by learning to correct detector errors, further minimizing the need for domain expertise. Additionally, we introduce a range of motion model architectures based on Recurrent Neural Networks, Neural Ordinary Differential Equations, and Conditional Neural Processes, that are combined with the proposed filtering methods. Our extensive evaluation across multiple datasets demonstrates that our proposed filters outperform the traditional KF in object tracking, especially in the case of non-linear motion patterns -- the use case our filters are best suited to. We also conduct noise robustness analysis of our filters with convincing positive results. We further propose a new cost function for associating observations with tracks. Our tracker, which incorporates this new association cost with our proposed filters, outperforms the conventional SORT method and other motion-based trackers in multi-object tracking according to multiple metrics on motion-rich DanceTrack and SportsMOT datasets.



### Lester: rotoscope animation through video object segmentation and tracking
- **Arxiv ID**: http://arxiv.org/abs/2402.09883v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2402.09883v1)
- **Published**: 2024-02-15 11:15:54+00:00
- **Updated**: 2024-02-15 11:15:54+00:00
- **Authors**: Ruben Tous
- **Comment**: None
- **Journal**: None
- **Summary**: This article introduces Lester, a novel method to automatically synthetise retro-style 2D animations from videos. The method approaches the challenge mainly as an object segmentation and tracking problem. Video frames are processed with the Segment Anything Model (SAM) and the resulting masks are tracked through subsequent frames with DeAOT, a method of hierarchical propagation for semi-supervised video object segmentation. The geometry of the masks' contours is simplified with the Douglas-Peucker algorithm. Finally, facial traits, pixelation and a basic shadow effect can be optionally added. The results show that the method exhibits an excellent temporal consistency and can correctly process videos with different poses and appearances, dynamic shots, partial shots and diverse backgrounds. The proposed method provides a more simple and deterministic approach than diffusion models based video-to-video translation pipelines, which suffer from temporal consistency problems and do not cope well with pixelated and schematic outputs. The method is also much most practical than techniques based on 3D human pose estimation, which require custom handcrafted 3D models and are very limited with respect to the type of scenes they can process.



### GaussianObject: High-Quality 3D Object Reconstruction from Four Views with Gaussian Splatting
- **Arxiv ID**: http://arxiv.org/abs/2402.10259v4
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2402.10259v4)
- **Published**: 2024-02-15 18:42:33+00:00
- **Updated**: 2024-11-13 17:35:00+00:00
- **Authors**: Chen Yang, Sikuang Li, Jiemin Fang, Ruofan Liang, Lingxi Xie, Xiaopeng Zhang, Wei Shen, Qi Tian
- **Comment**: ACM Transactions on Graphics (SIGGRAPH Asia 2024). Project page:
  https://gaussianobject.github.io/ Code:
  https://github.com/chensjtu/GaussianObject
- **Journal**: None
- **Summary**: Reconstructing and rendering 3D objects from highly sparse views is of critical importance for promoting applications of 3D vision techniques and improving user experience. However, images from sparse views only contain very limited 3D information, leading to two significant challenges: 1) Difficulty in building multi-view consistency as images for matching are too few; 2) Partially omitted or highly compressed object information as view coverage is insufficient. To tackle these challenges, we propose GaussianObject, a framework to represent and render the 3D object with Gaussian splatting that achieves high rendering quality with only 4 input images. We first introduce techniques of visual hull and floater elimination, which explicitly inject structure priors into the initial optimization process to help build multi-view consistency, yielding a coarse 3D Gaussian representation. Then we construct a Gaussian repair model based on diffusion models to supplement the omitted object information, where Gaussians are further refined. We design a self-generating strategy to obtain image pairs for training the repair model. We further design a COLMAP-free variant, where pre-given accurate camera poses are not required, which achieves competitive quality and facilitates wider applications. GaussianObject is evaluated on several challenging datasets, including MipNeRF360, OmniObject3D, OpenIllumination, and our-collected unposed images, achieving superior performance from only four views and significantly outperforming previous SOTA methods. Our demo is available at https://gaussianobject.github.io/, and the code has been released at https://github.com/GaussianObject/GaussianObject.



