# Arxiv Papers in cs.CV on 2024-02-13
### H2O-SDF: Two-phase Learning for 3D Indoor Reconstruction using Object Surface Fields
- **Arxiv ID**: http://arxiv.org/abs/2402.08138v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2402.08138v2)
- **Published**: 2024-02-13 00:23:31+00:00
- **Updated**: 2024-03-08 08:00:47+00:00
- **Authors**: Minyoung Park, Mirae Do, YeonJae Shin, Jaeseok Yoo, Jongkwang Hong, Joongrock Kim, Chul Lee
- **Comment**: None
- **Journal**: None
- **Summary**: Advanced techniques using Neural Radiance Fields (NeRF), Signed Distance Fields (SDF), and Occupancy Fields have recently emerged as solutions for 3D indoor scene reconstruction. We introduce a novel two-phase learning approach, H2O-SDF, that discriminates between object and non-object regions within indoor environments. This method achieves a nuanced balance, carefully preserving the geometric integrity of room layouts while also capturing intricate surface details of specific objects. A cornerstone of our two-phase learning framework is the introduction of the Object Surface Field (OSF), a novel concept designed to mitigate the persistent vanishing gradient problem that has previously hindered the capture of high-frequency details in other methods. Our proposed approach is validated through several experiments that include ablation studies.



### Object Detection in Thermal Images Using Deep Learning for Unmanned Aerial Vehicles
- **Arxiv ID**: http://arxiv.org/abs/2402.08251v1
- **DOI**: 10.1109/SII58957.2024.10417611
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2402.08251v1)
- **Published**: 2024-02-13 06:40:55+00:00
- **Updated**: 2024-02-13 06:40:55+00:00
- **Authors**: Minh Dang Tu, Kieu Trang Le, Manh Duong Phung
- **Comment**: Published in: 2024 IEEE/SICE International Symposium on System
  Integration (SII)
- **Journal**: None
- **Summary**: This work presents a neural network model capable of recognizing small and tiny objects in thermal images collected by unmanned aerial vehicles. Our model consists of three parts, the backbone, the neck, and the prediction head. The backbone is developed based on the structure of YOLOv5 combined with the use of a transformer encoder at the end. The neck includes a BI-FPN block combined with the use of a sliding window and a transformer to increase the information fed into the prediction head. The prediction head carries out the detection by evaluating feature maps with the Sigmoid function. The use of transformers with attention and sliding windows increases recognition accuracy while keeping the model at a reasonable number of parameters and computation requirements for embedded systems. Experiments conducted on public dataset VEDAI and our collected datasets show that our model has a higher accuracy than state-of-the-art methods such as ResNet, Faster RCNN, ComNet, ViT, YOLOv5, SMPNet, and DPNetV3. Experiments on the embedded computer Jetson AGX show that our model achieves a real-time computation speed with a stability rate of over 90%.



### Leveraging Self-Supervised Instance Contrastive Learning for Radar Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2402.08427v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2402.08427v1)
- **Published**: 2024-02-13 12:53:33+00:00
- **Updated**: 2024-02-13 12:53:33+00:00
- **Authors**: Colin Decourt, Rufin VanRullen, Didier Salle, Thomas Oberlin
- **Comment**: 8 pages, 3 figures, 1 table
- **Journal**: None
- **Summary**: In recent years, driven by the need for safer and more autonomous transport systems, the automotive industry has shifted toward integrating a growing number of Advanced Driver Assistance Systems (ADAS). Among the array of sensors employed for object recognition tasks, radar sensors have emerged as a formidable contender due to their abilities in adverse weather conditions or low-light scenarios and their robustness in maintaining consistent performance across diverse environments. However, the small size of radar datasets and the complexity of the labelling of those data limit the performance of radar object detectors. Driven by the promising results of self-supervised learning in computer vision, this paper presents RiCL, an instance contrastive learning framework to pre-train radar object detectors. We propose to exploit the detection from the radar and the temporal information to pre-train the radar object detection model in a self-supervised way using contrastive learning. We aim to pre-train an object detector's backbone, head and neck to learn with fewer data. Experiments on the CARRADA and the RADDet datasets show the effectiveness of our approach in learning generic representations of objects in range-Doppler maps. Notably, our pre-training strategy allows us to use only 20% of the labelled data to reach a similar mAP@0.5 than a supervised approach using the whole training set.



### PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs
- **Arxiv ID**: http://arxiv.org/abs/2402.08657v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2402.08657v1)
- **Published**: 2024-02-13 18:39:18+00:00
- **Updated**: 2024-02-13 18:39:18+00:00
- **Authors**: Michael Dorkenwald, Nimrod Barazani, Cees G. M. Snoek, Yuki M. Asano
- **Comment**: None
- **Journal**: None
- **Summary**: Vision-Language Models (VLMs), such as Flamingo and GPT-4V, have shown immense potential by integrating large language models with vision systems. Nevertheless, these models face challenges in the fundamental computer vision task of object localisation, due to their training on multimodal data containing mostly captions without explicit spatial grounding. While it is possible to construct custom, supervised training pipelines with bounding box annotations that integrate with VLMs, these result in specialized and hard-to-scale models. In this paper, we aim to explore the limits of caption-based VLMs and instead propose to tackle the challenge in a simpler manner by i) keeping the weights of a caption-based VLM frozen and ii) not using any supervised detection data. To this end, we introduce an input-agnostic Positional Insert (PIN), a learnable spatial prompt, containing a minimal set of parameters that are slid inside the frozen VLM, unlocking object localisation capabilities. Our PIN module is trained with a simple next-token prediction task on synthetic data without requiring the introduction of new output heads. Our experiments demonstrate strong zero-shot localisation performances on a variety of images, including Pascal VOC, COCO, LVIS, and diverse images like paintings or cartoons.



### Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance
- **Arxiv ID**: http://arxiv.org/abs/2402.08680v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.AI, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2402.08680v1)
- **Published**: 2024-02-13 18:59:05+00:00
- **Updated**: 2024-02-13 18:59:05+00:00
- **Authors**: Linxi Zhao, Yihe Deng, Weitong Zhang, Quanquan Gu
- **Comment**: 27 pages, 20 figures, 4 tables
- **Journal**: None
- **Summary**: The advancement of Large Vision-Language Models (LVLMs) has increasingly highlighted the critical issue of their tendency to hallucinate non-existing objects in the images. To address this issue, previous works focused on using specially curated datasets or powerful LLMs (e.g., GPT-3.5) to rectify the outputs of LVLMs. However, these approaches require either expensive training/fine-tuning or API access to advanced LLMs to correct the model's output post-generation. In this paper, we tackle this challenge by introducing a framework called Mitigating hallucinAtion via classifieR-Free guIdaNcE (MARINE), which is both training-free and API-free, and can effectively and efficiently reduce object hallucinations during the generation process. Specifically, MARINE enriches the visual context of LVLMs by integrating existing open-source vision models, and employs classifier-free guidance to incorporate the additional object grounding features to improve the precision of LVLMs' generations. Through comprehensive evaluations across $6$ popular LVLMs with diverse evaluation metrics, we demonstrate the effectiveness of MARINE, which even outperforms existing fine-tuning-based methods. Remarkably, it not only reduces hallucinations but also improves the detailedness of LVLMs' generations, as assessed by GPT-4V.



