# Arxiv Papers in cs.CV on 2024-02-18
### Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models
- **Arxiv ID**: http://arxiv.org/abs/2402.11622v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CL, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2402.11622v2)
- **Published**: 2024-02-18 15:28:39+00:00
- **Updated**: 2024-06-28 07:20:22+00:00
- **Authors**: Junfei Wu, Qiang Liu, Ding Wang, Jinghao Zhang, Shu Wu, Liang Wang, Tieniu Tan
- **Comment**: Accept to ACL 2024; 19 Pages, 15 Figures, 6 Tables
- **Journal**: None
- **Summary**: Object hallucination has been an Achilles' heel which hinders the broader applications of large vision-language models (LVLMs). Object hallucination refers to the phenomenon that the LVLMs claim non-existent objects in the image. To mitigate the object hallucinations, instruction tuning and external model-based detection methods have been proposed, which either require large-scare computational resources or depend on the detection result of external models. However, there remains an under-explored field to utilize the LVLM itself to alleviate object hallucinations. In this work, we adopt the intuition that the LVLM tends to respond logically consistently for existent objects but inconsistently for hallucinated objects. Therefore, we propose a Logical Closed Loop-based framework for Object Hallucination Detection and Mitigation, namely LogicCheckGPT. In specific, we devise logical consistency probing to raise questions with logical correlations, inquiring about attributes from objects and vice versa. Whether their responses can form a logical closed loop serves as an indicator of object hallucination. As a plug-and-play method, it can be seamlessly applied to all existing LVLMs. Comprehensive experiments conducted on three benchmarks across four LVLMs have demonstrated significant improvements brought by our method, indicating its effectiveness and generality.



### MultiCorrupt: A Multi-Modal Robustness Dataset and Benchmark of LiDAR-Camera Fusion for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2402.11677v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2402.11677v3)
- **Published**: 2024-02-18 18:56:13+00:00
- **Updated**: 2024-04-20 13:00:25+00:00
- **Authors**: Till Beemelmanns, Quan Zhang, Christian Geller, Lutz Eckstein
- **Comment**: Code: https://github.com/ika-rwth-aachen/MultiCorrupt
- **Journal**: None
- **Summary**: Multi-modal 3D object detection models for automated driving have demonstrated exceptional performance on computer vision benchmarks like nuScenes. However, their reliance on densely sampled LiDAR point clouds and meticulously calibrated sensor arrays poses challenges for real-world applications. Issues such as sensor misalignment, miscalibration, and disparate sampling frequencies lead to spatial and temporal misalignment in data from LiDAR and cameras. Additionally, the integrity of LiDAR and camera data is often compromised by adverse environmental conditions such as inclement weather, leading to occlusions and noise interference. To address this challenge, we introduce MultiCorrupt, a comprehensive benchmark designed to evaluate the robustness of multi-modal 3D object detectors against ten distinct types of corruptions. We evaluate five state-of-the-art multi-modal detectors on MultiCorrupt and analyze their performance in terms of their resistance ability. Our results show that existing methods exhibit varying degrees of robustness depending on the type of corruption and their fusion strategy. We provide insights into which multi-modal design choices make such models robust against certain perturbations. The dataset generation code and benchmark are open-sourced at https://github.com/ika-rwth-aachen/MultiCorrupt.



### LiRaFusion: Deep Adaptive LiDAR-Radar Fusion for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2402.11735v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2402.11735v1)
- **Published**: 2024-02-18 23:29:28+00:00
- **Updated**: 2024-02-18 23:29:28+00:00
- **Authors**: Jingyu Song, Lingjun Zhao, Katherine A. Skinner
- **Comment**: Accepted to ICRA 2024
- **Journal**: None
- **Summary**: We propose LiRaFusion to tackle LiDAR-radar fusion for 3D object detection to fill the performance gap of existing LiDAR-radar detectors. To improve the feature extraction capabilities from these two modalities, we design an early fusion module for joint voxel feature encoding, and a middle fusion module to adaptively fuse feature maps via a gated network. We perform extensive evaluation on nuScenes to demonstrate that LiRaFusion leverages the complementary information of LiDAR and radar effectively and achieves notable improvement over existing methods.



