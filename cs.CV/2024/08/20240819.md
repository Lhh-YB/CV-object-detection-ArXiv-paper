# Arxiv Papers in cs.CV on 2024-08-19
### Photorealistic Object Insertion with Diffusion-Guided Inverse Rendering
- **Arxiv ID**: http://arxiv.org/abs/2408.09702v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2408.09702v1)
- **Published**: 2024-08-19 05:15:45+00:00
- **Updated**: 2024-08-19 05:15:45+00:00
- **Authors**: Ruofan Liang, Zan Gojcic, Merlin Nimier-David, David Acuna, Nandita Vijaykumar, Sanja Fidler, Zian Wang
- **Comment**: ECCV 2024, Project page:
  https://research.nvidia.com/labs/toronto-ai/DiPIR/
- **Journal**: None
- **Summary**: The correct insertion of virtual objects in images of real-world scenes requires a deep understanding of the scene's lighting, geometry and materials, as well as the image formation process. While recent large-scale diffusion models have shown strong generative and inpainting capabilities, we find that current models do not sufficiently "understand" the scene shown in a single picture to generate consistent lighting effects (shadows, bright reflections, etc.) while preserving the identity and details of the composited object. We propose using a personalized large diffusion model as guidance to a physically based inverse rendering process. Our method recovers scene lighting and tone-mapping parameters, allowing the photorealistic composition of arbitrary virtual objects in single frames or videos of indoor or outdoor scenes. Our physically based pipeline further enables automatic materials and tone-mapping refinement.



### Mutually-Aware Feature Learning for Few-Shot Object Counting
- **Arxiv ID**: http://arxiv.org/abs/2408.09734v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2408.09734v1)
- **Published**: 2024-08-19 06:46:24+00:00
- **Updated**: 2024-08-19 06:46:24+00:00
- **Authors**: Yerim Jeon, Subeen Lee, Jihwan Kim, Jae-Pil Heo
- **Comment**: Submitted to Pattern Recognition
- **Journal**: None
- **Summary**: Few-shot object counting has garnered significant attention for its practicality as it aims to count target objects in a query image based on given exemplars without the need for additional training. However, there is a shortcoming in the prevailing extract-and-match approach: query and exemplar features lack interaction during feature extraction since they are extracted unaware of each other and later correlated based on similarity. This can lead to insufficient target awareness of the extracted features, resulting in target confusion in precisely identifying the actual target when multiple class objects coexist. To address this limitation, we propose a novel framework, Mutually-Aware FEAture learning(MAFEA), which encodes query and exemplar features mutually aware of each other from the outset. By encouraging interaction between query and exemplar features throughout the entire pipeline, we can obtain target-aware features that are robust to a multi-category scenario. Furthermore, we introduce a background token to effectively associate the target region of query with exemplars and decouple its background region from them. Our extensive experiments demonstrate that our model reaches a new state-of-the-art performance on the two challenging benchmarks, FSCD-LVIS and FSC-147, with a remarkably reduced degree of the target confusion problem.



### DiscoNeRF: Class-Agnostic Object Field for 3D Object Discovery
- **Arxiv ID**: http://arxiv.org/abs/2408.09928v2
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2408.09928v2)
- **Published**: 2024-08-19 12:07:24+00:00
- **Updated**: 2024-09-06 07:20:10+00:00
- **Authors**: Corentin Dumery, Aoxiang Fan, Ren Li, Nicolas Talabot, Pascal Fua
- **Comment**: None
- **Journal**: None
- **Summary**: Neural Radiance Fields (NeRFs) have become a powerful tool for modeling 3D scenes from multiple images. However, NeRFs remain difficult to segment into semantically meaningful regions. Previous approaches to 3D segmentation of NeRFs either require user interaction to isolate a single object, or they rely on 2D semantic masks with a limited number of classes for supervision. As a consequence, they generalize poorly to class-agnostic masks automatically generated in real scenes. This is attributable to the ambiguity arising from zero-shot segmentation, yielding inconsistent masks across views. In contrast, we propose a method that is robust to inconsistent segmentations and successfully decomposes the scene into a set of objects of any class. By introducing a limited number of competing object slots against which masks are matched, a meaningful object representation emerges that best explains the 2D supervision and minimizes an additional regularization term. Our experiments demonstrate the ability of our method to generate 3D panoptic segmentations on complex scenes, and extract high-quality 3D assets from NeRFs that can then be used in virtual 3D environments.



### Video Object Segmentation via SAM 2: The 4th Solution for LSVOS Challenge VOS Track
- **Arxiv ID**: http://arxiv.org/abs/2408.10125v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2408.10125v2)
- **Published**: 2024-08-19 16:13:14+00:00
- **Updated**: 2024-08-24 13:07:51+00:00
- **Authors**: Feiyu Pan, Hao Fang, Runmin Cong, Wei Zhang, Xiankai Lu
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2408.00714
- **Journal**: None
- **Summary**: Video Object Segmentation (VOS) task aims to segmenting a particular object instance throughout the entire video sequence given only the object mask of the first frame. Recently, Segment Anything Model 2 (SAM 2) is proposed, which is a foundation model towards solving promptable visual segmentation in images and videos. SAM 2 builds a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. SAM 2 is a simple transformer architecture with streaming memory for real-time video processing, which trained on the date provides strong performance across a wide range of tasks. In this work, we evaluate the zero-shot performance of SAM 2 on the more challenging VOS datasets MOSE and LVOS. Without fine-tuning on the training set, SAM 2 achieved 75.79 J&F on the test set and ranked 4th place for 6th LSVOS Challenge VOS Track.



### SpaRP: Fast 3D Object Reconstruction and Pose Estimation from Sparse Views
- **Arxiv ID**: http://arxiv.org/abs/2408.10195v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2408.10195v1)
- **Published**: 2024-08-19 17:53:10+00:00
- **Updated**: 2024-08-19 17:53:10+00:00
- **Authors**: Chao Xu, Ang Li, Linghao Chen, Yulin Liu, Ruoxi Shi, Hao Su, Minghua Liu
- **Comment**: ECCV 2024
- **Journal**: None
- **Summary**: Open-world 3D generation has recently attracted considerable attention. While many single-image-to-3D methods have yielded visually appealing outcomes, they often lack sufficient controllability and tend to produce hallucinated regions that may not align with users' expectations. In this paper, we explore an important scenario in which the input consists of one or a few unposed 2D images of a single object, with little or no overlap. We propose a novel method, SpaRP, to reconstruct a 3D textured mesh and estimate the relative camera poses for these sparse-view images. SpaRP distills knowledge from 2D diffusion models and finetunes them to implicitly deduce the 3D spatial relationships between the sparse views. The diffusion model is trained to jointly predict surrogate representations for camera poses and multi-view images of the object under known poses, integrating all information from the input sparse views. These predictions are then leveraged to accomplish 3D reconstruction and pose estimation, and the reconstructed 3D model can be used to further refine the camera poses of input views. Through extensive experiments on three datasets, we demonstrate that our method not only significantly outperforms baseline methods in terms of 3D reconstruction quality and pose prediction accuracy but also exhibits strong efficiency. It requires only about 20 seconds to produce a textured mesh and camera poses for the input views. Project page: https://chaoxu.xyz/sparp.



