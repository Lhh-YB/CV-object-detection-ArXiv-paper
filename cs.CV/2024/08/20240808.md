# Arxiv Papers in cs.CV on 2024-08-08
### Multi-Scale and Detail-Enhanced Segment Anything Model for Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2408.04326v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.MM
- **Links**: [PDF](http://arxiv.org/pdf/2408.04326v1)
- **Published**: 2024-08-08 09:09:37+00:00
- **Updated**: 2024-08-08 09:09:37+00:00
- **Authors**: Shixuan Gao, Pingping Zhang, Tianyu Yan, Huchuan Lu
- **Comment**: This work is accepted by ACM MM2024
- **Journal**: None
- **Summary**: Salient Object Detection (SOD) aims to identify and segment the most prominent objects in images. Advanced SOD methods often utilize various Convolutional Neural Networks (CNN) or Transformers for deep feature extraction. However, these methods still deliver low performance and poor generalization in complex cases. Recently, Segment Anything Model (SAM) has been proposed as a visual fundamental model, which gives strong segmentation and generalization capabilities. Nonetheless, SAM requires accurate prompts of target objects, which are unavailable in SOD. Additionally, SAM lacks the utilization of multi-scale and multi-level information, as well as the incorporation of fine-grained details. To address these shortcomings, we propose a Multi-scale and Detail-enhanced SAM (MDSAM) for SOD. Specifically, we first introduce a Lightweight Multi-Scale Adapter (LMSA), which allows SAM to learn multi-scale information with very few trainable parameters. Then, we propose a Multi-Level Fusion Module (MLFM) to comprehensively utilize the multi-level information from the SAM's encoder. Finally, we propose a Detail Enhancement Module (DEM) to incorporate SAM with fine-grained details. Experimental results demonstrate the superior performance of our model on multiple SOD datasets and its strong generalization on other segmentation tasks. The source code is released at https://github.com/BellyBeauty/MDSAM.



### Detecting Car Speed using Object Detection and Depth Estimation: A Deep Learning Framework
- **Arxiv ID**: http://arxiv.org/abs/2408.04360v1
- **DOI**: None
- **Categories**: **cs.LG**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2408.04360v1)
- **Published**: 2024-08-08 10:47:02+00:00
- **Updated**: 2024-08-08 10:47:02+00:00
- **Authors**: Subhasis Dasgupta, Arshi Naaz, Jayeeta Choudhury, Nancy Lahiri
- **Comment**: This is the pre-print of the paper which was accepted for oral
  presentation and publication in the proceedings of IEEE CONIT 2024, organized
  at Pune from June 21 to 23, 2024. The paper is 6 pages long and it contains
  11 figures and 1 table. This is not the final version of the paper
- **Journal**: None
- **Summary**: Road accidents are quite common in almost every part of the world, and, in majority, fatal accidents are attributed to over speeding of vehicles. The tendency to over speeding is usually tried to be controlled using check points at various parts of the road but not all traffic police have the device to check speed with existing speed estimating devices such as LIDAR based, or Radar based guns. The current project tries to address the issue of vehicle speed estimation with handheld devices such as mobile phones or wearable cameras with network connection to estimate the speed using deep learning frameworks.



### Embodied Uncertainty-Aware Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2408.04760v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2408.04760v1)
- **Published**: 2024-08-08 21:29:22+00:00
- **Updated**: 2024-08-08 21:29:22+00:00
- **Authors**: Xiaolin Fang, Leslie Pack Kaelbling, Tomás Lozano-Pérez
- **Comment**: IROS 2024
- **Journal**: None
- **Summary**: We introduce uncertainty-aware object instance segmentation (UncOS) and demonstrate its usefulness for embodied interactive segmentation. To deal with uncertainty in robot perception, we propose a method for generating a hypothesis distribution of object segmentation. We obtain a set of region-factored segmentation hypotheses together with confidence estimates by making multiple queries of large pre-trained models. This process can produce segmentation results that achieve state-of-the-art performance on unseen object segmentation problems. The output can also serve as input to a belief-driven process for selecting robot actions to perturb the scene to reduce ambiguity. We demonstrate the effectiveness of this method in real-robot experiments. Website: https://sites.google.com/view/embodied-uncertain-seg



### SOD-YOLOv8 -- Enhancing YOLOv8 for Small Object Detection in Traffic Scenes
- **Arxiv ID**: http://arxiv.org/abs/2408.04786v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2408.04786v1)
- **Published**: 2024-08-08 23:05:25+00:00
- **Updated**: 2024-08-08 23:05:25+00:00
- **Authors**: Boshra Khalili, Andrew W. Smyth
- **Comment**: 15 pages, 14 figures
- **Journal**: None
- **Summary**: Object detection as part of computer vision can be crucial for traffic management, emergency response, autonomous vehicles, and smart cities. Despite significant advances in object detection, detecting small objects in images captured by distant cameras remains challenging due to their size, distance from the camera, varied shapes, and cluttered backgrounds. To address these challenges, we propose Small Object Detection YOLOv8 (SOD-YOLOv8), a novel model specifically designed for scenarios involving numerous small objects. Inspired by Efficient Generalized Feature Pyramid Networks (GFPN), we enhance multi-path fusion within YOLOv8 to integrate features across different levels, preserving details from shallower layers and improving small object detection accuracy. Also, A fourth detection layer is added to leverage high-resolution spatial information effectively. The Efficient Multi-Scale Attention Module (EMA) in the C2f-EMA module enhances feature extraction by redistributing weights and prioritizing relevant features. We introduce Powerful-IoU (PIoU) as a replacement for CIoU, focusing on moderate-quality anchor boxes and adding a penalty based on differences between predicted and ground truth bounding box corners. This approach simplifies calculations, speeds up convergence, and enhances detection accuracy. SOD-YOLOv8 significantly improves small object detection, surpassing widely used models in various metrics, without substantially increasing computational cost or latency compared to YOLOv8s. Specifically, it increases recall from 40.1\% to 43.9\%, precision from 51.2\% to 53.9\%, $\text{mAP}_{0.5}$ from 40.6\% to 45.1\%, and $\text{mAP}_{0.5:0.95}$ from 24\% to 26.6\%. In dynamic real-world traffic scenes, SOD-YOLOv8 demonstrated notable improvements in diverse conditions, proving its reliability and effectiveness in detecting small objects even in challenging environments.



