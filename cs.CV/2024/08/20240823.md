# Arxiv Papers in cs.CV on 2024-08-23
### Context-Aware Temporal Embedding of Objects in Video Data
- **Arxiv ID**: http://arxiv.org/abs/2408.12789v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2408.12789v1)
- **Published**: 2024-08-23 01:44:10+00:00
- **Updated**: 2024-08-23 01:44:10+00:00
- **Authors**: Ahnaf Farhan, M. Shahriar Hossain
- **Comment**: None
- **Journal**: None
- **Summary**: In video analysis, understanding the temporal context is crucial for recognizing object interactions, event patterns, and contextual changes over time. The proposed model leverages adjacency and semantic similarities between objects from neighboring video frames to construct context-aware temporal object embeddings. Unlike traditional methods that rely solely on visual appearance, our temporal embedding model considers the contextual relationships between objects, creating a meaningful embedding space where temporally connected object's vectors are positioned in proximity. Empirical studies demonstrate that our context-aware temporal embeddings can be used in conjunction with conventional visual embeddings to enhance the effectiveness of downstream applications. Moreover, the embeddings can be used to narrate a video using a Large Language Model (LLM). This paper describes the intricate details of the proposed objective function to generate context-aware temporal object embeddings for video data and showcases the potential applications of the generated embeddings in video analysis and object classification tasks.



### BoostTrack++: using tracklet information to detect more objects in multiple object tracking
- **Arxiv ID**: http://arxiv.org/abs/2408.13003v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2408.13003v1)
- **Published**: 2024-08-23 11:44:21+00:00
- **Updated**: 2024-08-23 11:44:21+00:00
- **Authors**: Vukašin Stanojević, Branimir Todorović
- **Comment**: None
- **Journal**: None
- **Summary**: Multiple object tracking (MOT) depends heavily on selection of true positive detected bounding boxes. However, this aspect of the problem is mostly overlooked or mitigated by employing two-stage association and utilizing low confidence detections in the second stage. Recently proposed BoostTrack attempts to avoid the drawbacks of multiple stage association approach and use low-confidence detections by applying detection confidence boosting. In this paper, we identify the limitations of the confidence boost used in BoostTrack and propose a method to improve its performance. To construct a richer similarity measure and enable a better selection of true positive detections, we propose to use a combination of shape, Mahalanobis distance and novel soft BIoU similarity. We propose a soft detection confidence boost technique which calculates new confidence scores based on the similarity measure and the previous confidence scores, and we introduce varying similarity threshold to account for lower similarity measure between detections and tracklets which are not regularly updated. The proposed additions are mutually independent and can be used in any MOT algorithm.   Combined with the BoostTrack+ baseline, our method achieves near state of the art results on the MOT17 dataset and new state of the art HOTA and IDF1 scores on the MOT20 dataset.   The source code is available at: https://github.com/vukasin-stanojevic/BoostTrack .



### ShapeICP: Iterative Category-level Object Pose and Shape Estimation from Depth
- **Arxiv ID**: http://arxiv.org/abs/2408.13147v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2408.13147v1)
- **Published**: 2024-08-23 15:12:55+00:00
- **Updated**: 2024-08-23 15:12:55+00:00
- **Authors**: Yihao Zhang, John J. Leonard
- **Comment**: None
- **Journal**: None
- **Summary**: Category-level object pose and shape estimation from a single depth image has recently drawn research attention due to its wide applications in robotics and self-driving. The task is particularly challenging because the three unknowns, object pose, object shape, and model-to-measurement correspondences, are compounded together but only a single view of depth measurements is provided. The vast majority of the prior work heavily relies on data-driven approaches to obtain solutions to at least one of the unknowns and typically two, running with the risk of failing to generalize to unseen domains. The shape representations used in the prior work also mainly focus on point cloud and signed distance field (SDF). In stark contrast to the prior work, we approach the problem using an iterative estimation method that does not require learning from any pose-annotated data. In addition, we adopt a novel mesh-based object active shape model that has not been explored by the previous literature. Our algorithm, named ShapeICP, has its foundation in the iterative closest point (ICP) algorithm but is equipped with additional features for the category-level pose and shape estimation task. The results show that even without using any pose-annotated data, ShapeICP surpasses many data-driven approaches that rely on the pose data for training, opening up new solution space for researchers to consider.



### Identifying Crucial Objects in Blind and Low-Vision Individuals' Navigation
- **Arxiv ID**: http://arxiv.org/abs/2408.13175v1
- **DOI**: 10.1145/3663548.3688538
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2408.13175v1)
- **Published**: 2024-08-23 15:50:50+00:00
- **Updated**: 2024-08-23 15:50:50+00:00
- **Authors**: Md Touhidul Islam, Imran Kabir, Elena Ariel Pearce, Md Alimoor Reza, Syed Masum Billah
- **Comment**: Paper accepted at ASSETS'24 (Oct 27-30, 2024, St. Johns,
  Newfoundland, Canada). arXiv admin note: substantial text overlap with
  arXiv:2407.16777
- **Journal**: None
- **Summary**: This paper presents a curated list of 90 objects essential for the navigation of blind and low-vision (BLV) individuals, encompassing road, sidewalk, and indoor environments. We develop the initial list by analyzing 21 publicly available videos featuring BLV individuals navigating various settings. Then, we refine the list through feedback from a focus group study involving blind, low-vision, and sighted companions of BLV individuals. A subsequent analysis reveals that most contemporary datasets used to train recent computer vision models contain only a small subset of the objects in our proposed list. Furthermore, we provide detailed object labeling for these 90 objects across 31 video segments derived from the original 21 videos. Finally, we make the object list, the 21 videos, and object labeling in the 31 video segments publicly available. This paper aims to fill the existing gap and foster the development of more inclusive and effective navigation aids for the BLV community.



