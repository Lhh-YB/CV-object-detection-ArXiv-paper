# Arxiv Papers in cs.CV on 2024-08-13
### MV-DETR: Multi-modality indoor object detection by Multi-View DEtecton TRansformers
- **Arxiv ID**: http://arxiv.org/abs/2408.06604v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2408.06604v1)
- **Published**: 2024-08-13 03:37:13+00:00
- **Updated**: 2024-08-13 03:37:13+00:00
- **Authors**: Zichao Dong, Yilin Zhang, Xufeng Huang, Hang Ji, Zhan Shi, Xin Zhan, Junbo Chen
- **Comment**: None
- **Journal**: None
- **Summary**: We introduce a novel MV-DETR pipeline which is effective while efficient transformer based detection method. Given input RGBD data, we notice that there are super strong pretraining weights for RGB data while less effective works for depth related data. First and foremost , we argue that geometry and texture cues are both of vital importance while could be encoded separately. Secondly, we find that visual texture feature is relatively hard to extract compared with geometry feature in 3d space. Unfortunately, single RGBD dataset with thousands of data is not enough for training an discriminating filter for visual texture feature extraction. Last but certainly not the least, we designed a lightweight VG module consists of a visual textual encoder, a geometry encoder and a VG connector. Compared with previous state of the art works like V-DETR, gains from pretrained visual encoder could be seen. Extensive experiments on ScanNetV2 dataset shows the effectiveness of our method. It is worth mentioned that our method achieve 78\% AP which create new state of the art on ScanNetv2 benchmark.



### Unified-IoU: For High-Quality Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2408.06636v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2408.06636v1)
- **Published**: 2024-08-13 04:56:45+00:00
- **Updated**: 2024-08-13 04:56:45+00:00
- **Authors**: Xiangjie Luo, Zhihao Cai, Bo Shao, Yingxun Wang
- **Comment**: None
- **Journal**: None
- **Summary**: Object detection is an important part in the field of computer vision, and the effect of object detection is directly determined by the regression accuracy of the prediction box. As the key to model training, IoU (Intersection over Union) greatly shows the difference between the current prediction box and the Ground Truth box. Subsequent researchers have continuously added more considerations to IoU, such as center distance, aspect ratio, and so on. However, there is an upper limit to just refining the geometric differences; And there is a potential connection between the new consideration index and the IoU itself, and the direct addition or subtraction between the two may lead to the problem of "over-consideration". Based on this, we propose a new IoU loss function, called Unified-IoU (UIoU), which is more concerned with the weight assignment between different quality prediction boxes. Specifically, the loss function dynamically shifts the model's attention from low-quality prediction boxes to high-quality prediction boxes in a novel way to enhance the model's detection performance on high-precision or intensive datasets and achieve a balance in training speed. Our proposed method achieves better performance on multiple datasets, especially at a high IoU threshold, UIoU has a more significant improvement effect compared with other improved IoU losses. Our code is publicly available at: https://github.com/lxj-drifter/UIOU_files.



### DC3DO: Diffusion Classifier for 3D Objects
- **Arxiv ID**: http://arxiv.org/abs/2408.06693v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.CG
- **Links**: [PDF](http://arxiv.org/pdf/2408.06693v1)
- **Published**: 2024-08-13 07:35:56+00:00
- **Updated**: 2024-08-13 07:35:56+00:00
- **Authors**: Nursena Koprucu, Meher Shashwat Nigam, Shicheng Xu, Biruk Abere, Gabriele Dominici, Andrew Rodriguez, Sharvaree Vadgama, Berfin Inal, Alberto Tono
- **Comment**: None
- **Journal**: None
- **Summary**: Inspired by Geoffrey Hinton emphasis on generative modeling, To recognize shapes, first learn to generate them, we explore the use of 3D diffusion models for object classification. Leveraging the density estimates from these models, our approach, the Diffusion Classifier for 3D Objects (DC3DO), enables zero-shot classification of 3D shapes without additional training. On average, our method achieves a 12.5 percent improvement compared to its multiview counterparts, demonstrating superior multimodal reasoning over discriminative approaches. DC3DO employs a class-conditional diffusion model trained on ShapeNet, and we run inferences on point clouds of chairs and cars. This work highlights the potential of generative models in 3D object classification.



### SlotLifter: Slot-guided Feature Lifting for Learning Object-centric Radiance Fields
- **Arxiv ID**: http://arxiv.org/abs/2408.06697v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2408.06697v1)
- **Published**: 2024-08-13 07:51:37+00:00
- **Updated**: 2024-08-13 07:51:37+00:00
- **Authors**: Yu Liu, Baoxiong Jia, Yixin Chen, Siyuan Huang
- **Comment**: Accepted by ECCV 2024. Project website: https://slotlifter.github.io
- **Journal**: None
- **Summary**: The ability to distill object-centric abstractions from intricate visual scenes underpins human-level generalization. Despite the significant progress in object-centric learning methods, learning object-centric representations in the 3D physical world remains a crucial challenge. In this work, we propose SlotLifter, a novel object-centric radiance model addressing scene reconstruction and decomposition jointly via slot-guided feature lifting. Such a design unites object-centric learning representations and image-based rendering methods, offering state-of-the-art performance in scene decomposition and novel-view synthesis on four challenging synthetic and four complex real-world datasets, outperforming existing 3D object-centric learning methods by a large margin. Through extensive ablative studies, we showcase the efficacy of designs in SlotLifter, revealing key insights for potential future directions.



### Enhancing Visual Dialog State Tracking through Iterative Object-Entity Alignment in Multi-Round Conversations
- **Arxiv ID**: http://arxiv.org/abs/2408.06725v1
- **DOI**: 10.1049/cit2.12370
- **Categories**: **cs.AI**, cs.CL, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2408.06725v1)
- **Published**: 2024-08-13 08:36:15+00:00
- **Updated**: 2024-08-13 08:36:15+00:00
- **Authors**: Wei Pang, Ruixue Duan, Jinfu Yang, Ning Li
- **Comment**: This article has been accepted in CAAI Transactions on Intelligence
  Technology! Article ID: CIT2_12370, Article DOI: 10.1049/cit2.12370
- **Journal**: None
- **Summary**: Visual Dialog (VD) is a task where an agent answers a series of image-related questions based on a multi-round dialog history. However, previous VD methods often treat the entire dialog history as a simple text input, disregarding the inherent conversational information flows at the round level. In this paper, we introduce Multi-round Dialogue State Tracking model (MDST), a framework that addresses this limitation by leveraging the dialogue state learned from dialog history to answer questions. MDST captures each round of dialog history, constructing internal dialogue state representations defined as 2-tuples of vision-language representations. These representations effectively ground the current question, enabling the generation of accurate answers. Experimental results on the VisDial v1.0 dataset demonstrate that MDST achieves a new state-of-the-art performance in generative setting. Furthermore, through a series of human studies, we validate the effectiveness of MDST in generating long, consistent, and human-like answers while consistently answering a series of questions correctly.



### Exploring Domain Shift on Radar-Based 3D Object Detection Amidst Diverse Environmental Conditions
- **Arxiv ID**: http://arxiv.org/abs/2408.06772v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2408.06772v1)
- **Published**: 2024-08-13 09:55:38+00:00
- **Updated**: 2024-08-13 09:55:38+00:00
- **Authors**: Miao Zhang, Sherif Abdulatif, Benedikt Loesch, Marco Altmann, Marius Schwarz, Bin Yang
- **Comment**: 6 pages, 5 figures, 3 tables, accepted in IEEE International
  Conference on Intelligent Transportation Systems (ITSC) 2024
- **Journal**: None
- **Summary**: The rapid evolution of deep learning and its integration with autonomous driving systems have led to substantial advancements in 3D perception using multimodal sensors. Notably, radar sensors show greater robustness compared to cameras and lidar under adverse weather and varying illumination conditions. This study delves into the often-overlooked yet crucial issue of domain shift in 4D radar-based object detection, examining how varying environmental conditions, such as different weather patterns and road types, impact 3D object detection performance. Our findings highlight distinct domain shifts across various weather scenarios, revealing unique dataset sensitivities that underscore the critical role of radar point cloud generation. Additionally, we demonstrate that transitioning between different road types, especially from highways to urban settings, introduces notable domain shifts, emphasizing the necessity for diverse data collection across varied road environments. To the best of our knowledge, this is the first comprehensive analysis of domain shift effects on 4D radar-based object detection. We believe this empirical study contributes to understanding the complex nature of domain shifts in radar data and suggests paths forward for data collection strategy in the face of environmental variability.



### Integrating Saliency Ranking and Reinforcement Learning for Enhanced Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2408.06803v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2408.06803v1)
- **Published**: 2024-08-13 10:46:42+00:00
- **Updated**: 2024-08-13 10:46:42+00:00
- **Authors**: Matthias Bartolo, Dylan Seychell, Josef Bajada
- **Comment**: Resultant work from Dissertation, Department of AI, University of
  Malta. Code available at: https://github.com/mbar0075/SaRLVision
- **Journal**: None
- **Summary**: With the ever-growing variety of object detection approaches, this study explores a series of experiments that combine reinforcement learning (RL)-based visual attention methods with saliency ranking techniques to investigate transparent and sustainable solutions. By integrating saliency ranking for initial bounding box prediction and subsequently applying RL techniques to refine these predictions through a finite set of actions over multiple time steps, this study aims to enhance RL object detection accuracy. Presented as a series of experiments, this research investigates the use of various image feature extraction methods and explores diverse Deep Q-Network (DQN) architectural variations for deep reinforcement learning-based localisation agent training. Additionally, we focus on optimising the detection pipeline at every step by prioritising lightweight and faster models, while also incorporating the capability to classify detected objects, a feature absent in previous RL approaches. We show that by evaluating the performance of these trained agents using the Pascal VOC 2007 dataset, faster and more optimised models were developed. Notably, the best mean Average Precision (mAP) achieved in this study was 51.4, surpassing benchmarks set by RL-based single object detectors in the literature.



### PBIR-NIE: Glossy Object Capture under Non-Distant Lighting
- **Arxiv ID**: http://arxiv.org/abs/2408.06878v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2408.06878v1)
- **Published**: 2024-08-13 13:26:24+00:00
- **Updated**: 2024-08-13 13:26:24+00:00
- **Authors**: Guangyan Cai, Fujun Luan, Miloš Hašan, Kai Zhang, Sai Bi, Zexiang Xu, Iliyan Georgiev, Shuang Zhao
- **Comment**: None
- **Journal**: None
- **Summary**: Glossy objects present a significant challenge for 3D reconstruction from multi-view input images under natural lighting. In this paper, we introduce PBIR-NIE, an inverse rendering framework designed to holistically capture the geometry, material attributes, and surrounding illumination of such objects. We propose a novel parallax-aware non-distant environment map as a lightweight and efficient lighting representation, accurately modeling the near-field background of the scene, which is commonly encountered in real-world capture setups. This feature allows our framework to accommodate complex parallax effects beyond the capabilities of standard infinite-distance environment maps. Our method optimizes an underlying signed distance field (SDF) through physics-based differentiable rendering, seamlessly connecting surface gradients between a triangle mesh and the SDF via neural implicit evolution (NIE). To address the intricacies of highly glossy BRDFs in differentiable rendering, we integrate the antithetic sampling algorithm to mitigate variance in the Monte Carlo gradient estimator. Consequently, our framework exhibits robust capabilities in handling glossy object reconstruction, showcasing superior quality in geometry, relighting, and material estimation.



### Efficient Human-Object-Interaction (EHOI) Detection via Interaction Label Coding and Conditional Decision
- **Arxiv ID**: http://arxiv.org/abs/2408.07018v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2408.07018v1)
- **Published**: 2024-08-13 16:34:06+00:00
- **Updated**: 2024-08-13 16:34:06+00:00
- **Authors**: Tsung-Shan Yang, Yun-Cheng Wang, Chengwei Wei, Suya You, C. -C. Jay Kuo
- **Comment**: None
- **Journal**: None
- **Summary**: Human-Object Interaction (HOI) detection is a fundamental task in image understanding. While deep-learning-based HOI methods provide high performance in terms of mean Average Precision (mAP), they are computationally expensive and opaque in training and inference processes. An Efficient HOI (EHOI) detector is proposed in this work to strike a good balance between detection performance, inference complexity, and mathematical transparency. EHOI is a two-stage method. In the first stage, it leverages a frozen object detector to localize the objects and extract various features as intermediate outputs. In the second stage, the first-stage outputs predict the interaction type using the XGBoost classifier. Our contributions include the application of error correction codes (ECCs) to encode rare interaction cases, which reduces the model size and the complexity of the XGBoost classifier in the second stage. Additionally, we provide a mathematical formulation of the relabeling and decision-making process. Apart from the architecture, we present qualitative results to explain the functionalities of the feedforward modules. Experimental results demonstrate the advantages of ECC-coded interaction labels and the excellent balance of detection performance and complexity of the proposed EHOI method.



