# Arxiv Papers in cs.CV on 2024-08-28
### A Comprehensive Review of 3D Object Detection in Autonomous Driving: Technological Advances and Future Directions
- **Arxiv ID**: http://arxiv.org/abs/2408.16530v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2408.16530v1)
- **Published**: 2024-08-28 01:08:33+00:00
- **Updated**: 2024-08-28 01:08:33+00:00
- **Authors**: Yu Wang, Shaohua Wang, Yicheng Li, Mingchun Liu
- **Comment**: None
- **Journal**: None
- **Summary**: In recent years, 3D object perception has become a crucial component in the development of autonomous driving systems, providing essential environmental awareness. However, as perception tasks in autonomous driving evolve, their variants have increased, leading to diverse insights from industry and academia. Currently, there is a lack of comprehensive surveys that collect and summarize these perception tasks and their developments from a broader perspective. This review extensively summarizes traditional 3D object detection methods, focusing on camera-based, LiDAR-based, and fusion detection techniques. We provide a comprehensive analysis of the strengths and limitations of each approach, highlighting advancements in accuracy and robustness. Furthermore, we discuss future directions, including methods to improve accuracy such as temporal perception, occupancy grids, and end-to-end learning frameworks. We also explore cooperative perception methods that extend the perception range through collaborative communication. By providing a holistic view of the current state and future developments in 3D object perception, we aim to offer a more comprehensive understanding of perception tasks for autonomous driving. Additionally, we have established an active repository to provide continuous updates on the latest advancements in this field, accessible at: https://github.com/Fishsoup0/Autonomous-Driving-Perception.



### Small Object Detection for Indoor Assistance to the Blind using YOLO NAS Small and Super Gradients
- **Arxiv ID**: http://arxiv.org/abs/2409.07469v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2409.07469v1)
- **Published**: 2024-08-28 05:38:20+00:00
- **Updated**: 2024-08-28 05:38:20+00:00
- **Authors**: Rashmi BN, R. Guru, Anusuya M A
- **Comment**: None
- **Journal**: None
- **Summary**: Advancements in object detection algorithms have opened new avenues for assistive technologies that cater to the needs of visually impaired individuals. This paper presents a novel approach for indoor assistance to the blind by addressing the challenge of small object detection. We propose a technique YOLO NAS Small architecture, a lightweight and efficient object detection model, optimized using the Super Gradients training framework. This combination enables real-time detection of small objects crucial for assisting the blind in navigating indoor environments, such as furniture, appliances, and household items. Proposed method emphasizes low latency and high accuracy, enabling timely and informative voice-based guidance to enhance the user's spatial awareness and interaction with their surroundings. The paper details the implementation, experimental results, and discusses the system's effectiveness in providing a practical solution for indoor assistance to the visually impaired.



### ConsistencyTrack: A Robust Multi-Object Tracker with a Generation Strategy of Consistency Model
- **Arxiv ID**: http://arxiv.org/abs/2408.15548v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2408.15548v1)
- **Published**: 2024-08-28 05:53:30+00:00
- **Updated**: 2024-08-28 05:53:30+00:00
- **Authors**: Lifan Jiang, Zhihui Wang, Siqi Yin, Guangxiao Ma, Peng Zhang, Boxi Wu
- **Comment**: arXiv admin note: text overlap with arXiv:2308.09905 by other authors
- **Journal**: None
- **Summary**: Multi-object tracking (MOT) is a critical technology in computer vision, designed to detect multiple targets in video sequences and assign each target a unique ID per frame. Existed MOT methods excel at accurately tracking multiple objects in real-time across various scenarios. However, these methods still face challenges such as poor noise resistance and frequent ID switches. In this research, we propose a novel ConsistencyTrack, joint detection and tracking(JDT) framework that formulates detection and association as a denoising diffusion process on perturbed bounding boxes. This progressive denoising strategy significantly improves the model's noise resistance. During the training phase, paired object boxes within two adjacent frames are diffused from ground-truth boxes to a random distribution, and then the model learns to detect and track by reversing this process. In inference, the model refines randomly generated boxes into detection and tracking results through minimal denoising steps. ConsistencyTrack also introduces an innovative target association strategy to address target occlusion. Experiments on the MOT17 and DanceTrack datasets demonstrate that ConsistencyTrack outperforms other compared methods, especially better than DiffusionTrack in inference speed and other performance metrics. Our code is available at https://github.com/Tankowa/ConsistencyTrack.



### Transfer Learning from Simulated to Real Scenes for Monocular 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2408.15637v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2408.15637v1)
- **Published**: 2024-08-28 08:44:58+00:00
- **Updated**: 2024-08-28 08:44:58+00:00
- **Authors**: Sondos Mohamed, Walter Zimmer, Ross Greer, Ahmed Alaaeldin Ghita, Modesto Castrill√≥n-Santana, Mohan Trivedi, Alois Knoll, Salvatore Mario Carta, Mirko Marras
- **Comment**: 18 pages. Accepted for ECVA European Conference on Computer Vision
  2024 (ECCV'24)
- **Journal**: None
- **Summary**: Accurately detecting 3D objects from monocular images in dynamic roadside scenarios remains a challenging problem due to varying camera perspectives and unpredictable scene conditions. This paper introduces a two-stage training strategy to address these challenges. Our approach initially trains a model on the large-scale synthetic dataset, RoadSense3D, which offers a diverse range of scenarios for robust feature learning. Subsequently, we fine-tune the model on a combination of real-world datasets to enhance its adaptability to practical conditions. Experimental results of the Cube R-CNN model on challenging public benchmarks show a remarkable improvement in detection performance, with a mean average precision rising from 0.26 to 12.76 on the TUM Traffic A9 Highway dataset and from 2.09 to 6.60 on the DAIR-V2X-I dataset when performing transfer learning. Code, data, and qualitative video results are available on the project website: https://roadsense3d.github.io.



### RIDE: Boosting 3D Object Detection for LiDAR Point Clouds via Rotation-Invariant Analysis
- **Arxiv ID**: http://arxiv.org/abs/2408.15643v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2408.15643v2)
- **Published**: 2024-08-28 08:53:33+00:00
- **Updated**: 2024-08-29 03:47:04+00:00
- **Authors**: Zhaoxuan Wang, Xu Han, Hongxin Liu, Xianzhi Li
- **Comment**: None
- **Journal**: None
- **Summary**: The rotation robustness property has drawn much attention to point cloud analysis, whereas it still poses a critical challenge in 3D object detection. When subjected to arbitrary rotation, most existing detectors fail to produce expected outputs due to the poor rotation robustness. In this paper, we present RIDE, a pioneering exploration of Rotation-Invariance for the 3D LiDAR-point-based object DEtector, with the key idea of designing rotation-invariant features from LiDAR scenes and then effectively incorporating them into existing 3D detectors. Specifically, we design a bi-feature extractor that extracts (i) object-aware features though sensitive to rotation but preserve geometry well, and (ii) rotation-invariant features, which lose geometric information to a certain extent but are robust to rotation. These two kinds of features complement each other to decode 3D proposals that are robust to arbitrary rotations. Particularly, our RIDE is compatible and easy to plug into the existing one-stage and two-stage 3D detectors, and boosts both detection performance and rotation robustness. Extensive experiments on the standard benchmarks showcase that the mean average precision (mAP) and rotation robustness can be significantly boosted by integrating with our RIDE, with +5.6% mAP and 53% rotation robustness improvement on KITTI, +5.1% and 28% improvement correspondingly on nuScenes. The code will be available soon.



### Object Detection for Vehicle Dashcams using Transformers
- **Arxiv ID**: http://arxiv.org/abs/2408.15809v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2408.15809v1)
- **Published**: 2024-08-28 14:08:24+00:00
- **Updated**: 2024-08-28 14:08:24+00:00
- **Authors**: Osama Mustafa, Khizer Ali, Anam Bibi, Imran Siddiqi, Momina Moetesum
- **Comment**: 7 Pages, and 6 Figures
- **Journal**: None
- **Summary**: The use of intelligent automation is growing significantly in the automotive industry, as it assists drivers and fleet management companies, thus increasing their productivity. Dash cams are now been used for this purpose which enables the instant identification and understanding of multiple objects and occurrences in the surroundings. In this paper, we propose a novel approach for object detection in dashcams using transformers. Our system is based on the state-of-the-art DEtection TRansformer (DETR), which has demonstrated strong performance in a variety of conditions, including different weather and illumination scenarios. The use of transformers allows for the consideration of contextual information in decisionmaking, improving the accuracy of object detection. To validate our approach, we have trained our DETR model on a dataset that represents real-world conditions. Our results show that the use of intelligent automation through transformers can significantly enhance the capabilities of dashcam systems. The model achieves an mAP of 0.95 on detection.



### Network transferability of adversarial patches in real-time object detection
- **Arxiv ID**: http://arxiv.org/abs/2408.15833v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2408.15833v1)
- **Published**: 2024-08-28 14:47:34+00:00
- **Updated**: 2024-08-28 14:47:34+00:00
- **Authors**: Jens Bayer, Stefan Becker, David M√ºnch, Michael Arens
- **Comment**: 7 pages, 6 figures, 1 table
- **Journal**: None
- **Summary**: Adversarial patches in computer vision can be used, to fool deep neural networks and manipulate their decision-making process. One of the most prominent examples of adversarial patches are evasion attacks for object detectors. By covering parts of objects of interest, these patches suppress the detections and thus make the target object 'invisible' to the object detector. Since these patches are usually optimized on a specific network with a specific train dataset, the transferability across multiple networks and datasets is not given. This paper addresses these issues and investigates the transferability across numerous object detector architectures. Our extensive evaluation across various models on two distinct datasets indicates that patches optimized with larger models provide better network transferability than patches that are optimized with smaller models.



### What is YOLOv8: An In-Depth Exploration of the Internal Features of the Next-Generation Object Detector
- **Arxiv ID**: http://arxiv.org/abs/2408.15857v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2408.15857v1)
- **Published**: 2024-08-28 15:18:46+00:00
- **Updated**: 2024-08-28 15:18:46+00:00
- **Authors**: Muhammad Yaseen
- **Comment**: None
- **Journal**: None
- **Summary**: This study presents a detailed analysis of the YOLOv8 object detection model, focusing on its architecture, training techniques, and performance improvements over previous iterations like YOLOv5. Key innovations, including the CSPNet backbone for enhanced feature extraction, the FPN+PAN neck for superior multi-scale object detection, and the transition to an anchor-free approach, are thoroughly examined. The paper reviews YOLOv8's performance across benchmarks like Microsoft COCO and Roboflow 100, highlighting its high accuracy and real-time capabilities across diverse hardware platforms. Additionally, the study explores YOLOv8's developer-friendly enhancements, such as its unified Python package and CLI, which streamline model training and deployment. Overall, this research positions YOLOv8 as a state-of-the-art solution in the evolving object detection field.



### microYOLO: Towards Single-Shot Object Detection on Microcontrollers
- **Arxiv ID**: http://arxiv.org/abs/2408.15865v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2408.15865v1)
- **Published**: 2024-08-28 15:29:27+00:00
- **Updated**: 2024-08-28 15:29:27+00:00
- **Authors**: Mark Deutel, Christopher Mutschler, J√ºrgen Teich
- **Comment**: Published at the ECML PKDD Conference 2023, at the 4th Workshop on
  IoT, Edge, and Mobile for Embedded Machine Learning
- **Journal**: None
- **Summary**: This work-in-progress paper presents results on the feasibility of single-shot object detection on microcontrollers using YOLO. Single-shot object detectors like YOLO are widely used, however due to their complexity mainly on larger GPU-based platforms. We present microYOLO, which can be used on Cortex-M based microcontrollers, such as the OpenMV H7 R2, achieving about 3.5 FPS when classifying 128x128 RGB images while using less than 800 KB Flash and less than 350 KB RAM. Furthermore, we share experimental results for three different object detection tasks, analyzing the accuracy of microYOLO on them.



### Unleashing the Temporal-Spatial Reasoning Capacity of GPT for Training-Free Audio and Language Referenced Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2408.15876v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2408.15876v2)
- **Published**: 2024-08-28 15:47:32+00:00
- **Updated**: 2024-12-23 08:10:30+00:00
- **Authors**: Shaofei Huang, Rui Ling, Hongyu Li, Tianrui Hui, Zongheng Tang, Xiaoming Wei, Jizhong Han, Si Liu
- **Comment**: Accepted by AAAI 2025
- **Journal**: None
- **Summary**: In this paper, we propose an Audio-Language-Referenced SAM 2 (AL-Ref-SAM 2) pipeline to explore the training-free paradigm for audio and language-referenced video object segmentation, namely AVS and RVOS tasks. The intuitive solution leverages GroundingDINO to identify the target object from a single frame and SAM 2 to segment the identified object throughout the video, which is less robust to spatiotemporal variations due to a lack of video context exploration. Thus, in our AL-Ref-SAM 2 pipeline, we propose a novel GPT-assisted Pivot Selection (GPT-PS) module to instruct GPT-4 to perform two-step temporal-spatial reasoning for sequentially selecting pivot frames and pivot boxes, thereby providing SAM 2 with a high-quality initial object prompt. Within GPT-PS, two task-specific Chain-of-Thought prompts are designed to unleash GPT's temporal-spatial reasoning capacity by guiding GPT to make selections based on a comprehensive understanding of video and reference information. Furthermore, we propose a Language-Binded Reference Unification (LBRU) module to convert audio signals into language-formatted references, thereby unifying the formats of AVS and RVOS tasks in the same pipeline. Extensive experiments on both tasks show that our training-free AL-Ref-SAM 2 pipeline achieves performances comparable to or even better than fully-supervised fine-tuning methods. The code is available at: https://github.com/appletea233/AL-Ref-SAM2.



