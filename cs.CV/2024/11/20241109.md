# Arxiv Papers in cs.CV on 2024-11-09
### GlocalCLIP: Object-agnostic Global-Local Prompt Learning for Zero-shot Anomaly Detection
- **Arxiv ID**: http://arxiv.org/abs/2411.06071v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2411.06071v3)
- **Published**: 2024-11-09 05:22:13+00:00
- **Updated**: 2024-12-08 13:35:08+00:00
- **Authors**: Jiyul Ham, Yonggon Jung, Jun-Geol Baek
- **Comment**: 29 pages, 36 figures
- **Journal**: None
- **Summary**: Zero-shot anomaly detection (ZSAD) is crucial for detecting anomalous patterns in target datasets without using training samples, specifically in scenarios where there are distributional differences between the target domain and training data or where data scarcity arises because of restricted access. Although recently pretrained vision-language models demonstrate strong zero-shot performance across various visual tasks, they focus on learning class semantics, which makes their direct application to ZSAD challenging. To address this scenario, we propose GlocalCLIP, which uniquely separates global and local prompts and jointly optimizes them. This approach enables the object-agnostic glocal semantic prompt to effectively capture general normal and anomalous patterns without dependency on specific objects in the image. We refine the text prompts for more precise adjustments by utilizing deep-text prompt tuning in the text encoder. In the vision encoder, we apply V-V attention layers to capture detailed local image features. Finally, we introduce glocal contrastive learning to improve the complementary learning of global and local prompts, effectively detecting anomalous patterns across various domains. The generalization performance of GlocalCLIP in ZSAD was demonstrated on 15 real-world datasets from both the industrial and medical domains, achieving superior performance compared to existing methods. Code will be made available at https://github.com/YUL-git/GlocalCLIP.



### ViTOC: Vision Transformer and Object-aware Captioner
- **Arxiv ID**: http://arxiv.org/abs/2411.07265v3
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2411.07265v3)
- **Published**: 2024-11-09 13:13:49+00:00
- **Updated**: 2024-11-27 15:45:23+00:00
- **Authors**: Feiyang Huang
- **Comment**: Major changes
- **Journal**: None
- **Summary**: This paper presents ViTOC (Vision Transformer and Object-aware Captioner), a novel vision-language model for image captioning that addresses the challenges of accuracy and diversity in generated descriptions. Unlike conventional approaches, ViTOC employs a dual-path architecture based on Vision Transformer and object detector, effectively fusing global visual features and local object information through learnable vectors. The model introduces an innovative object-aware prompting strategy that significantly enhances its capability in handling long-tail data. Experiments on the standard COCO dataset demonstrate that ViTOC outperforms baseline models across all evaluation metrics. Additionally, we propose a reference-free evaluation method based on CLIP to further validate the model's effectiveness. By utilizing pretrained visual model parameters, ViTOC achieves efficient end-to-end training.



### Multi-object Tracking by Detection and Query: an efficient end-to-end manner
- **Arxiv ID**: http://arxiv.org/abs/2411.06197v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2411.06197v1)
- **Published**: 2024-11-09 14:38:08+00:00
- **Updated**: 2024-11-09 14:38:08+00:00
- **Authors**: Shukun Jia, Yichao Cao, Feng Yang, Xin Lu, Xiaobo Lu
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-object tracking is advancing through two dominant paradigms: traditional tracking by detection and newly emerging tracking by query. In this work, we fuse them together and propose the tracking-by-detection-and-query paradigm, which is achieved by a Learnable Associator. Specifically, the basic information interaction module and the content-position alignment module are proposed for thorough information Interaction among object queries. Tracking results are directly Decoded from these queries. Hence, we name the method as LAID. Compared to tracking-by-query models, LAID achieves competitive tracking accuracy with notably higher training efficiency. With regard to tracking-by-detection methods, experimental results on DanceTrack show that LAID significantly surpasses the state-of-the-art heuristic method by 3.9% on HOTA metric and 6.1% on IDF1 metric. On SportsMOT, LAID also achieves the best score on HOTA metric. By holding low training cost, strong tracking capabilities, and an elegant end-to-end approach all at once, LAID presents a forward-looking direction for the field.



### Online Collision Risk Estimation via Monocular Depth-Aware Object Detectors and Fuzzy Inference
- **Arxiv ID**: http://arxiv.org/abs/2411.08060v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2411.08060v1)
- **Published**: 2024-11-09 20:20:36+00:00
- **Updated**: 2024-11-09 20:20:36+00:00
- **Authors**: Brian Hsuan-Cheng Liao, Yingjie Xu, Chih-Hong Cheng, Hasan Esen, Alois Knoll
- **Comment**: 7 pages (IEEE double column format), 5 figures, 3 tables, submitted
  to ICRA 2025
- **Journal**: None
- **Summary**: This paper presents a monitoring framework that infers the level of autonomous vehicle (AV) collision risk based on its object detector's performance using only monocular camera images. Essentially, the framework takes two sets of predictions produced by different algorithms and associates their inconsistencies with the collision risk via fuzzy inference. The first set of predictions is obtained through retrieving safety-critical 2.5D objects from a depth map, and the second set comes from the AV's 3D object detector. We experimentally validate that, based on Intersection-over-Union (IoU) and a depth discrepancy measure, the inconsistencies between the two sets of predictions strongly correlate to the safety-related error of the 3D object detector against ground truths. This correlation allows us to construct a fuzzy inference system and map the inconsistency measures to an existing collision risk indicator. In particular, we apply various knowledge- and data-driven techniques and find using particle swarm optimization that learns general fuzzy rules gives the best mapping result. Lastly, we validate our monitor's capability to produce relevant risk estimates with the large-scale nuScenes dataset and show it can safeguard an AV in closed-loop simulations.



