# Arxiv Papers in cs.CV on 2024-11-08
### SimpleBEV: Improved LiDAR-Camera Fusion Architecture for 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2411.05292v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2411.05292v1)
- **Published**: 2024-11-08 02:51:39+00:00
- **Updated**: 2024-11-08 02:51:39+00:00
- **Authors**: Yun Zhao, Zhan Gong, Peiru Zheng, Hong Zhu, Shaohua Wu
- **Comment**: None
- **Journal**: None
- **Summary**: More and more research works fuse the LiDAR and camera information to improve the 3D object detection of the autonomous driving system. Recently, a simple yet effective fusion framework has achieved an excellent detection performance, fusing the LiDAR and camera features in a unified bird's-eye-view (BEV) space. In this paper, we propose a LiDAR-camera fusion framework, named SimpleBEV, for accurate 3D object detection, which follows the BEV-based fusion framework and improves the camera and LiDAR encoders, respectively. Specifically, we perform the camera-based depth estimation using a cascade network and rectify the depth results with the depth information derived from the LiDAR points. Meanwhile, an auxiliary branch that implements the 3D object detection using only the camera-BEV features is introduced to exploit the camera information during the training phase. Besides, we improve the LiDAR feature extractor by fusing the multi-scaled sparse convolutional features. Experimental results demonstrate the effectiveness of our proposed method. Our method achieves 77.6\% NDS accuracy on the nuScenes dataset, showcasing superior performance in the 3D object detection track.



### POC-SLT: Partial Object Completion with SDF Latent Transformers
- **Arxiv ID**: http://arxiv.org/abs/2411.05419v1
- **DOI**: None
- **Categories**: **cs.CV**, I.4.8; I.4.5
- **Links**: [PDF](http://arxiv.org/pdf/2411.05419v1)
- **Published**: 2024-11-08 09:13:20+00:00
- **Updated**: 2024-11-08 09:13:20+00:00
- **Authors**: Faezeh Zakeri, Raphael Braun, Lukas Ruppert, Henrik P. A. Lensch
- **Comment**: None
- **Journal**: None
- **Summary**: 3D geometric shape completion hinges on representation learning and a deep understanding of geometric data. Without profound insights into the three-dimensional nature of the data, this task remains unattainable. Our work addresses this challenge of 3D shape completion given partial observations by proposing a transformer operating on the latent space representing Signed Distance Fields (SDFs). Instead of a monolithic volume, the SDF of an object is partitioned into smaller high-resolution patches leading to a sequence of latent codes. The approach relies on a smooth latent space encoding learned via a variational autoencoder (VAE), trained on millions of 3D patches. We employ an efficient masked autoencoder transformer to complete partial sequences into comprehensive shapes in latent space. Our approach is extensively evaluated on partial observations from ShapeNet and the ABC dataset where only fractions of the objects are given. The proposed POC-SLT architecture compares favorably with several baseline state-of-the-art methods, demonstrating a significant improvement in 3D shape completion, both qualitatively and quantitatively.



### Training objective drives the consistency of representational similarity across datasets
- **Arxiv ID**: http://arxiv.org/abs/2411.05561v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2411.05561v1)
- **Published**: 2024-11-08 13:35:45+00:00
- **Updated**: 2024-11-08 13:35:45+00:00
- **Authors**: Laure Ciernik, Lorenz Linhardt, Marco Morik, Jonas Dippel, Simon Kornblith, Lukas Muttenthaler
- **Comment**: 26 pages
- **Journal**: None
- **Summary**: The Platonic Representation Hypothesis claims that recent foundation models are converging to a shared representation space as a function of their downstream task performance, irrespective of the objectives and data modalities used to train these models. Representational similarity is generally measured for individual datasets and is not necessarily consistent across datasets. Thus, one may wonder whether this convergence of model representations is confounded by the datasets commonly used in machine learning. Here, we propose a systematic way to measure how representational similarity between models varies with the set of stimuli used to construct the representations. We find that the objective function is the most crucial factor in determining the consistency of representational similarities across datasets. Specifically, self-supervised vision models learn representations whose relative pairwise similarities generalize better from one dataset to another compared to those of image classification or image-text models. Moreover, the correspondence between representational similarities and the models' task behavior is dataset-dependent, being most strongly pronounced for single-domain datasets. Our work provides a framework for systematically measuring similarities of model representations across datasets and linking those similarities to differences in task behavior.



### Open-set object detection: towards unified problem formulation and benchmarking
- **Arxiv ID**: http://arxiv.org/abs/2411.05564v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2411.05564v1)
- **Published**: 2024-11-08 13:40:01+00:00
- **Updated**: 2024-11-08 13:40:01+00:00
- **Authors**: Hejer Ammar, Nikita Kiselov, Guillaume Lapouge, Romaric Audigier
- **Comment**: Accepted at ECCV 2024 Workshop: "The 3rd Workshop for
  Out-of-Distribution Generalization in Computer Vision Foundation Models"
- **Journal**: None
- **Summary**: In real-world applications where confidence is key, like autonomous driving, the accurate detection and appropriate handling of classes differing from those used during training are crucial. Despite the proposal of various unknown object detection approaches, we have observed widespread inconsistencies among them regarding the datasets, metrics, and scenarios used, alongside a notable absence of a clear definition for unknown objects, which hampers meaningful evaluation. To counter these issues, we introduce two benchmarks: a unified VOC-COCO evaluation, and the new OpenImagesRoad benchmark which provides clear hierarchical object definition besides new evaluation metrics. Complementing the benchmark, we exploit recent self-supervised Vision Transformers performance, to improve pseudo-labeling-based OpenSet Object Detection (OSOD), through OW-DETR++. State-of-the-art methods are extensively evaluated on the proposed benchmarks. This study provides a clear problem definition, ensures consistent evaluations, and draws new conclusions about effectiveness of OSOD strategies.



### Integrating Object Detection Modality into Visual Language Model for Enhanced Autonomous Driving Agent
- **Arxiv ID**: http://arxiv.org/abs/2411.05898v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2411.05898v1)
- **Published**: 2024-11-08 15:50:30+00:00
- **Updated**: 2024-11-08 15:50:30+00:00
- **Authors**: Linfeng He, Yiming Sun, Sihao Wu, Jiaxu Liu, Xiaowei Huang
- **Comment**: accepted by SafeGenAI workshop of NeurIPS 2024
- **Journal**: None
- **Summary**: In this paper, we propose a novel framework for enhancing visual comprehension in autonomous driving systems by integrating visual language models (VLMs) with additional visual perception module specialised in object detection. We extend the Llama-Adapter architecture by incorporating a YOLOS-based detection network alongside the CLIP perception network, addressing limitations in object detection and localisation. Our approach introduces camera ID-separators to improve multi-view processing, crucial for comprehensive environmental awareness. Experiments on the DriveLM visual question answering challenge demonstrate significant improvements over baseline models, with enhanced performance in ChatGPT scores, BLEU scores, and CIDEr metrics, indicating closeness of model answer to ground truth. Our method represents a promising step towards more capable and interpretable autonomous driving systems. Possible safety enhancement enabled by detection modality is also discussed.



