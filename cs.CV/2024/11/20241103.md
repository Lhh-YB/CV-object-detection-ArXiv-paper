# Arxiv Papers in cs.CV on 2024-11-03
### Object segmentation from common fate: Motion energy processing enables human-like zero-shot generalization to random dot stimuli
- **Arxiv ID**: http://arxiv.org/abs/2411.01505v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2411.01505v1)
- **Published**: 2024-11-03 09:59:45+00:00
- **Updated**: 2024-11-03 09:59:45+00:00
- **Authors**: Matthias Tangemann, Matthias KÃ¼mmerer, Matthias Bethge
- **Comment**: Accepted at NeurIPS 2024
- **Journal**: None
- **Summary**: Humans excel at detecting and segmenting moving objects according to the Gestalt principle of "common fate". Remarkably, previous works have shown that human perception generalizes this principle in a zero-shot fashion to unseen textures or random dots. In this work, we seek to better understand the computational basis for this capability by evaluating a broad range of optical flow models and a neuroscience inspired motion energy model for zero-shot figure-ground segmentation of random dot stimuli. Specifically, we use the extensively validated motion energy model proposed by Simoncelli and Heeger in 1998 which is fitted to neural recordings in cortex area MT. We find that a cross section of 40 deep optical flow models trained on different datasets struggle to estimate motion patterns in random dot videos, resulting in poor figure-ground segmentation performance. Conversely, the neuroscience-inspired model significantly outperforms all optical flow models on this task. For a direct comparison to human perception, we conduct a psychophysical study using a shape identification task as a proxy to measure human segmentation performance. All state-of-the-art optical flow models fall short of human performance, but only the motion energy model matches human capability. This neuroscience-inspired model successfully addresses the lack of human-like zero-shot generalization to random dot stimuli in current computer vision models, and thus establishes a compelling link between the Gestalt psychology of human object perception and cortical motion processing in the brain.   Code, models and datasets are available at https://github.com/mtangemann/motion_energy_segmentation



### Towards Small Object Editing: A Benchmark Dataset and A Training-Free Approach
- **Arxiv ID**: http://arxiv.org/abs/2411.01545v1
- **DOI**: 10.1145/3664647
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2411.01545v1)
- **Published**: 2024-11-03 12:38:23+00:00
- **Updated**: 2024-11-03 12:38:23+00:00
- **Authors**: Qihe Pan, Zhen Zhao, Zicheng Wang, Sifan Long, Yiming Wu, Wei Ji, Haoran Liang, Ronghua Liang
- **Comment**: 9 pages, 8 figures, Accepted by ACMMM 2024
- **Journal**: None
- **Summary**: A plethora of text-guided image editing methods has recently been developed by leveraging the impressive capabilities of large-scale diffusion-based generative models especially Stable Diffusion. Despite the success of diffusion models in producing high-quality images, their application to small object generation has been limited due to difficulties in aligning cross-modal attention maps between text and these objects. Our approach offers a training-free method that significantly mitigates this alignment issue with local and global attention guidance , enhancing the model's ability to accurately render small objects in accordance with textual descriptions. We detail the methodology in our approach, emphasizing its divergence from traditional generation techniques and highlighting its advantages. What's more important is that we also provide~\textit{SOEBench} (Small Object Editing), a standardized benchmark for quantitatively evaluating text-based small object generation collected from \textit{MSCOCO} and \textit{OpenImage}. Preliminary results demonstrate the effectiveness of our method, showing marked improvements in the fidelity and accuracy of small object generation compared to existing models. This advancement not only contributes to the field of AI and computer vision but also opens up new possibilities for applications in various industries where precise image generation is critical. We will release our dataset on our project page: \href{https://soebench.github.io/}{https://soebench.github.io/}.



### One for All: Multi-Domain Joint Training for Point Cloud Based 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2411.01584v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2411.01584v1)
- **Published**: 2024-11-03 14:21:56+00:00
- **Updated**: 2024-11-03 14:21:56+00:00
- **Authors**: Zhenyu Wang, Yali Li, Hengshuang Zhao, Shengjin Wang
- **Comment**: NeurIPS 2024
- **Journal**: None
- **Summary**: The current trend in computer vision is to utilize one universal model to address all various tasks. Achieving such a universal model inevitably requires incorporating multi-domain data for joint training to learn across multiple problem scenarios. In point cloud based 3D object detection, however, such multi-domain joint training is highly challenging, because large domain gaps among point clouds from different datasets lead to the severe domain-interference problem. In this paper, we propose \textbf{OneDet3D}, a universal one-for-all model that addresses 3D detection across different domains, including diverse indoor and outdoor scenes, within the \emph{same} framework and only \emph{one} set of parameters. We propose the domain-aware partitioning in scatter and context, guided by a routing mechanism, to address the data interference issue, and further incorporate the text modality for a language-guided classification to unify the multi-dataset label spaces and mitigate the category interference issue. The fully sparse structure and anchor-free head further accommodate point clouds with significant scale disparities. Extensive experiments demonstrate the strong universal ability of OneDet3D to utilize only one trained model for addressing almost all 3D object detection tasks.



