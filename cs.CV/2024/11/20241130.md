# Arxiv Papers in cs.CV on 2024-11-30
### Instant3dit: Multiview Inpainting for Fast Editing of 3D Objects
- **Arxiv ID**: http://arxiv.org/abs/2412.00518v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.GR
- **Links**: [PDF](http://arxiv.org/pdf/2412.00518v1)
- **Published**: 2024-11-30 15:58:33+00:00
- **Updated**: 2024-11-30 15:58:33+00:00
- **Authors**: Amir Barda, Matheus Gadelha, Vladimir G. Kim, Noam Aigerman, Amit H. Bermano, Thibault Groueix
- **Comment**: project page: https://amirbarda.github.io/Instant3dit.github.io/
- **Journal**: None
- **Summary**: We propose a generative technique to edit 3D shapes, represented as meshes, NeRFs, or Gaussian Splats, in approximately 3 seconds, without the need for running an SDS type of optimization. Our key insight is to cast 3D editing as a multiview image inpainting problem, as this representation is generic and can be mapped back to any 3D representation using the bank of available Large Reconstruction Models. We explore different fine-tuning strategies to obtain both multiview generation and inpainting capabilities within the same diffusion model. In particular, the design of the inpainting mask is an important factor of training an inpainting model, and we propose several masking strategies to mimic the types of edits a user would perform on a 3D shape. Our approach takes 3D generative editing from hours to seconds and produces higher-quality results compared to previous works.



### Generative LiDAR Editing with Controllable Novel Object Layouts
- **Arxiv ID**: http://arxiv.org/abs/2412.00592v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2412.00592v1)
- **Published**: 2024-11-30 21:39:51+00:00
- **Updated**: 2024-11-30 21:39:51+00:00
- **Authors**: Shing-Hei Ho, Bao Thach, Minghan Zhu
- **Comment**: Submitted to IEEE International Conference on Robotics and Automation
  (ICRA). 6 pages, 7 figures
- **Journal**: None
- **Summary**: We propose a framework to edit real-world Lidar scans with novel object layouts while preserving a realistic background environment. Compared to the synthetic data generation frameworks where Lidar point clouds are generated from scratch, our framework focuses on new scenario generation in a given background environment, and our method also provides labels for the generated data. This approach ensures the generated data remains relevant to the specific environment, aiding both the development and the evaluation of algorithms in real-world scenarios. Compared with novel view synthesis, our framework allows the creation of counterfactual scenarios with significant changes in the object layout and does not rely on multi-frame optimization. In our framework, the object removal and insertion are supported by generative background inpainting and object point cloud completion, and the entire pipeline is built upon spherical voxelization, which realizes the correct Lidar projective geometry by construction. Experiments show that our framework generates realistic Lidar scans with object layout changes and benefits the development of Lidar-based self-driving systems.



