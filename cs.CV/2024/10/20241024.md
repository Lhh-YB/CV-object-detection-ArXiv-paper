# Arxiv Papers in cs.CV on 2024-10-24
### You Only Look Around: Learning Illumination Invariant Feature for Low-light Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2410.18398v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2410.18398v1)
- **Published**: 2024-10-24 03:23:50+00:00
- **Updated**: 2024-10-24 03:23:50+00:00
- **Authors**: Mingbo Hong, Shen Cheng, Haibin Huang, Haoqiang Fan, Shuaicheng Liu
- **Comment**: Accepted by NeurIPS2024
- **Journal**: None
- **Summary**: In this paper, we introduce YOLA, a novel framework for object detection in low-light scenarios. Unlike previous works, we propose to tackle this challenging problem from the perspective of feature learning. Specifically, we propose to learn illumination-invariant features through the Lambertian image formation model. We observe that, under the Lambertian assumption, it is feasible to approximate illumination-invariant feature maps by exploiting the interrelationships between neighboring color channels and spatially adjacent pixels. By incorporating additional constraints, these relationships can be characterized in the form of convolutional kernels, which can be trained in a detection-driven manner within a network. Towards this end, we introduce a novel module dedicated to the extraction of illumination-invariant features from low-light images, which can be easily integrated into existing object detection frameworks. Our empirical findings reveal significant improvements in low-light object detection tasks, as well as promising results in both well-lit and over-lit scenarios. Code is available at \url{https://github.com/MingboHong/YOLA}.



### Radar and Camera Fusion for Object Detection and Tracking: A Comprehensive Survey
- **Arxiv ID**: http://arxiv.org/abs/2410.19872v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2410.19872v1)
- **Published**: 2024-10-24 07:37:57+00:00
- **Updated**: 2024-10-24 07:37:57+00:00
- **Authors**: Kun Shi, Shibo He, Zhenyu Shi, Anjun Chen, Zehui Xiong, Jiming Chen, Jun Luo
- **Comment**: None
- **Journal**: None
- **Summary**: Multi-modal fusion is imperative to the implementation of reliable object detection and tracking in complex environments. Exploiting the synergy of heterogeneous modal information endows perception systems the ability to achieve more comprehensive, robust, and accurate performance. As a nucleus concern in wireless-vision collaboration, radar-camera fusion has prompted prospective research directions owing to its extensive applicability, complementarity, and compatibility. Nonetheless, there still lacks a systematic survey specifically focusing on deep fusion of radar and camera for object detection and tracking. To fill this void, we embark on an endeavor to comprehensively review radar-camera fusion in a holistic way. First, we elaborate on the fundamental principles, methodologies, and applications of radar-camera fusion perception. Next, we delve into the key techniques concerning sensor calibration, modal representation, data alignment, and fusion operation. Furthermore, we provide a detailed taxonomy covering the research topics related to object detection and tracking in the context of radar and camera technologies.Finally, we discuss the emerging perspectives in the field of radar-camera fusion perception and highlight the potential areas for future research.



### Moving Object Segmentation in Point Cloud Data using Hidden Markov Models
- **Arxiv ID**: http://arxiv.org/abs/2410.18638v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2410.18638v1)
- **Published**: 2024-10-24 10:56:02+00:00
- **Updated**: 2024-10-24 10:56:02+00:00
- **Authors**: Vedant Bhandari, Jasmin James, Tyson Phillips, P. Ross McAree
- **Comment**: Accepted to the IEEE IROS 2024 workshop on Long-Term Perception for
  Autonomy in Dynamic Human-shared Environments: What Do Robots Need?
- **Journal**: None
- **Summary**: Autonomous agents require the capability to identify dynamic objects in their environment for safe planning and navigation. Incomplete and erroneous dynamic detections jeopardize the agent's ability to accomplish its task. Dynamic detection is a challenging problem due to the numerous sources of uncertainty inherent in the problem's inputs and the wide variety of applications, which often lead to use-case-tailored solutions. We propose a robust learning-free approach to segment moving objects in point cloud data. The foundation of the approach lies in modelling each voxel using a hidden Markov model (HMM), and probabilistically integrating beliefs into a map using an HMM filter. The proposed approach is tested on benchmark datasets and consistently performs better than or as well as state-of-the-art methods with strong generalized performance across sensor characteristics and environments. The approach is open-sourced at https://github.com/vb44/HMM-MOS.



### Learning Global Object-Centric Representations via Disentangled Slot Attention
- **Arxiv ID**: http://arxiv.org/abs/2410.18809v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2410.18809v1)
- **Published**: 2024-10-24 14:57:00+00:00
- **Updated**: 2024-10-24 14:57:00+00:00
- **Authors**: Tonglin Chen, Yinxuan Huang, Zhimeng Shen, Jinghao Huang, Bin Li, Xiangyang Xue
- **Comment**: Global Object-Centric Representations, Object Identification,
  Unsupervised Learning, Disentangled Learning
- **Journal**: None
- **Summary**: Humans can discern scene-independent features of objects across various environments, allowing them to swiftly identify objects amidst changing factors such as lighting, perspective, size, and position and imagine the complete images of the same object in diverse settings. Existing object-centric learning methods only extract scene-dependent object-centric representations, lacking the ability to identify the same object across scenes as humans. Moreover, some existing methods discard the individual object generation capabilities to handle complex scenes. This paper introduces a novel object-centric learning method to empower AI systems with human-like capabilities to identify objects across scenes and generate diverse scenes containing specific objects by learning a set of global object-centric representations. To learn the global object-centric representations that encapsulate globally invariant attributes of objects (i.e., the complete appearance and shape), this paper designs a Disentangled Slot Attention module to convert the scene features into scene-dependent attributes (such as scale, position and orientation) and scene-independent representations (i.e., appearance and shape). Experimental results substantiate the efficacy of the proposed method, demonstrating remarkable proficiency in global object-centric representation learning, object identification, scene generation with specific objects and scene decomposition.



