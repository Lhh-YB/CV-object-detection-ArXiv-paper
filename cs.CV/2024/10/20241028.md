# Arxiv Papers in cs.CV on 2024-10-28
### Novel Object Synthesis via Adaptive Text-Image Harmony
- **Arxiv ID**: http://arxiv.org/abs/2410.20823v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2410.20823v1)
- **Published**: 2024-10-28 08:14:40+00:00
- **Updated**: 2024-10-28 08:14:40+00:00
- **Authors**: Zeren Xiong, Zedong Zhang, Zikun Chen, Shuo Chen, Xiang Li, Gan Sun, Jian Yang, Jun Li
- **Comment**: NeurIPS2024
- **Journal**: None
- **Summary**: In this paper, we study an object synthesis task that combines an object text with an object image to create a new object image. However, most diffusion models struggle with this task, \textit{i.e.}, often generating an object that predominantly reflects either the text or the image due to an imbalance between their inputs. To address this issue, we propose a simple yet effective method called Adaptive Text-Image Harmony (ATIH) to generate novel and surprising objects. First, we introduce a scale factor and an injection step to balance text and image features in cross-attention and to preserve image information in self-attention during the text-image inversion diffusion process, respectively. Second, to better integrate object text and image, we design a balanced loss function with a noise parameter, ensuring both optimal editability and fidelity of the object image. Third, to adaptively adjust these parameters, we present a novel similarity score function that not only maximizes the similarities between the generated object image and the input text/image but also balances these similarities to harmonize text and image integration. Extensive experiments demonstrate the effectiveness of our approach, showcasing remarkable object creations such as colobus-glass jar. Project page: https://xzr52.github.io/ATIH/.



### EEG-Driven 3D Object Reconstruction with Style Consistency and Diffusion Prior
- **Arxiv ID**: http://arxiv.org/abs/2410.20981v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2410.20981v3)
- **Published**: 2024-10-28 12:59:24+00:00
- **Updated**: 2024-11-16 04:08:36+00:00
- **Authors**: Xin Xiang, Wenhui Zhou, Guojun Dai
- **Comment**: None
- **Journal**: None
- **Summary**: Electroencephalography (EEG)-based visual perception reconstruction has become an important area of research. Neuroscientific studies indicate that humans can decode imagined 3D objects by perceiving or imagining various visual information, such as color, shape, and rotation. Existing EEG-based visual decoding methods typically focus only on the reconstruction of 2D visual stimulus images and face various challenges in generation quality, including inconsistencies in texture, shape, and color between the visual stimuli and the reconstructed images. This paper proposes an EEG-based 3D object reconstruction method with style consistency and diffusion priors. The method consists of an EEG-driven multi-task joint learning stage and an EEG-to-3D diffusion stage. The first stage uses a neural EEG encoder based on regional semantic learning, employing a multi-task joint learning scheme that includes a masked EEG signal recovery task and an EEG based visual classification task. The second stage introduces a latent diffusion model (LDM) fine-tuning strategy with style-conditioned constraints and a neural radiance field (NeRF) optimization strategy. This strategy explicitly embeds semantic- and location-aware latent EEG codes and combines them with visual stimulus maps to fine-tune the LDM. The fine-tuned LDM serves as a diffusion prior, which, combined with the style loss of visual stimuli, is used to optimize NeRF for generating 3D objects. Finally, through experimental validation, we demonstrate that this method can effectively use EEG data to reconstruct 3D objects with style consistency.



### On the Black-box Explainability of Object Detection Models for Safe and Trustworthy Industrial Applications
- **Arxiv ID**: http://arxiv.org/abs/2411.00818v2
- **DOI**: 10.1016/j.rineng.2024.103498
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2411.00818v2)
- **Published**: 2024-10-28 13:28:05+00:00
- **Updated**: 2024-11-28 08:09:26+00:00
- **Authors**: Alain Andres, Aitor Martinez-Seras, Ibai Laña, Javier Del Ser
- **Comment**: 14 pages, 10 figures, 6 tables
- **Journal**: Volume 24, Year 2024, Page number 103498
- **Summary**: In the realm of human-machine interaction, artificial intelligence has become a powerful tool for accelerating data modeling tasks. Object detection methods have achieved outstanding results and are widely used in critical domains like autonomous driving and video surveillance. However, their adoption in high-risk applications, where errors may cause severe consequences, remains limited. Explainable Artificial Intelligence methods aim to address this issue, but many existing techniques are model-specific and designed for classification tasks, making them less effective for object detection and difficult for non-specialists to interpret. In this work we focus on model-agnostic explainability methods for object detection models and propose D-MFPP, an extension of the Morphological Fragmental Perturbation Pyramid (MFPP) technique based on segmentation-based masks to generate explanations. Additionally, we introduce D-Deletion, a novel metric combining faithfulness and localization, adapted specifically to meet the unique demands of object detectors. We evaluate these methods on real-world industrial and robotic datasets, examining the influence of parameters such as the number of masks, model size, and image resolution on the quality of explanations. Our experiments use single-stage object detection models applied to two safety-critical robotic environments: i) a shared human-robot workspace where safety is of paramount importance, and ii) an assembly area of battery kits, where safety is critical due to the potential for damage among high-risk components. Our findings evince that D-Deletion effectively gauges the performance of explanations when multiple elements of the same class appear in a scene, while D-MFPP provides a promising alternative to D-RISE when fewer masks are used.



### TACO: Adversarial Camouflage Optimization on Trucks to Fool Object Detectors
- **Arxiv ID**: http://arxiv.org/abs/2410.21443v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2410.21443v1)
- **Published**: 2024-10-28 18:40:06+00:00
- **Updated**: 2024-10-28 18:40:06+00:00
- **Authors**: Adonisz Dimitriu, Tamás Michaletzky, Viktor Remeli
- **Comment**: None
- **Journal**: None
- **Summary**: Adversarial attacks threaten the reliability of machine learning models in critical applications like autonomous vehicles and defense systems. As object detectors become more robust with models like YOLOv8, developing effective adversarial methodologies is increasingly challenging. We present Truck Adversarial Camouflage Optimization (TACO), a novel framework that generates adversarial camouflage patterns on 3D vehicle models to deceive state-of-the-art object detectors. Adopting Unreal Engine 5, TACO integrates differentiable rendering with a Photorealistic Rendering Network to optimize adversarial textures targeted at YOLOv8. To ensure the generated textures are both effective in deceiving detectors and visually plausible, we introduce the Convolutional Smooth Loss function, a generalized smooth loss function. Experimental evaluations demonstrate that TACO significantly degrades YOLOv8's detection performance, achieving an AP@0.5 of 0.0099 on unseen test data. Furthermore, these adversarial patterns exhibit strong transferability to other object detection models such as Faster R-CNN and earlier YOLO versions.



### Detection of moving objects through turbulent media. Decomposition of Oscillatory vs Non-Oscillatory spatio-temporal vector fields
- **Arxiv ID**: http://arxiv.org/abs/2410.21551v1
- **DOI**: 10.1016/j.imavis.2018.03.006
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2410.21551v1)
- **Published**: 2024-10-28 21:29:56+00:00
- **Updated**: 2024-10-28 21:29:56+00:00
- **Authors**: Jerome Gilles, Francis Alvarez, Nicholas B. Ferrante, Margaret Fortman, Lena Tahir, Alex Tarter, Anneke von Seeger
- **Comment**: None
- **Journal**: Image and Vision Computing Journal, Vol.73, 40--55, May 2018
- **Summary**: In this paper, we investigate how moving objects can be detected when images are impacted by atmospheric turbulence. We present a geometric spatio-temporal point of view to the problem and show that it is possible to distinguish movement due to the turbulence vs. moving objects. To perform this task, we propose an extension of 2D cartoon+texture decomposition algorithms to 3D vector fields. Our algorithm is based on curvelet spaces which permit to better characterize the movement flow geometry. We present experiments on real data which illustrate the efficiency of the proposed method.



### MVSDet: Multi-View Indoor 3D Object Detection via Efficient Plane Sweeps
- **Arxiv ID**: http://arxiv.org/abs/2410.21566v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2410.21566v1)
- **Published**: 2024-10-28 21:58:41+00:00
- **Updated**: 2024-10-28 21:58:41+00:00
- **Authors**: Yating Xu, Chen Li, Gim Hee Lee
- **Comment**: Accepted by NeurIPS 2024
- **Journal**: None
- **Summary**: The key challenge of multi-view indoor 3D object detection is to infer accurate geometry information from images for precise 3D detection. Previous method relies on NeRF for geometry reasoning. However, the geometry extracted from NeRF is generally inaccurate, which leads to sub-optimal detection performance. In this paper, we propose MVSDet which utilizes plane sweep for geometry-aware 3D object detection. To circumvent the requirement for a large number of depth planes for accurate depth prediction, we design a probabilistic sampling and soft weighting mechanism to decide the placement of pixel features on the 3D volume. We select multiple locations that score top in the probability volume for each pixel and use their probability score to indicate the confidence. We further apply recent pixel-aligned Gaussian Splatting to regularize depth prediction and improve detection performance with little computation overhead. Extensive experiments on ScanNet and ARKitScenes datasets are conducted to show the superiority of our model. Our code is available at https://github.com/Pixie8888/MVSDet.



