# Arxiv Papers in cs.CV on 2024-10-19
### 3D Multi-Object Tracking Employing MS-GLMB Filter for Autonomous Driving
- **Arxiv ID**: http://arxiv.org/abs/2410.14977v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2410.14977v1)
- **Published**: 2024-10-19 04:59:47+00:00
- **Updated**: 2024-10-19 04:59:47+00:00
- **Authors**: Linh Van Ma, Muhammad Ishfaq Hussain, Kin-Choong Yow, Moongu Jeon
- **Comment**: 2024 International Conference on Control, Automation and Information
  Sciences (ICCAIS), November 26th to 28th, 2024 in Ho Chi Minh City
- **Journal**: None
- **Summary**: The MS-GLMB filter offers a robust framework for tracking multiple objects through the use of multi-sensor data. Building on this, the MV-GLMB and MV-GLMB-AB filters enhance the MS-GLMB capabilities by employing cameras for 3D multi-sensor multi-object tracking, effectively addressing occlusions. However, both filters depend on overlapping fields of view from the cameras to combine complementary information. In this paper, we introduce an improved approach that integrates an additional sensor, such as LiDAR, into the MS-GLMB framework for 3D multi-object tracking. Specifically, we present a new LiDAR measurement model, along with a multi-camera and LiDAR multi-object measurement model. Our experimental results demonstrate a significant improvement in tracking performance compared to existing MS-GLMB-based methods. Importantly, our method eliminates the need for overlapping fields of view, broadening the applicability of the MS-GLMB filter. Our source code for nuScenes dataset is available at https://github.com/linh-gist/ms-glmb-nuScenes.



### The Solution for Single Object Tracking Task of Perception Test Challenge 2024
- **Arxiv ID**: http://arxiv.org/abs/2410.16329v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2410.16329v1)
- **Published**: 2024-10-19 06:35:13+00:00
- **Updated**: 2024-10-19 06:35:13+00:00
- **Authors**: Zhiqiang Zhong, Yang Yang, Fengqiang Wan, Henglu Wei, Xiangyang Ji
- **Comment**: None
- **Journal**: None
- **Summary**: This report presents our method for Single Object Tracking (SOT), which aims to track a specified object throughout a video sequence. We employ the LoRAT method. The essence of the work lies in adapting LoRA, a technique that fine-tunes a small subset of model parameters without adding inference latency, to the domain of visual tracking. We train our model using the extensive LaSOT and GOT-10k datasets, which provide a solid foundation for robust performance. Additionally, we implement the alpha-refine technique for post-processing the bounding box outputs. Although the alpha-refine method does not yield the anticipated results, our overall approach achieves a score of 0.813, securing first place in the competition.



### MambaSOD: Dual Mamba-Driven Cross-Modal Fusion Network for RGB-D Salient Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2410.15015v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2410.15015v1)
- **Published**: 2024-10-19 07:08:40+00:00
- **Updated**: 2024-10-19 07:08:40+00:00
- **Authors**: Yue Zhan, Zhihong Zeng, Haijun Liu, Xiaoheng Tan, Yinli Tian
- **Comment**: None
- **Journal**: None
- **Summary**: The purpose of RGB-D Salient Object Detection (SOD) is to pinpoint the most visually conspicuous areas within images accurately. While conventional deep models heavily rely on CNN extractors and overlook the long-range contextual dependencies, subsequent transformer-based models have addressed the issue to some extent but introduce high computational complexity. Moreover, incorporating spatial information from depth maps has been proven effective for this task. A primary challenge of this issue is how to fuse the complementary information from RGB and depth effectively. In this paper, we propose a dual Mamba-driven cross-modal fusion network for RGB-D SOD, named MambaSOD. Specifically, we first employ a dual Mamba-driven feature extractor for both RGB and depth to model the long-range dependencies in multiple modality inputs with linear complexity. Then, we design a cross-modal fusion Mamba for the captured multi-modal features to fully utilize the complementary information between the RGB and depth features. To the best of our knowledge, this work is the first attempt to explore the potential of the Mamba in the RGB-D SOD task, offering a novel perspective. Numerous experiments conducted on six prevailing datasets demonstrate our method's superiority over sixteen state-of-the-art RGB-D SOD models. The source code will be released at https://github.com/YueZhan721/MambaSOD.



### Cutting-Edge Detection of Fatigue in Drivers: A Comparative Study of Object Detection Models
- **Arxiv ID**: http://arxiv.org/abs/2410.15030v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2410.15030v1)
- **Published**: 2024-10-19 08:06:43+00:00
- **Updated**: 2024-10-19 08:06:43+00:00
- **Authors**: Amelia Jones
- **Comment**: None
- **Journal**: None
- **Summary**: This research delves into the development of a fatigue detection system based on modern object detection algorithms, particularly YOLO (You Only Look Once) models, including YOLOv5, YOLOv6, YOLOv7, and YOLOv8. By comparing the performance of these models, we evaluate their effectiveness in real-time detection of fatigue-related behavior in drivers. The study addresses challenges like environmental variability and detection accuracy and suggests a roadmap for enhancing real-time detection. Experimental results demonstrate that YOLOv8 offers superior performance, balancing accuracy with speed. Data augmentation techniques and model optimization have been key in enhancing system adaptability to various driving conditions.



