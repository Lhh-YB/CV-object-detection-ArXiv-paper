# Arxiv Papers in cs.CV on 2024-10-01
### Can We Remove the Ground? Obstacle-aware Point Cloud Compression for Remote Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2410.00582v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2410.00582v1)
- **Published**: 2024-10-01 11:05:33+00:00
- **Updated**: 2024-10-01 11:05:33+00:00
- **Authors**: Pengxi Zeng, Alberto Presta, Jonah Reinis, Dinesh Bharadia, Hang Qiu, Pamela Cosman
- **Comment**: 7 Pages; submitted to ICRA 2025
- **Journal**: None
- **Summary**: Efficient point cloud (PC) compression is crucial for streaming applications, such as augmented reality and cooperative perception. Classic PC compression techniques encode all the points in a frame. Tailoring compression towards perception tasks at the receiver side, we ask the question, "Can we remove the ground points during transmission without sacrificing the detection performance?" Our study reveals a strong dependency on the ground from state-of-the-art (SOTA) 3D object detection models, especially on those points below and around the object. In this work, we propose a lightweight obstacle-aware Pillar-based Ground Removal (PGR) algorithm. PGR filters out ground points that do not provide context to object recognition, significantly improving compression ratio without sacrificing the receiver side perception performance. Not using heavy object detection or semantic segmentation models, PGR is light-weight, highly parallelizable, and effective. Our evaluations on KITTI and Waymo Open Dataset show that SOTA detection models work equally well with PGR removing 20-30% of the points, with a speeding of 86 FPS.



### Simplified priors for Object-Centric Learning
- **Arxiv ID**: http://arxiv.org/abs/2410.00728v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.LG
- **Links**: [PDF](http://arxiv.org/pdf/2410.00728v1)
- **Published**: 2024-10-01 14:16:13+00:00
- **Updated**: 2024-10-01 14:16:13+00:00
- **Authors**: Vihang Patil, Andreas Radler, Daniel Klotz, Sepp Hochreiter
- **Comment**: None
- **Journal**: None
- **Summary**: Humans excel at abstracting data and constructing \emph{reusable} concepts, a capability lacking in current continual learning systems. The field of object-centric learning addresses this by developing abstract representations, or slots, from data without human supervision. Different methods have been proposed to tackle this task for images, whereas most are overly complex, non-differentiable, or poorly scalable. In this paper, we introduce a conceptually simple, fully-differentiable, non-iterative, and scalable method called SAMP Simplified Slot Attention with Max Pool Priors). It is implementable using only Convolution and MaxPool layers and an Attention layer. Our method encodes the input image with a Convolutional Neural Network and then uses a branch of alternating Convolution and MaxPool layers to create specialized sub-networks and extract primitive slots. These primitive slots are then used as queries for a Simplified Slot Attention over the encoded image. Despite its simplicity, our method is competitive or outperforms previous methods on standard benchmarks.



### ARPOV: Expanding Visualization of Object Detection in AR with Panoramic Mosaic Stitching
- **Arxiv ID**: http://arxiv.org/abs/2410.01055v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2410.01055v1)
- **Published**: 2024-10-01 20:29:14+00:00
- **Updated**: 2024-10-01 20:29:14+00:00
- **Authors**: Erin McGowan, Ethan Brewer, Claudio Silva
- **Comment**: 6 pages, 6 figures, to be published in SIBGRAPI 2024 - 37th
  conference on Graphics, Patterns, and Images proceedings
- **Journal**: None
- **Summary**: As the uses of augmented reality (AR) become more complex and widely available, AR applications will increasingly incorporate intelligent features that require developers to understand the user's behavior and surrounding environment (e.g. an intelligent assistant). Such applications rely on video captured by an AR headset, which often contains disjointed camera movement with a limited field of view that cannot capture the full scope of what the user sees at any given time. Moreover, standard methods of visualizing object detection model outputs are limited to capturing objects within a single frame and timestep, and therefore fail to capture the temporal and spatial context that is often necessary for various domain applications. We propose ARPOV, an interactive visual analytics tool for analyzing object detection model outputs tailored to video captured by an AR headset that maximizes user understanding of model performance. The proposed tool leverages panorama stitching to expand the view of the environment while automatically filtering undesirable frames, and includes interactive features that facilitate object detection model debugging. ARPOV was designed as part of a collaboration between visualization researchers and machine learning and AR experts; we validate our design choices through interviews with 5 domain experts.



### Pose Estimation of Buried Deep-Sea Objects using 3D Vision Deep Learning Models
- **Arxiv ID**: http://arxiv.org/abs/2410.01061v2
- **DOI**: 10.1109/OCEANS55160.2024.10754044
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2410.01061v2)
- **Published**: 2024-10-01 20:43:20+00:00
- **Updated**: 2024-12-11 00:43:16+00:00
- **Authors**: Jerry Yan, Chinmay Talegaonkar, Nicholas Antipa, Eric Terrill, Sophia Merrifield
- **Comment**: Submitted to OCEANS 2024 Halifax
- **Journal**: OCEANS 2024 - Halifax
- **Summary**: We present an approach for pose and burial fraction estimation of debris field barrels found on the seabed in the Southern California San Pedro Basin. Our computational workflow leverages recent advances in foundation models for segmentation and a vision transformer-based approach to estimate the point cloud which defines the geometry of the barrel. We propose BarrelNet for estimating the 6-DOF pose and radius of buried barrels from the barrel point clouds as input. We train BarrelNet using synthetically generated barrel point clouds, and qualitatively demonstrate the potential of our approach using remotely operated vehicle (ROV) video footage of barrels found at a historic dump site. We compare our method to a traditional least squares fitting approach and show significant improvement according to our defined benchmarks.



### Synthetic imagery for fuzzy object detection: A comparative study
- **Arxiv ID**: http://arxiv.org/abs/2410.01124v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2410.01124v1)
- **Published**: 2024-10-01 23:22:54+00:00
- **Updated**: 2024-10-01 23:22:54+00:00
- **Authors**: Siavash H. Khajavi, Mehdi Moshtaghi, Dikai Yu, Zixuan Liu, Kary Främling, Jan Holmström
- **Comment**: None
- **Journal**: None
- **Summary**: The fuzzy object detection is a challenging field of research in computer vision (CV). Distinguishing between fuzzy and non-fuzzy object detection in CV is important. Fuzzy objects such as fire, smoke, mist, and steam present significantly greater complexities in terms of visual features, blurred edges, varying shapes, opacity, and volume compared to non-fuzzy objects such as trees and cars. Collection of a balanced and diverse dataset and accurate annotation is crucial to achieve better ML models for fuzzy objects, however, the task of collection and annotation is still highly manual. In this research, we propose and leverage an alternative method of generating and automatically annotating fully synthetic fire images based on 3D models for training an object detection model. Moreover, the performance, and efficiency of the trained ML models on synthetic images is compared with ML models trained on real imagery and mixed imagery. Findings proved the effectiveness of the synthetic data for fire detection, while the performance improves as the test dataset covers a broader spectrum of real fires. Our findings illustrates that when synthetic imagery and real imagery is utilized in a mixed training set the resulting ML model outperforms models trained on real imagery as well as models trained on synthetic imagery for detection of a broad spectrum of fires. The proposed method for automating the annotation of synthetic fuzzy objects imagery carries substantial implications for reducing both time and cost in creating computer vision models specifically tailored for detecting fuzzy objects.



