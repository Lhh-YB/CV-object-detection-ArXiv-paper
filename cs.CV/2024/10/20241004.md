# Arxiv Papers in cs.CV on 2024-10-04
### CLIP-Clique: Graph-based Correspondence Matching Augmented by Vision Language Models for Object-based Global Localization
- **Arxiv ID**: http://arxiv.org/abs/2410.03054v1
- **DOI**: 10.1109/LRA.2024.3474482
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2410.03054v1)
- **Published**: 2024-10-04 00:23:20+00:00
- **Updated**: 2024-10-04 00:23:20+00:00
- **Authors**: Shigemichi Matsuzaki, Kazuhito Tanaka, Kazuhiro Shintani
- **Comment**: IEEE Robotics and Automation Letters
- **Journal**: None
- **Summary**: This letter proposes a method of global localization on a map with semantic object landmarks. One of the most promising approaches for localization on object maps is to use semantic graph matching using landmark descriptors calculated from the distribution of surrounding objects. These descriptors are vulnerable to misclassification and partial observations. Moreover, many existing methods rely on inlier extraction using RANSAC, which is stochastic and sensitive to a high outlier rate. To address the former issue, we augment the correspondence matching using Vision Language Models (VLMs). Landmark discriminability is improved by VLM embeddings, which are independent of surrounding objects. In addition, inliers are estimated deterministically using a graph-theoretic approach. We also incorporate pose calculation using the weighted least squares considering correspondence similarity and observation completeness to improve the robustness. We confirmed improvements in matching and pose estimation accuracy through experiments on ScanNet and TUM datasets.



### Investigating and Mitigating Object Hallucinations in Pretrained Vision-Language (CLIP) Models
- **Arxiv ID**: http://arxiv.org/abs/2410.03176v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2410.03176v1)
- **Published**: 2024-10-04 06:24:49+00:00
- **Updated**: 2024-10-04 06:24:49+00:00
- **Authors**: Yufang Liu, Tao Ji, Changzhi Sun, Yuanbin Wu, Aimin Zhou
- **Comment**: EMNLP 2024
- **Journal**: None
- **Summary**: Large Vision-Language Models (LVLMs) have achieved impressive performance, yet research has pointed out a serious issue with object hallucinations within these models. However, there is no clear conclusion as to which part of the model these hallucinations originate from. In this paper, we present an in-depth investigation into the object hallucination problem specifically within the CLIP model, which serves as the backbone for many state-of-the-art vision-language systems. We unveil that even in isolation, the CLIP model is prone to object hallucinations, suggesting that the hallucination problem is not solely due to the interaction between vision and language modalities. To address this, we propose a counterfactual data augmentation method by creating negative samples with a variety of hallucination issues. We demonstrate that our method can effectively mitigate object hallucinations for CLIP model, and we show the the enhanced model can be employed as a visual encoder, effectively alleviating the object hallucination issue in LVLMs.



### Enhancing Autonomous Navigation by Imaging Hidden Objects using Single-Photon LiDAR
- **Arxiv ID**: http://arxiv.org/abs/2410.03555v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.CV
- **Links**: [PDF](http://arxiv.org/pdf/2410.03555v1)
- **Published**: 2024-10-04 16:03:13+00:00
- **Updated**: 2024-10-04 16:03:13+00:00
- **Authors**: Aaron Young, Nevindu M. Batagoda, Harry Zhang, Akshat Dave, Adithya Pediredla, Dan Negrut, Ramesh Raskar
- **Comment**: Project webpage:
  https://github.com/camera-culture/nlos-aided-autonomous-navigation
- **Journal**: None
- **Summary**: Robust autonomous navigation in environments with limited visibility remains a critical challenge in robotics. We present a novel approach that leverages Non-Line-of-Sight (NLOS) sensing using single-photon LiDAR to improve visibility and enhance autonomous navigation. Our method enables mobile robots to "see around corners" by utilizing multi-bounce light information, effectively expanding their perceptual range without additional infrastructure. We propose a three-module pipeline: (1) Sensing, which captures multi-bounce histograms using SPAD-based LiDAR; (2) Perception, which estimates occupancy maps of hidden regions from these histograms using a convolutional neural network; and (3) Control, which allows a robot to follow safe paths based on the estimated occupancy. We evaluate our approach through simulations and real-world experiments on a mobile robot navigating an L-shaped corridor with hidden obstacles. Our work represents the first experimental demonstration of NLOS imaging for autonomous navigation, paving the way for safer and more efficient robotic systems operating in complex environments. We also contribute a novel dynamics-integrated transient rendering framework for simulating NLOS scenarios, facilitating future research in this domain.



### STONE: A Submodular Optimization Framework for Active 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2410.03918v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2410.03918v2)
- **Published**: 2024-10-04 20:45:33+00:00
- **Updated**: 2024-11-01 05:23:35+00:00
- **Authors**: Ruiyu Mao, Sarthak Kumar Maharana, Rishabh K Iyer, Yunhui Guo
- **Comment**: None
- **Journal**: None
- **Summary**: 3D object detection is fundamentally important for various emerging applications, including autonomous driving and robotics. A key requirement for training an accurate 3D object detector is the availability of a large amount of LiDAR-based point cloud data. Unfortunately, labeling point cloud data is extremely challenging, as accurate 3D bounding boxes and semantic labels are required for each potential object. This paper proposes a unified active 3D object detection framework, for greatly reducing the labeling cost of training 3D object detectors. Our framework is based on a novel formulation of submodular optimization, specifically tailored to the problem of active 3D object detection. In particular, we address two fundamental challenges associated with active 3D object detection: data imbalance and the need to cover the distribution of the data, including LiDAR-based point cloud data of varying difficulty levels. Extensive experiments demonstrate that our method achieves state-of-the-art performance with high computational efficiency compared to existing active learning methods. The code is available at https://github.com/RuiyuM/STONE.



### Learning Object Properties Using Robot Proprioception via Differentiable Robot-Object Interaction
- **Arxiv ID**: http://arxiv.org/abs/2410.03920v1
- **DOI**: None
- **Categories**: **cs.RO**, cs.AI, cs.CE, cs.CV, physics.comp-ph
- **Links**: [PDF](http://arxiv.org/pdf/2410.03920v1)
- **Published**: 2024-10-04 20:48:38+00:00
- **Updated**: 2024-10-04 20:48:38+00:00
- **Authors**: Peter Yichen Chen, Chao Liu, Pingchuan Ma, John Eastman, Daniela Rus, Dylan Randle, Yuri Ivanov, Wojciech Matusik
- **Comment**: arXiv admin comment: This version has been removed by arXiv
  administrators as the submitter did not have the rights to agree to the
  license at the time of submission
- **Journal**: None
- **Summary**: Differentiable simulation has become a powerful tool for system identification. While prior work has focused on identifying robot properties using robot-specific data or object properties using object-specific data, our approach calibrates object properties by using information from the robot, without relying on data from the object itself. Specifically, we utilize robot joint encoder information, which is commonly available in standard robotic systems. Our key observation is that by analyzing the robot's reactions to manipulated objects, we can infer properties of those objects, such as inertia and softness. Leveraging this insight, we develop differentiable simulations of robot-object interactions to inversely identify the properties of the manipulated objects. Our approach relies solely on proprioception -- the robot's internal sensing capabilities -- and does not require external measurement tools or vision-based tracking systems. This general method is applicable to any articulated robot and requires only joint position information. We demonstrate the effectiveness of our method on a low-cost robotic platform, achieving accurate mass and elastic modulus estimations of manipulated objects with just a few seconds of computation on a laptop.



