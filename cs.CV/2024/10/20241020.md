# Arxiv Papers in cs.CV on 2024-10-20
### Open-vocabulary vs. Closed-set: Best Practice for Few-shot Object Detection Considering Text Describability
- **Arxiv ID**: http://arxiv.org/abs/2410.15315v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2410.15315v1)
- **Published**: 2024-10-20 06:59:35+00:00
- **Updated**: 2024-10-20 06:59:35+00:00
- **Authors**: Yusuke Hosoya, Masanori Suganuma, Takayuki Okatani
- **Comment**: 20 pages, 3 figures
- **Journal**: None
- **Summary**: Open-vocabulary object detection (OVD), detecting specific classes of objects using only their linguistic descriptions (e.g., class names) without any image samples, has garnered significant attention. However, in real-world applications, the target class concepts is often hard to describe in text and the only way to specify target objects is to provide their image examples, yet it is often challenging to obtain a good number of samples. Thus, there is a high demand from practitioners for few-shot object detection (FSOD). A natural question arises: Can the benefits of OVD extend to FSOD for object classes that are difficult to describe in text? Compared to traditional methods that learn only predefined classes (referred to in this paper as closed-set object detection, COD), can the extra cost of OVD be justified? To answer these questions, we propose a method to quantify the ``text-describability'' of object detection datasets using the zero-shot image classification accuracy with CLIP. This allows us to categorize various OD datasets with different text-describability and emprically evaluate the FSOD performance of OVD and COD methods within each category. Our findings reveal that: i) there is little difference between OVD and COD for object classes with low text-describability under equal conditions in OD pretraining; and ii) although OVD can learn from more diverse data than OD-specific data, thereby increasing the volume of training data, it can be counterproductive for classes with low-text-describability. These findings provide practitioners with valuable guidance amidst the recent advancements of OVD methods.



### TrackMe:A Simple and Effective Multiple Object Tracking Annotation Tool
- **Arxiv ID**: http://arxiv.org/abs/2410.15518v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2410.15518v1)
- **Published**: 2024-10-20 21:57:25+00:00
- **Updated**: 2024-10-20 21:57:25+00:00
- **Authors**: Thinh Phan, Isaac Phillips, Andrew Lockett, Michael T. Kidd, Ngan Le
- **Comment**: None
- **Journal**: None
- **Summary**: Object tracking, especially animal tracking, is one of the key topics that attract a lot of attention due to its benefits of animal behavior understanding and monitoring. Recent state-of-the-art tracking methods are founded on deep learning architectures for object detection, appearance feature extraction and track association. Despite the good tracking performance, these methods are trained and evaluated on common objects such as human and cars. To perform on the animal, there is a need to create large datasets of different types in multiple conditions. The dataset construction comprises of data collection and data annotation. In this work, we put more focus on the latter task. Particularly, we renovate the well-known tool, LabelMe, so as to assist common user with or without in-depth knowledge about computer science to annotate the data with less effort. The new tool named as TrackMe inherits the simplicity, high compatibility with varied systems, minimal hardware requirement and convenient feature utilization from the predecessor. TrackMe is an upgraded version with essential features for multiple object tracking annotation.



