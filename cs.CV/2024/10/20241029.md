# Arxiv Papers in cs.CV on 2024-10-29
### Investigation of moving objects through atmospheric turbulence from a non-stationary platform
- **Arxiv ID**: http://arxiv.org/abs/2410.21639v1
- **DOI**: 10.1117/12.2528201
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2410.21639v1)
- **Published**: 2024-10-29 00:54:28+00:00
- **Updated**: 2024-10-29 00:54:28+00:00
- **Authors**: Nicholas Ferrante, Jerome Gilles, Shibin Parameswaran
- **Comment**: None
- **Journal**: Proceedings of the SPIE 11137, Applications of Digital Image
  Processing XLII, 111370X, San Diego, August 2019
- **Summary**: In this work, we extract the optical flow field corresponding to moving objects from an image sequence of a scene impacted by atmospheric turbulence \emph{and} captured from a moving camera. Our procedure first computes the optical flow field and creates a motion model to compensate for the flow field induced by camera motion. After subtracting the motion model from the optical flow, we proceed with our previous work, Gilles et al~\cite{gilles2018detection}, where a spatial-temporal cartoon+texture inspired decomposition is performed on the motion-compensated flow field in order to separate flows corresponding to atmospheric turbulence and object motion. Finally, the geometric component is processed with the detection and tracking method and is compared against a ground truth. All of the sequences and code used in this work are open source and are available by contacting the authors.



### DOFS: A Real-world 3D Deformable Object Dataset with Full Spatial Information for Dynamics Model Learning
- **Arxiv ID**: http://arxiv.org/abs/2410.21758v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2410.21758v1)
- **Published**: 2024-10-29 05:46:16+00:00
- **Updated**: 2024-10-29 05:46:16+00:00
- **Authors**: Zhen Zhang, Xiangyu Chu, Yunxi Tang, K. W. Samuel Au
- **Comment**: 5 pages, 6 figures, 2024 CoRL Workshop on Learning Robot Fine and
  Dexterous Manipulation: Perception and Control
- **Journal**: None
- **Summary**: This work proposes DOFS, a pilot dataset of 3D deformable objects (DOs) (e.g., elasto-plastic objects) with full spatial information (i.e., top, side, and bottom information) using a novel and low-cost data collection platform with a transparent operating plane. The dataset consists of active manipulation action, multi-view RGB-D images, well-registered point clouds, 3D deformed mesh, and 3D occupancy with semantics, using a pinching strategy with a two-parallel-finger gripper. In addition, we trained a neural network with the down-sampled 3D occupancy and action as input to model the dynamics of an elasto-plastic object. Our dataset and all CADs of the data collection system will be released soon on our website.



### Diffusion as Reasoning: Enhancing Object Goal Navigation with LLM-Biased Diffusion Model
- **Arxiv ID**: http://arxiv.org/abs/2410.21842v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2410.21842v1)
- **Published**: 2024-10-29 08:10:06+00:00
- **Updated**: 2024-10-29 08:10:06+00:00
- **Authors**: Yiming Ji, Yang Liu, Zhengpu Wang, Boyu Ma, Zongwu Xie, Hong Liu
- **Comment**: None
- **Journal**: None
- **Summary**: The Object Goal Navigation (ObjectNav) task requires the agent to navigate to a specified target in an unseen environment. Since the environment layout is unknown, the agent needs to perform semantic reasoning to infer the potential location of the target, based on its accumulated memory of the environment during the navigation process. Diffusion models have been shown to be able to learn the distribution relationships between features in RGB images, and thus generate new realistic images.In this work, we propose a new approach to solving the ObjectNav task, by training a diffusion model to learn the statistical distribution patterns of objects in semantic maps, and using the map of the explored regions during navigation as the condition to generate the map of the unknown regions, thereby realizing the semantic reasoning of the target object, i.e., diffusion as reasoning (DAR). Meanwhile, we propose the global target bias and local LLM bias methods, where the former can constrain the diffusion model to generate the target object more effectively, and the latter utilizes the common sense knowledge extracted from the LLM to improve the generalization of the reasoning process. Based on the generated map in the unknown region, the agent sets the predicted location of the target as the goal and moves towards it. Experiments on Gibson and MP3D show the effectiveness of our method.



### Multi-Object 3D Grounding with Dynamic Modules and Language-Informed Spatial Attention
- **Arxiv ID**: http://arxiv.org/abs/2410.22306v2
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2410.22306v2)
- **Published**: 2024-10-29 17:52:20+00:00
- **Updated**: 2024-12-20 03:52:45+00:00
- **Authors**: Haomeng Zhang, Chiao-An Yang, Raymond A. Yeh
- **Comment**: NeurIPS 2024
- **Journal**: None
- **Summary**: Multi-object 3D Grounding involves locating 3D boxes based on a given query phrase from a point cloud. It is a challenging and significant task with numerous applications in visual understanding, human-computer interaction, and robotics. To tackle this challenge, we introduce D-LISA, a two-stage approach incorporating three innovations. First, a dynamic vision module that enables a variable and learnable number of box proposals. Second, a dynamic camera positioning that extracts features for each proposal. Third, a language-informed spatial attention module that better reasons over the proposals to output the final prediction. Empirically, experiments show that our method outperforms the state-of-the-art methods on multi-object 3D grounding by 12.8% (absolute) and is competitive in single-object 3D grounding.



### Addressing Issues with Working Memory in Video Object Segmentation
- **Arxiv ID**: http://arxiv.org/abs/2410.22451v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI, 68T45, I.4.6; I.2.10
- **Links**: [PDF](http://arxiv.org/pdf/2410.22451v1)
- **Published**: 2024-10-29 18:34:41+00:00
- **Updated**: 2024-10-29 18:34:41+00:00
- **Authors**: Clayton Bromley, Alexander Moore, Amar Saini, Douglas Poland, Carmen Carrano
- **Comment**: 12 pages, 11 figures
- **Journal**: None
- **Summary**: Contemporary state-of-the-art video object segmentation (VOS) models compare incoming unannotated images to a history of image-mask relations via affinity or cross-attention to predict object masks. We refer to the internal memory state of the initial image-mask pair and past image-masks as a working memory buffer. While the current state of the art models perform very well on clean video data, their reliance on a working memory of previous frames leaves room for error. Affinity-based algorithms include the inductive bias that there is temporal continuity between consecutive frames. To account for inconsistent camera views of the desired object, working memory models need an algorithmic modification that regulates the memory updates and avoid writing irrelevant frames into working memory. A simple algorithmic change is proposed that can be applied to any existing working memory-based VOS model to improve performance on inconsistent views, such as sudden camera cuts, frame interjections, and extreme context changes. The resulting model performances show significant improvement on video data with these frame interjections over the same model without the algorithmic addition. Our contribution is a simple decision function that determines whether working memory should be updated based on the detection of sudden, extreme changes and the assumption that the object is no longer in frame. By implementing algorithmic changes, such as this, we can increase the real-world applicability of current VOS models.



### Unified Domain Generalization and Adaptation for Multi-View 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2410.22461v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2410.22461v1)
- **Published**: 2024-10-29 18:51:49+00:00
- **Updated**: 2024-10-29 18:51:49+00:00
- **Authors**: Gyusam Chang, Jiwon Lee, Donghyun Kim, Jinkyu Kim, Dongwook Lee, Daehyun Ji, Sujin Jang, Sangpil Kim
- **Comment**: Accepted to NeurIPS 2024
- **Journal**: None
- **Summary**: Recent advances in 3D object detection leveraging multi-view cameras have demonstrated their practical and economical value in various challenging vision tasks. However, typical supervised learning approaches face challenges in achieving satisfactory adaptation toward unseen and unlabeled target datasets (\ie, direct transfer) due to the inevitable geometric misalignment between the source and target domains. In practice, we also encounter constraints on resources for training models and collecting annotations for the successful deployment of 3D object detectors. In this paper, we propose Unified Domain Generalization and Adaptation (UDGA), a practical solution to mitigate those drawbacks. We first propose Multi-view Overlap Depth Constraint that leverages the strong association between multi-view, significantly alleviating geometric gaps due to perspective view changes. Then, we present a Label-Efficient Domain Adaptation approach to handle unfamiliar targets with significantly fewer amounts of labels (\ie, 1$\%$ and 5$\%)$, while preserving well-defined source knowledge for training efficiency. Overall, UDGA framework enables stable detection performance in both source and target domains, effectively bridging inevitable domain gaps, while demanding fewer annotations. We demonstrate the robustness of UDGA with large-scale benchmarks: nuScenes, Lyft, and Waymo, where our framework outperforms the current state-of-the-art methods.



