# Arxiv Papers in cs.CV on 2024-07-24
### DVPE: Divided View Position Embedding for Multi-View 3D Object Detection
- **Arxiv ID**: http://arxiv.org/abs/2407.16955v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.RO
- **Links**: [PDF](http://arxiv.org/pdf/2407.16955v1)
- **Published**: 2024-07-24 02:44:41+00:00
- **Updated**: 2024-07-24 02:44:41+00:00
- **Authors**: Jiasen Wang, Zhenglin Li, Ke Sun, Xianyuan Liu, Yang Zhou
- **Comment**: None
- **Journal**: None
- **Summary**: Sparse query-based paradigms have achieved significant success in multi-view 3D detection for autonomous vehicles. Current research faces challenges in balancing between enlarging receptive fields and reducing interference when aggregating multi-view features. Moreover, different poses of cameras present challenges in training global attention models. To address these problems, this paper proposes a divided view method, in which features are modeled globally via the visibility crossattention mechanism, but interact only with partial features in a divided local virtual space. This effectively reduces interference from other irrelevant features and alleviates the training difficulties of the transformer by decoupling the position embedding from camera poses. Additionally, 2D historical RoI features are incorporated into the object-centric temporal modeling to utilize highlevel visual semantic information. The model is trained using a one-to-many assignment strategy to facilitate stability. Our framework, named DVPE, achieves state-of-the-art performance (57.2% mAP and 64.5% NDS) on the nuScenes test set. Codes will be available at https://github.com/dop0/DVPE.



### Diffree: Text-Guided Shape Free Object Inpainting with Diffusion Model
- **Arxiv ID**: http://arxiv.org/abs/2407.16982v1
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2407.16982v1)
- **Published**: 2024-07-24 03:58:58+00:00
- **Updated**: 2024-07-24 03:58:58+00:00
- **Authors**: Lirui Zhao, Tianshuo Yang, Wenqi Shao, Yuxin Zhang, Yu Qiao, Ping Luo, Kaipeng Zhang, Rongrong Ji
- **Comment**: None
- **Journal**: None
- **Summary**: This paper addresses an important problem of object addition for images with only text guidance. It is challenging because the new object must be integrated seamlessly into the image with consistent visual context, such as lighting, texture, and spatial location. While existing text-guided image inpainting methods can add objects, they either fail to preserve the background consistency or involve cumbersome human intervention in specifying bounding boxes or user-scribbled masks. To tackle this challenge, we introduce Diffree, a Text-to-Image (T2I) model that facilitates text-guided object addition with only text control. To this end, we curate OABench, an exquisite synthetic dataset by removing objects with advanced image inpainting techniques. OABench comprises 74K real-world tuples of an original image, an inpainted image with the object removed, an object mask, and object descriptions. Trained on OABench using the Stable Diffusion model with an additional mask prediction module, Diffree uniquely predicts the position of the new object and achieves object addition with guidance from only text. Extensive experiments demonstrate that Diffree excels in adding new objects with a high success rate while maintaining background consistency, spatial appropriateness, and object relevance and quality.



### ALPI: Auto-Labeller with Proxy Injection for 3D Object Detection using 2D Labels Only
- **Arxiv ID**: http://arxiv.org/abs/2407.17197v3
- **DOI**: None
- **Categories**: **cs.CV**, cs.AI
- **Links**: [PDF](http://arxiv.org/pdf/2407.17197v3)
- **Published**: 2024-07-24 11:58:31+00:00
- **Updated**: 2024-11-27 07:22:47+00:00
- **Authors**: Saad Lahlali, Nicolas Granger, Herv√© Le Borgne, Quoc-Cuong Pham
- **Comment**: accepted at WACV2025
- **Journal**: None
- **Summary**: 3D object detection plays a crucial role in various applications such as autonomous vehicles, robotics and augmented reality. However, training 3D detectors requires a costly precise annotation, which is a hindrance to scaling annotation to large datasets. To address this challenge, we propose a weakly supervised 3D annotator that relies solely on 2D bounding box annotations from images, along with size priors. One major problem is that supervising a 3D detection model using only 2D boxes is not reliable due to ambiguities between different 3D poses and their identical 2D projection. We introduce a simple yet effective and generic solution: we build 3D proxy objects with annotations by construction and add them to the training dataset. Our method requires only size priors to adapt to new classes. To better align 2D supervision with 3D detection, our method ensures depth invariance with a novel expression of the 2D losses. Finally, to detect more challenging instances, our annotator follows an offline pseudo-labelling scheme which gradually improves its 3D pseudo-labels. Extensive experiments on the KITTI dataset demonstrate that our method not only performs on-par or above previous works on the Car category, but also achieves performance close to fully supervised methods on more challenging classes. We further demonstrate the effectiveness and robustness of our method by being the first to experiment on the more challenging nuScenes dataset. We additionally propose a setting where weak labels are obtained from a 2D detector pre-trained on MS-COCO instead of human annotations. The code is available at https://github.com/CEA-LIST/ALPI



### PEEKABOO: Hiding parts of an image for unsupervised object localization
- **Arxiv ID**: http://arxiv.org/abs/2407.17628v1
- **DOI**: None
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2407.17628v1)
- **Published**: 2024-07-24 20:35:20+00:00
- **Updated**: 2024-07-24 20:35:20+00:00
- **Authors**: Hasib Zunair, A. Ben Hamza
- **Comment**: None
- **Journal**: None
- **Summary**: Localizing objects in an unsupervised manner poses significant challenges due to the absence of key visual information such as the appearance, type and number of objects, as well as the lack of labeled object classes typically available in supervised settings. While recent approaches to unsupervised object localization have demonstrated significant progress by leveraging self-supervised visual representations, they often require computationally intensive training processes, resulting in high resource demands in terms of computation, learnable parameters, and data. They also lack explicit modeling of visual context, potentially limiting their accuracy in object localization. To tackle these challenges, we propose a single-stage learning framework, dubbed PEEKABOO, for unsupervised object localization by learning context-based representations at both the pixel- and shape-level of the localized objects through image masking. The key idea is to selectively hide parts of an image and leverage the remaining image information to infer the location of objects without explicit supervision. The experimental results, both quantitative and qualitative, across various benchmark datasets, demonstrate the simplicity, effectiveness and competitive performance of our approach compared to state-of-the-art methods in both single object discovery and unsupervised salient object detection tasks. Code and pre-trained models are available at: https://github.com/hasibzunair/peekaboo



### SDLNet: Statistical Deep Learning Network for Co-Occurring Object Detection and Identification
- **Arxiv ID**: http://arxiv.org/abs/2407.17664v1
- **DOI**: 10.1145/3674029.3674055
- **Categories**: **cs.CV**
- **Links**: [PDF](http://arxiv.org/pdf/2407.17664v1)
- **Published**: 2024-07-24 22:21:35+00:00
- **Updated**: 2024-07-24 22:21:35+00:00
- **Authors**: Binay Kumar Singh, Niels Da Vitoria Lobo
- **Comment**: 8 pages, 3 figures, ICMLT-2024. arXiv admin note: text overlap with
  arXiv:2403.17223
- **Journal**: (ICMLT 2024) 2024 9th International Conference on Machine Learning
  Technologies
- **Summary**: With the growing advances in deep learning based technologies the detection and identification of co-occurring objects is a challenging task which has many applications in areas such as, security and surveillance. In this paper, we propose a novel framework called SDLNet- Statistical analysis with Deep Learning Network that identifies co-occurring objects in conjunction with base objects in multilabel object categories. The pipeline of proposed work is implemented in two stages: in the first stage of SDLNet we deal with multilabel detectors for discovering labels, and in the second stage we perform co-occurrence matrix analysis. In co-occurrence matrix analysis, we learn co-occurrence statistics by setting base classes and frequently occurring classes, following this we build association rules and generate frequent patterns. The crucial part of SDLNet is recognizing base classes and making consideration for co-occurring classes. Finally, the generated co-occurrence matrix based on frequent patterns will show base classes and their corresponding co-occurring classes. SDLNet is evaluated on two publicly available datasets: Pascal VOC and MS-COCO. The experimental results on these benchmark datasets are reported in Sec 4.



